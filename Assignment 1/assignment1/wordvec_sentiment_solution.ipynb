{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Word Vectors and Sentiment Analysis\n",
    "CS 224D Assignment 1  \n",
    "Spring 2015\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://cs224d.stanford.edu/assignment1) on the course website.*\n",
    "\n",
    "In this assignment, we will walk you through the process of implementing \n",
    "\n",
    "- A softmax function\n",
    "- A simple neural network\n",
    "- Back propagation\n",
    "- Word2vec models\n",
    "\n",
    "and training your own word vectors with stochastic gradient descent (SGD) for a sentiment analysis task. Please make sure to finish the corresponding problems in the problem set PDF when instructed by the worksheet.\n",
    "\n",
    "The purpose of this assignment is to familiarize you with basic knowledge about neural networks and machine learning, including optimization and cross-validation, and help you gain proficiency in writing efficient, vectorized code.\n",
    "\n",
    "** Please don't add or remove any code cells, as it might break our automatic grading system and affect your grade. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Honor Code:** I hereby agree to abide the Stanford Honor Code and that of the Computer Science Department, promise that the submitted assignment is my own work, and understand that my code is subject to plagiarism test.\n",
    "\n",
    "**Signature**: *(double click on this block and type your name here)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Run some setup code for this notebook. Don't modify anything in this cell.\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from cs224d.data_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Softmax\n",
    "*Please answer the first first complementary problem before starting this part.*\n",
    "\n",
    "Given an input matrix of *N* rows and *d* columns, compute the softmax prediction for each row. That is, when the input is\n",
    "\n",
    "    [[1,2],\n",
    "    [3,4]]\n",
    "    \n",
    "the output of your functions should be\n",
    "\n",
    "    [[0.2689, 0.7311],\n",
    "    [0.2689, 0.7311]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\" Softmax function \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the softmax function for the input here.                #\n",
    "    # It is crucial that this function is optimized for speed because #\n",
    "    # it will be used frequently in later code.                       #\n",
    "    # You might find numpy functions np.exp, np.sum, np.reshape,      #\n",
    "    # np.max, and numpy broadcasting useful for this task. (numpy     #\n",
    "    # broadcasting documentation:                                     #\n",
    "    # http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)  #\n",
    "    # You should also make sure that your code works for one          #\n",
    "    # dimensional inputs (treat the vector as a row), you might find  #\n",
    "    # it helpful for your later problems.                             #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    x = x.T\n",
    "    norm_x = x - np.max(x, axis = 0)\n",
    "    exp_x = np.exp(norm_x)\n",
    "    prob = exp_x / np.sum(exp_x, axis = 0)\n",
    "    return prob.T\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "[[ 0.09003057  0.24472847  0.66524096]\n",
      " [ 0.09003057  0.24472847  0.66524096]]\n",
      "Wall time: 2 ms\n",
      "[[ 0.26894142  0.73105858]\n",
      " [ 0.26894142  0.73105858]]\n",
      "Wall time: 2 ms\n",
      "[[ 0.73105858  0.26894142]]\n",
      "Wall time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "# Verify your softmax implementation\n",
    "\n",
    "print \"=== For autograder ===\"\n",
    "%time print softmax(np.array([[1,2,3],[3,4,5]]))\n",
    "%time print softmax(np.array([[1001,1002],[3,4]]))\n",
    "%time print softmax(np.array([[-1001,-1002]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural network basics\n",
    "\n",
    "*Please answer the second complementary question before starting this part.*\n",
    "\n",
    "In this part, you're going to implement\n",
    "\n",
    "* A sigmoid activation function and its gradient\n",
    "* A forward propagation for a simple neural network with cross-entropy cost\n",
    "* A backward propagation algorithm to compute gradients for the parameters\n",
    "* Gradient / derivative check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\" Sigmoid function \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the sigmoid function for the input here.                #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "    ### END YOUR CODE\n",
    "\n",
    "def sigmoid_grad(f):\n",
    "    \"\"\" Sigmoid gradient function \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the gradient for the sigmoid function here. Note that   #\n",
    "    # for this implementation, the input f should be the sigmoid      #\n",
    "    # function value of your original input x.                        #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    return (1 - f) * f\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1 ms\n",
      "Wall time: 0 ns\n",
      "=== For autograder ===\n",
      "[[ 0.73105858  0.88079708]\n",
      " [ 0.26894142  0.11920292]]\n",
      "[[ 0.19661193  0.10499359]\n",
      " [ 0.19661193  0.10499359]]\n"
     ]
    }
   ],
   "source": [
    "# Check your sigmoid implementation\n",
    "x = np.array([[1, 2], [-1, -2]])\n",
    "%time f = sigmoid(x)\n",
    "%time g = sigmoid_grad(f)\n",
    "print \"=== For autograder ===\"\n",
    "print f\n",
    "print g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the functions you just implemented, fill in the following functions to implement a neural network with one sigmoid hidden layer. You might find the handout and your answers to the second complementary problem helpful for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First implement a gradient checker by filling in the following functions\n",
    "def gradcheck_naive(f, x):\n",
    "    \"\"\" \n",
    "    Gradient check for a function f \n",
    "    - f should be a function that takes a single argument and outputs the cost and its gradients\n",
    "    - x is the point (numpy array) to check the gradient at\n",
    "    \"\"\" \n",
    "\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)  \n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-4\n",
    "\n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "    \n",
    "        ### YOUR CODE HERE: try modifying x[ix] with h defined above to compute numerical gradients\n",
    "        ### make sure you call random.setstate(rndstate) before calling f(x) each time, this will make it \n",
    "        ### possible to test cost functions with built in randomness later\n",
    "    \n",
    "        x[ix] -= h\n",
    "        random.setstate(rndstate)\n",
    "        f0 = f(x)[0]\n",
    "        x[ix] += 2 * h\n",
    "        random.setstate(rndstate)\n",
    "        f1 = f(x)[0]\n",
    "        x[ix] -= h\n",
    "        numgrad = (f1 - f0) / (2 * h)\n",
    "    \n",
    "        ### END YOUR CODE\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print \"Gradient check failed.\"\n",
    "            print \"First gradient error found at index %s\" % str(ix)\n",
    "            print \"Your gradient: %f \\t Numerical gradient: %f\" % (grad[ix], numgrad)\n",
    "            return\n",
    "    \n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print \"Gradient check passed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "1 loops, best of 3: 483 µs per loop\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "1 loops, best of 3: 1.19 ms per loop\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "1 loops, best of 3: 3.32 ms per loop\n"
     ]
    }
   ],
   "source": [
    "# Sanity check for the gradient checker\n",
    "quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "print \"=== For autograder ===\"\n",
    "%timeit -n 1 gradcheck_naive(quad, np.array(123.456))      # scalar test\n",
    "%timeit -n 1 gradcheck_naive(quad, np.random.randn(3,))    # 1-D test\n",
    "%timeit -n 1 gradcheck_naive(quad, np.random.randn(4,5))   # 2-D test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up fake data and parameters for the neural network\n",
    "N = 20\n",
    "dimensions = [10, 5, 10]\n",
    "data = np.random.randn(N, dimensions[0])   # each row will be a datum\n",
    "labels = np.zeros((N, dimensions[2]))\n",
    "for i in xrange(N):\n",
    "    labels[i,random.randint(0,dimensions[2]-1)] = 1\n",
    "\n",
    "params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (dimensions[1] + 1) * dimensions[2], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_backward_prop(data, labels, params):\n",
    "    \"\"\" Forward and backward propagation for a two-layer sigmoidal network \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the forward propagation and for the cross entropy cost, #\n",
    "    # and backward propagation for the gradients for all parameters.  #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### Unpack network parameters (do not modify)\n",
    "    t = 0\n",
    "    W1 = np.reshape(params[t:t+dimensions[0]*dimensions[1]], (dimensions[0], dimensions[1]))\n",
    "    t += dimensions[0]*dimensions[1]\n",
    "    b1 = np.reshape(params[t:t+dimensions[1]], (1, dimensions[1]))\n",
    "    t += dimensions[1]\n",
    "    W2 = np.reshape(params[t:t+dimensions[1]*dimensions[2]], (dimensions[1], dimensions[2]))\n",
    "    t += dimensions[1]*dimensions[2]\n",
    "    b2 = np.reshape(params[t:t+dimensions[2]], (1, dimensions[2]))\n",
    "    \n",
    "    ### YOUR CODE HERE: forward propagation\n",
    "    \n",
    "    z1 = data.dot(W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    a2 = softmax(z2)\n",
    "    cost = -np.sum(np.log(a2[labels == 1]))\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    ### YOUR CODE HERE: backward propagation\n",
    "\n",
    "    delta2 = a2 - labels\n",
    "    gradW2 = a1.T.dot(delta2)\n",
    "    gradb2 = np.sum(delta2, axis = 0).reshape(1, -1)\n",
    "    delta1 = delta2.dot(W2.T) * sigmoid_grad(z1)\n",
    "    gradW1 = data.T.dot(delta1)\n",
    "    gradb1 = np.sum(delta1, axis = 0).reshape(1, -1)\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    ### Stack gradients (do not modify)\n",
    "    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(), gradW2.flatten(), gradb2.flatten()))\n",
    "    \n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "Gradient check failed.\n",
      "First gradient error found at index (0,)\n",
      "Your gradient: -205.344454 \t Numerical gradient: -0.198558\n"
     ]
    }
   ],
   "source": [
    "# Perform gradcheck on your neural network\n",
    "print \"=== For autograder ===\"\n",
    "gradcheck_naive(lambda params: forward_backward_prop(data, labels, params), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word2vec\n",
    "\n",
    "*Please answer the third complementary problem before starting this part.*\n",
    "\n",
    "In this part you will implement the `word2vec` models and train your own word vectors with stochastic gradient descent (SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Implement your skip-gram and CBOW models here\n",
    "\n",
    "# Interface to the dataset for negative sampling\n",
    "dataset = type('dummy', (), {})()\n",
    "def dummySampleTokenIdx():\n",
    "    return random.randint(0, 4)\n",
    "def getRandomContext(C):\n",
    "    tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "    return tokens[random.randint(0,4)], [tokens[random.randint(0,4)] for i in xrange(2*C)]\n",
    "dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "dataset.getRandomContext = getRandomContext\n",
    "\n",
    "def softmaxCostAndGradient(predicted, target, outputVectors):\n",
    "    \"\"\" Softmax cost function for word2vec models \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement the cost and gradients for one predicted word vector  #\n",
    "    # and one target word vector as a building block for word2vec     #\n",
    "    # models, assuming the softmax prediction function and cross      #\n",
    "    # entropy loss.                                                   #\n",
    "    # Inputs:                                                         #\n",
    "    #   - predicted: numpy ndarray, predicted word vector (\\hat{r} in #\n",
    "    #           the written component)                                #\n",
    "    #   - target: integer, the index of the target word               #\n",
    "    #   - outputVectors: \"output\" vectors for all tokens              #\n",
    "    # Outputs:                                                        #\n",
    "    #   - cost: cross entropy cost for the softmax word prediction    #\n",
    "    #   - gradPred: the gradient with respect to the predicted word   #\n",
    "    #           vector                                                #\n",
    "    #   - grad: the gradient with respect to all the other word       # \n",
    "    #           vectors                                               #\n",
    "    # We will not provide starter code for this function, but feel    #\n",
    "    # free to reference the code you previously wrote for this        #\n",
    "    # assignment!                                                     #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    # predicted: d-dimensional 1-d array\n",
    "    # outputVectors: V * d\n",
    "    # gradPred: d-dimensional 1-d array\n",
    "    # grad : V * d\n",
    "    y = softmax(outputVectors.dot(predicted))\n",
    "    #print \"outputVectors.shape, predicted.shape, y.shape\", outputVectors.shape, predicted.shape, y.shape\n",
    "    cost = -np.log(y[target])\n",
    "    y[target] -= 1\n",
    "    gradPred = outputVectors.T.dot(y)\n",
    "    grad = np.outer(y, predicted)\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradPred, grad\n",
    "\n",
    "def negSamplingCostAndGradient(predicted, target, outputVectors, K=10):\n",
    "    \"\"\" Negative sampling cost function for word2vec models \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement the cost and gradients for one predicted word vector  #\n",
    "    # and one target word vector as a building block for word2vec     #\n",
    "    # models, using the negative sampling technique. K is the sample  #\n",
    "    # size. You might want to use dataset.sampleTokenIdx() to sample  #\n",
    "    # a random word index.                                            #\n",
    "    # Input/Output Specifications: same as softmaxCostAndGradient     #\n",
    "    # We will not provide starter code for this function, but feel    #\n",
    "    # free to reference the code you previously wrote for this        #\n",
    "    # assignment!                                                     #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    y = sigmoid(outputVectors[target, :].dot(predicted))\n",
    "    cost = -np.log(y)\n",
    "    gradPred = (y - 1) * outputVectors[target, :]\n",
    "    grad = np.zeros_like(outputVectors)\n",
    "    grad[target, :] += (y - 1) * predicted\n",
    "    for _ in xrange(K):\n",
    "        i = dataset.sampleTokenIdx()\n",
    "        y = sigmoid(-outputVectors[i, :].dot(predicted))\n",
    "        cost += -np.log(y)\n",
    "        gradPred += (1 - y) * outputVectors[i, :]\n",
    "        grad[i, :] += (1 - y) * predicted\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradPred, grad\n",
    "\n",
    "def skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement the skip-gram model in this function.                 #         \n",
    "    # Inputs:                                                         #\n",
    "    #   - currrentWord: a string of the current center word           #\n",
    "    #   - C: integer, context size                                    #\n",
    "    #   - contextWords: list of no more than 2*C strings, the context #\n",
    "    #             words                                               #\n",
    "    #   - tokens: a dictionary that maps words to their indices in    #\n",
    "    #             the word vector list                                #\n",
    "    #   - inputVectors: \"input\" word vectors for all tokens           #\n",
    "    #   - outputVectors: \"output\" word vectors for all tokens         #\n",
    "    #   - word2vecCostAndGradient: the cost and gradient function for #\n",
    "    #             a prediction vector given the target word vectors,  #\n",
    "    #             could be one of the two cost functions you          #\n",
    "    #             implemented above                                   #\n",
    "    # Outputs:                                                        #\n",
    "    #   - cost: the cost function value for the skip-gram model       #\n",
    "    #   - grad: the gradient with respect to the word vectors         #\n",
    "    # We will not provide starter code for this function, but feel    #\n",
    "    # free to reference the code you previously wrote for this        #\n",
    "    # assignment!                                                     #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    cost = 0\n",
    "    gradIn = np.zeros_like(inputVectors)\n",
    "    gradOut = np.zeros_like(outputVectors)\n",
    "    \n",
    "    idx = tokens[currentWord]\n",
    "    predicted = inputVectors[idx, :]\n",
    "    for word in contextWords:\n",
    "        target = tokens[word]\n",
    "        c, gPred, grad = word2vecCostAndGradient(predicted, target, outputVectors)\n",
    "        cost += c\n",
    "        gradIn[idx, :] += gPred\n",
    "        gradOut += grad\n",
    "    #print cost, gradIn, gradOut\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradIn, gradOut\n",
    "\n",
    "def cbow(currentWord, C, contextWords, tokens, inputVectors, outputVectors, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    \"\"\" CBOW model in word2vec \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement the continuous bag-of-words model in this function.   #         \n",
    "    # Input/Output specifications: same as the skip-gram model        #\n",
    "    # We will not provide starter code for this function, but feel    #\n",
    "    # free to reference the code you previously wrote for this        #\n",
    "    # assignment!                                                     #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    gradIn = np.zeros_like(inputVectors)\n",
    "    \n",
    "    target = tokens[currentWord]\n",
    "    sumContext = np.sum(inputVectors[[tokens[cw] for cw in contextWords], :], axis = 0)\n",
    "    \n",
    "    cost, gradPred, gradOut = word2vecCostAndGradient(sumContext, target, outputVectors)\n",
    "    for cw in contextWords:\n",
    "        idx = tokens[cw]\n",
    "        gradIn[idx, :] += gradPred\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradIn, gradOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "[[ 0.6         0.8       ]\n",
      " [ 0.4472136   0.89442719]]\n"
     ]
    }
   ],
   "source": [
    "# Implement a function that normalizes each row of a matrix to have unit length\n",
    "def normalizeRows(x):\n",
    "    \"\"\" Row normalization function \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    norm_x = x / np.reshape(np.linalg.norm(x, ord = 2, axis = 1), (-1, 1))\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return norm_x\n",
    "\n",
    "# Test this function\n",
    "print \"=== For autograder ===\"\n",
    "print normalizeRows(np.array([[3.0,4.0],[1, 2]]))  # the result should be [[0.6, 0.8], [0.4472, 0.8944]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for skip-gram ====\n",
      "Gradient check passed!\n",
      "Wall time: 81 ms\n",
      "Gradient check passed!\n",
      "Wall time: 374 ms\n",
      "\n",
      "==== Gradient check for CBOW      ====\n",
      "Gradient check passed!\n",
      "Wall time: 33 ms\n",
      "Gradient check passed!\n",
      "Wall time: 69 ms\n",
      "\n",
      "=== For autograder ===\n",
      "(11.166109001533981, array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-1.26947339, -1.36873189,  2.45158957],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[-0.41045956,  0.18834851,  1.43272264],\n",
      "       [ 0.38202831, -0.17530219, -1.33348241],\n",
      "       [ 0.07009355, -0.03216399, -0.24466386],\n",
      "       [ 0.09472154, -0.04346509, -0.33062865],\n",
      "       [-0.13638384,  0.06258276,  0.47605228]]))\n",
      "(15.560688733849862, array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-3.83753697, -0.47396996,  0.29523754],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[-0.88442555,  0.40583834,  3.08711651],\n",
      "       [-0.42732551,  0.19608782,  1.49159376],\n",
      "       [-0.34146328,  0.15668803,  1.19188884],\n",
      "       [-0.42136814,  0.19335415,  1.47079939],\n",
      "       [-0.32248118,  0.14797767,  1.1256312 ]]))\n",
      "(0.79899580109066481, array([[ 0.23330542, -0.51643128, -0.8281311 ],\n",
      "       [ 0.11665271, -0.25821564, -0.41406555],\n",
      "       [ 0.11665271, -0.25821564, -0.41406555],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[ 0.80954933,  0.21962514, -0.54095764],\n",
      "       [-0.03556575, -0.00964874,  0.02376577],\n",
      "       [-0.13016109, -0.0353118 ,  0.08697634],\n",
      "       [-0.1650812 , -0.04478539,  0.11031068],\n",
      "       [-0.47874129, -0.1298792 ,  0.31990485]]))\n",
      "(8.6998572983542939, array([[-4.15335678,  2.66772976,  0.78331673],\n",
      "       [-2.07667839,  1.33386488,  0.39165836],\n",
      "       [-2.07667839,  1.33386488,  0.39165836],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[-3.53424992, -0.95881757,  2.36165904],\n",
      "       [-1.72281309, -0.46738728,  1.15121941],\n",
      "       [-1.55404334, -0.42160121,  1.03844397],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]))\n"
     ]
    }
   ],
   "source": [
    "# Gradient check!\n",
    "\n",
    "def word2vec_sgd_wrapper(word2vecModel, tokens, wordVectors, dataset, C, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    batchsize = 50\n",
    "    cost = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    inputVectors = wordVectors[:N/2,:]\n",
    "    outputVectors = wordVectors[N/2:,:]\n",
    "    for i in xrange(batchsize):\n",
    "        C1 = random.randint(1,C)\n",
    "        centerword, context = dataset.getRandomContext(C1)\n",
    "        \n",
    "        if word2vecModel == skipgram:\n",
    "            denom = 1\n",
    "        else:\n",
    "            denom = 1\n",
    "        \n",
    "        c, gin, gout = word2vecModel(centerword, C1, context, tokens, inputVectors, outputVectors, word2vecCostAndGradient)\n",
    "        cost += c / batchsize / denom\n",
    "        grad[:N/2, :] += gin / batchsize / denom\n",
    "        grad[N/2:, :] += gout / batchsize / denom\n",
    "        break\n",
    "    return cost, grad\n",
    "\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "print \"==== Gradient check for skip-gram ====\"\n",
    "%time gradcheck_naive(lambda vec: word2vec_sgd_wrapper(skipgram, dummy_tokens, vec, dataset, 5), dummy_vectors)\n",
    "%time gradcheck_naive(lambda vec: word2vec_sgd_wrapper(skipgram, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient), dummy_vectors)\n",
    "print \"\\n==== Gradient check for CBOW      ====\"\n",
    "%time gradcheck_naive(lambda vec: word2vec_sgd_wrapper(cbow, dummy_tokens, vec, dataset, 5), dummy_vectors)\n",
    "%time gradcheck_naive(lambda vec: word2vec_sgd_wrapper(cbow, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient), dummy_vectors)\n",
    "\n",
    "print \"\\n=== For autograder ===\"\n",
    "print skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:])\n",
    "print skipgram(\"c\", 1, [\"a\", \"b\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], negSamplingCostAndGradient)\n",
    "print cbow(\"a\", 2, [\"a\", \"b\", \"c\", \"a\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:])\n",
    "print cbow(\"a\", 2, [\"a\", \"b\", \"a\", \"c\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], negSamplingCostAndGradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, implement SGD\n",
    "\n",
    "# Save parameters every a few SGD iterations as fail-safe\n",
    "SAVE_PARAMS_EVERY = 1000\n",
    "\n",
    "import glob\n",
    "import os.path as op\n",
    "import cPickle as pickle\n",
    "\n",
    "def load_saved_params():\n",
    "    \"\"\" A helper function that loads previously saved parameters and resets iteration start \"\"\"\n",
    "    st = 0\n",
    "    for f in glob.glob(\"saved_params_*.npy\"):\n",
    "        iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
    "        if (iter > st):\n",
    "            st = iter\n",
    "            \n",
    "    if st > 0:\n",
    "        with open(\"saved_params_%d.npy\" % st, \"r\") as f:\n",
    "            params = pickle.load(f)\n",
    "            state = pickle.load(f)\n",
    "        return st, params, state\n",
    "    else:\n",
    "        return st, None, None\n",
    "    \n",
    "def save_params(iter, params):\n",
    "    with open(\"saved_params_%d.npy\" % iter, \"w\") as f:\n",
    "        pickle.dump(params, f)\n",
    "        pickle.dump(random.getstate(), f)\n",
    "\n",
    "def sgd(f, x0, step, iterations, postprocessing = None, useSaved = False, PRINT_EVERY=10):\n",
    "    \"\"\" Stochastic Gradient Descent \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement the stochastic gradient descent method in this        #\n",
    "    # function.                                                       #\n",
    "    # Inputs:                                                         #\n",
    "    #   - f: the function to optimize, it should take a single        #\n",
    "    #        argument and yield two outputs, a cost and the gradient  #\n",
    "    #        with respect to the arguments                            #\n",
    "    #   - x0: the initial point to start SGD from                     #\n",
    "    #   - step: the step size for SGD                                 #\n",
    "    #   - iterations: total iterations to run SGD for                 #\n",
    "    #   - postprocessing: postprocessing function for the parameters  #\n",
    "    #        if necessary. In the case of word2vec we will need to    #\n",
    "    #        normalize the word vectors to have unit length.          #\n",
    "    #   - PRINT_EVERY: specifies every how many iterations to output  #\n",
    "    # Output:                                                         #\n",
    "    #   - x: the parameter value after SGD finishes                   #\n",
    "    ###################################################################\n",
    "    \n",
    "    # Anneal learning rate every several iterations\n",
    "    ANNEAL_EVERY = 20000\n",
    "    \n",
    "    if useSaved:\n",
    "        start_iter, oldx, state = load_saved_params()\n",
    "        if start_iter > 0:\n",
    "            x0 = oldx;\n",
    "            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n",
    "            \n",
    "        if state:\n",
    "            random.setstate(state)\n",
    "    else:\n",
    "        start_iter = 0\n",
    "    \n",
    "    x = x0\n",
    "    \n",
    "    if not postprocessing:\n",
    "        postprocessing = lambda x: x\n",
    "    \n",
    "    expcost = None\n",
    "    \n",
    "    for iter in xrange(start_iter + 1, iterations + 1):\n",
    "        ### YOUR CODE HERE\n",
    "        ### Don't forget to apply the postprocessing after every iteration!\n",
    "        ### You might want to print the progress every few iterations.\n",
    "        cost, grad = f(x)\n",
    "        x -= step * grad\n",
    "        x = postprocessing(x)\n",
    "        if iter % PRINT_EVERY == 0:\n",
    "            print \"After %sth iteration, cost function is %s\" % (iter, cost)\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        if iter % SAVE_PARAMS_EVERY == 0 and useSaved:\n",
    "            save_params(iter, x)\n",
    "            \n",
    "        if iter % ANNEAL_EVERY == 0:\n",
    "            step *= 0.5\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show time! Now we are going to load some real data and train word vectors with everything you just implemented!**\n",
    "\n",
    "We are going to use the Stanford Sentiment Treebank (SST) dataset to train word vectors, and later apply them to a simple sentiment analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load some data and initialize word vectors\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(314)\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)\n",
    "\n",
    "# We are going to train 10-dimensional vectors for this assignment\n",
    "dimVectors = 10\n",
    "\n",
    "# Context size\n",
    "C = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.4 s\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'f_out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-f53fce0664a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m                               np.zeros((nWords, dimVectors))), axis=0)\n\u001b[0;32m      8\u001b[0m wordVectors0 = sgd(lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C, negSamplingCostAndGradient), \n\u001b[1;32m----> 9\u001b[1;33m                    wordVectors, 0.3, 40000, None, True, PRINT_EVERY=10)\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m# sanity check: cost at convergence should be around or below 10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-f1eded266817>\u001b[0m in \u001b[0;36msgd\u001b[1;34m(f, x0, step, iterations, postprocessing, useSaved, PRINT_EVERY)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;31m### You might want to print the progress every few iterations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time f_out = f(x)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf_out\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpostprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'f_out' is not defined"
     ]
    }
   ],
   "source": [
    "# Train word vectors (this could take a while!)\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "wordVectors = np.concatenate(((np.random.rand(nWords, dimVectors) - .5) / dimVectors, \n",
    "                              np.zeros((nWords, dimVectors))), axis=0)\n",
    "wordVectors0 = sgd(lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C, negSamplingCostAndGradient), \n",
    "                   wordVectors, 0.3, 40000, None, True, PRINT_EVERY=10)\n",
    "# sanity check: cost at convergence should be around or below 10\n",
    "\n",
    "# sum the input and output word vectors\n",
    "wordVectors = (wordVectors0[:nWords,:] + wordVectors0[nWords:,:])\n",
    "\n",
    "print \"\\n=== For autograder ===\"\n",
    "checkWords = [\"the\", \"a\", \"an\", \"movie\", \"ordinary\", \"but\", \"and\"]\n",
    "checkIdx = [tokens[word] for word in checkWords]\n",
    "checkVecs = wordVectors[checkIdx, :]\n",
    "print checkVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.12949208345550939, 0.1192901876875743)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAmIAAAHiCAYAAABLDqCjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAIABJREFUeJzs3XeYVdXdt/F7TQcEcRQUkGDDYIyxJBYejWJsYGKJnehj\n",
       "S9SYxP4kJipIwN5ijK/GHk1sMaZgi6IRTVEERLENQVSMqIAMvU1b7x8cJoMMw9Szzszcn+sinr3P\n",
       "Wvv8zr5g5pu19t4rxBiRJElS9uWlLkCSJKmzMohJkiQlYhCTJElKpCB1AesSeocBlFKUuo4mKaci\n",
       "zokzU5chSZLah5wNYpRSxHCWpi6jSR6kW+oSJElS++HUpCRJUiIGsbrG8OfUJUiSpM7DIFbXCA5P\n",
       "XYIkSeo8cvcasbou5y5q6EukmA25k3N4gFFMpyu/YQX7kcccBnAVM7mEGvrSm0v5PuN4gs2Zwk1E\n",
       "ugLwBS7mJCZzLf/HCg4AoIZNKGI8P+MCRjGdUQzkLgbzCReQxzyqGEQBU7mIswC4jW8wm0sJLKOI\n",
       "SVTRn4s5OdWpkSRJ7Vf7GBE7kPMZwTBO4GAW8l0m0BPoQk/+wUi+QWAJM/kJZ3MM2/Jd5vJ/AGzH\n",
       "Z5zMcYxgKDtwJh8yBoAfcx0jOIhvcRSB+Qzgnswn/XeZgSq2ZzdG8jP2oZov8Bu+xkyK+ZSr+Rrf\n",
       "YQTDqKZ0jT6SJElN0D5GxMbzPZ7iIABq6MM0tgQqOJ0XACjiHfJYSQ9qOJQyrqE/AAsp5K9cTiVf\n",
       "IlBNNVvXHrMSeJKb6cltfIc31/rMAl5jf2YDUMhbLKY//2I5+czkYGYBUMqfmcfxbfjNJUlSB5b7\n",
       "I2J3MZgV7MVJHMJIDqSAt6iiGKiq0yoSqASgK5HVAfNZTief2Yxkf85hGFBY2+NGLiCfWZzNI/V+\n",
       "bmBlna0aIgWsPfoVWvjtJElSJ5b7QayS7uSxgAGs5A9sQxW7NLpvDRtQwFwAfsPRQD4Av+YAVvB1\n",
       "vsvIRh8rEPkfZlDNAJ6kHwDlHIpTk5IkqZlCjLmZI8KgMJDhLOU6ZrGcVXEnL/PfImA50D3TeCWr\n",
       "xqZWP4d/cea9mkw7WDVGVpHZvyzzXqjzXnGdflWZtl0z769gVYQrzLy3eqwsP1NPlzrtKlv+3SVJ\n",
       "UvsVY2z0jFnuXyMWgBGZEahcUEYXBmXi3VVcThHvcT53AfAg3WJZnN5Q9xDCqBjjqDavs5PwfLYu\n",
       "z2fr8ny2Ls9n6/J8tq7V5zOE0KQRrtwPYrnmSY7nEY4mUkQhb3Akv0tdkiRJap8MYk11PncCd6Yu\n",
       "Q5IktX+5G8TKqeBBurGC9rOYdjkVjWg1vq3L6GTGpy6ggxmfuoAOZnzqAjqY8akL6GDGpy6ggxnf\n",
       "nE45e7H+aiGE2JSL3iRJklJpam7J/cdXSJIkdVAGMUmSpEQMYpIkSYkYxCRJkhIxiEmSJCViEJMk\n",
       "SUrEICZJkpSIQUySJCkRg5gkSVIiBjFJkqREDGKSJEmJGMQkSZISMYhJkiQlYhCTJElKxCAmSZKU\n",
       "iEFMkiQpEYOYJElSIgYxSZKkRAxikiRJiRjEJEmSEjGISZIkJVKQugBJkqSGhN5hAKUUtdoBy6mI\n",
       "c+LMVjteCxjEJElSbiuliOEsbbXjPUi3VjtWCzk1KUmSlIhBTJIktQ9T6M6NnAjAXQzmcn6TtqCW\n",
       "a3EQCyEMDSGUhRCmhxAurOf9QSGEl0IIK0IIFzSlryRJUq2P2ZBFnJS6jNbUomvEQgj5wM3A/sAs\n",
       "YGIIYWyM8Z06zeYBZwGHN6OvJEnSKq9zETVswRieBqoILOMKbqOKQRQwlYs4C4D72YH3uZRINwLl\n",
       "7Mm57MvctMXXr6UjYrsB78YYP4gxVgIPAYfVbRBjnBtjnARUNrWvJElSrZ24nDw+YAQH0Y8xVPFl\n",
       "dmMkP2MfqvkCv2FX5lLAe1zGfpzGCIaxEQ/zMj9NXfq6tPSuyX7Af+psfwTsnoW+kiSps4mENV4X\n",
       "8Br7MxuAQt5iMZvzPIuo5os8y0M8C0Tyycu0yUEtDWIxG31DCKPqbI6PMY5vwedKkqSOILCyzlYN\n",
       "MZNr8pnGiOzMsoUQhgBDmtu/pUFsFtC/znZ/Vo1stWrfGOOo5hQnSZI6kFKWEtlgne8HIvsygzI2\n",
       "5j524UReZS4FjGdLjmZ6W5SUGRwaX1tCCJc2pX9Lg9gkYGAIYQvgY+BYYPg62obPbTelryRJ6uwG\n",
       "M5/nmchoniOwgjzmrNWmF1UM5HRmMIbRdAcK2JDboW2CWEuFGFsyuwghhGHAjUA+cFeM8coQwhkA\n",
       "McbbQgibAROBHkANsBj4UoxxSX196zl+jDF+PsRJkqROIgwKA1v7yfqxLLZJMGtqbmlxEGtrBjFJ\n",
       "kjq3jhzEfLK+JElSIi76LUmScls5Fa26UHc5Fa12rBZyalKSJKmVODUpSZLUThjEJEmSEjGISZIk\n",
       "JWIQkyRJSsQgJkmSlIhBTJIkKRGDmCRJUiIGMUmSpEQMYpIkSYkYxCRJkhJxrUlJktRhhN5hAKUU\n",
       "rbGznIo4J85MVFKDDGKSJKnjKKWI4SxdY19rLhjeyjp8EKs3Gbc3OZzkJUlS83X4IFZvMm5vcjjJ\n",
       "S5Kk5vNi/bZ2PztwNaNTlyFJknJPxx8RS+143gDeSF2GJEnKPZ0riF3OXdTQl0gxG3In5/AAo5hO\n",
       "N+5gOfsTWME+nMLezONKfkEei6ngK0R6swmX8QOepBL4BZewgn2ByMb8kh/yGFdyIz15kjN5BoAr\n",
       "uJlSxlLIYj7lDC7mZK7lAqroSzVfoIZ+dOcOzuMeAK7jXJZxBIF55PMxJUzlfG5Ld7IkSVJb61xT\n",
       "kwdyPiMYxgkczEK+ywR6Al3owWRGciDFTOAVjq9tX00vRnA4X+JEPuMiAO7gYCrYnp+xH0M4ls8Y\n",
       "wfP0YlMepJxjAZhCdyr5Kicybq0aqtiaMxjOAXyTRVzAIvL4LTuynGGcxn4cyfFU8RUgZuOUSJKk\n",
       "dDpXEBvP9xjNM/yOsdTQh2lsCVRwBs8B0I2pVNI/0zrSg78CcBTvUkMvABazGz34E4XA3syjiJeY\n",
       "xk6cygSq2ZJ/shEvcDhdeJyua4WpSFeepRdVDGY+gc+YQG/msislPE0fKvkSyyhmHBCycUokSeqw\n",
       "LuNhnqN36jIa0nmC2F0MZgV7cRKHMJIDKeAtqigGqmrbBGqA/NrtPCrrHGF1MIqsGZICq0evuvEH\n",
       "JnAUizmGrXmo3jrCGsesppL8dRxTkiQ11zIC1WzBF1mQupSGdJ4gVkl38ljAAFbyB7ahil2adZwe\n",
       "TGARh7KMwD8opYLd+RJTANiVh1nMaUDkSGbU03vtgBWI9GYiKziAjyjibbqykv1walKSpOZ7nIGU\n",
       "8DibU5G6lIZ0nov1D+N57uZ/Gc148plBAZMz79QNPLGe7TVfn8lfuYavch3PApFNGMPezANWTVW+\n",
       "yL/pzlO1vcIax/z88Vc5galcyzPczbPk8RkFlJHPohZ9X0mSOrNj+DcwJnUZ6xNizO2BlxBCjDE2\n",
       "e6ouDAoDs/ZA1+mU8CDP8W0OZIcmfmYZXRjEcqZTwkP8kW34McN5C4AH6RbL4vS2KFmSpI6k3t/7\n",
       "Wfw92tTc0nlGxNranXydWVxHD25rcggD+CPXUs1AIiV04+HaECZJkjosR8TaA0fEJElqlHrXmM7i\n",
       "ms2OiEmSpE4rW4GrtXT8IFZORbtfNLs8t+/4kCRJzdPhpyYlSZKypam5pfM8R0ySJCnHGMQkSZIS\n",
       "MYhJkiQlYhCTJElKxCAmSZKUiEFMkiQpEYOYJElSIgYxSZKkRAxikiRJiRjEJEmSEjGISZIkJWIQ\n",
       "kyRJSsQgJkmSlIhBTJIkKRGDmCRJUiIGMUmSpEQMYpIkSYkYxCRJkhIxiEmSJCViEJMkSUrEICZJ\n",
       "kpSIQUySJCkRg5gkSVIiBjFJkqREDGKSJEmJGMQkSZISMYhJkiQlYhCTJElKxCAmSZKUiEFMkiQp\n",
       "EYOYJElSIgYxSZKkRAxikiRJiRjEJEmSEjGISZIkJWIQkyRJSsQgJkmSlIhBTJIkKRGDmCRJUiIG\n",
       "MUmSpEQMYpIkSYkYxCRJkhIxiEmSJCViEJMkSUrEICZJkpSIQUySJCkRg5gkSVIiBjFJkqREDGKS\n",
       "JEmJFKQuQJIkrV/oHQZQStE6G5RTEefEmVksSa2gxUEshDAUuBHIB+6MMV5dT5ubgGHAMuDkGOOU\n",
       "zP4PgEVANVAZY9ytpfVIktQhlVLEcJau8/0H6ZbFatRKWhTEQgj5wM3A/sAsYGIIYWyM8Z06bQ4G\n",
       "tokxDgwh7A7cCuyReTsCQ2KM5S2pQ5IkqT1q6TViuwHvxhg/iDFWAg8Bh32uzaHAvQAxxglAzxDC\n",
       "pnXeDy2sQZIkqV1q6dRkP+A/dbY/AnZvRJt+wGxWjYg9G0KoBm6LMd7RwnokSer4LucuauhLpJgN\n",
       "uZNzeCB1SWqelgax2Mh26xr12ivG+HEIoRcwLoRQFmP8+1qdQxhVZ3N8jHF808qUJKkDOZDz2ZWF\n",
       "vEcJv+MJJvAkUJm6rM4ohDAEGNLc/i0NYrOA/nW2+7NqxKuhNptn9hFj/Djz37khhD+xaqpzrSAW\n",
       "YxzVwjolSeo4xvM9nuIgAGrowzS2pJB/J66qU8oMDo1fvR1CuLQp/Vt6jdgkYGAIYYsQQhFwLDD2\n",
       "c23GAidmitsDWBBjnB1C6BpC6J7Z3w04EHijhfVIktSx3cVgVrAXJ3EIIzmQAt6iqoHHWiintWhE\n",
       "LMZYFUL4EfA0qx5fcVeM8Z0QwhmZ92+LMT4ZQjg4hPAusBQ4JdN9M+CPIYTVddwfY3ymJfVIktTh\n",
       "VdKdPBYwgJU8ytZUsUvqktR8IcbGXuaVRgghxhi9s1KS1KmFQWEgw1nKJxRyN3dTTX/ymUGkO325\n",
       "ni68Gcvi9NR1dnZNzS0+WV+SpPakD5VczP+utd8HurZLrjUpSZKUiEFMkiQpEYOYJElSIl4jJklS\n",
       "e1BORYPXgZVTkcVq1Eq8a1KSJKmVNDW3ODUpSZKUiEFMkiQpEYOYJElSIgYxSZKkRAxikiRJiRjE\n",
       "JEmSEjGISZIkJWIQkyRJSsQgJkmSlIhLHEkCIPQOAyilqNEdyqmIc+LMNixJkjo8g5ikVUopYjhL\n",
       "G92+oTXvJEmN4tSkpDWVk89l3M8jDKx3W5LUagxiktZUSjVDOIt/cxHl5K+xvcifGZLUmkKMMXUN\n",
       "DWrqKuaSmicMCgObOjUZy+L0NixJktqdpuYW/9+tJElSIgYxSZKkRAxikiRJiRjEJEmSEjGISZIk\n",
       "JWIQkyRJSsQgJkmSlIhBTJIkKRGDmCRJUiIu+i1plXIqmrSQdzkVbViNJHUKLnEkSZLUSlziSJIk\n",
       "qZ0wiEmSJCViEJMkSUrEICZJkpSIQUySJCkRH1+h9Qq9wwBKKUpdx3qVUxHnxJmpy5AkqbEMYlq/\n",
       "UooYztLUZaxXU56BJUlSDjCItbE2HU1yBEiSpHbNINbW2nI0qY1HgOqEyC14iWV8wmZM42qGcFKD\n",
       "HV/hVHrwGoN4lb9zE/34FVsxnfE8wk6cSk8W8wK3sA8/aFZhrzOMvrxCL+Z97p2uYdB6nqFneJUk\n",
       "5RCDmNZtdYh8lWXswjKmsJwPiQxm2Tr7VBIYzM2125Oopi8rGMwyXiYyiOVsyTIGc3Kz65rEgXTn\n",
       "LQbznzX2FwO7rCf0On0pScohBrH25FouIJ8lnM9tCavI51YuYyWDKGIGxzKS+3iUHjzNUvagL/fy\n",
       "Gf9DL17kSP62zqNczz+5gD35lC78mr9TyjQiBQzgFg7nBabQhxe4mS5MYTk7UsAcjuc8/sbXqWB7\n",
       "JnIjf6cnhUzlZ5y+zs/5ORMYykHszoK2OBmSJLWEj69oX9IvDFrNFmzN7zmXo8hjGY9zDAAFLOBs\n",
       "juconmFVneurddX7G7GSwArO5ni+yel8wHl1Pqs/2/Ew53I0+SxmHPtxOM9RyFtU8wk7cXqDIazu\n",
       "50iSlIMcEcumX3EU8zkDiBTyNjtyLa/yC2rYiDzm8TXOZygf8wSbM4Ub1tqfyot8h0ksYVdeYCXL\n",
       "eJUfcCDfp4B3mcUJ5FPEfzicyzmE7vy99taEn/NPipnPP7mKl1hIcSb4v0xfllLC5TxMd/5OpIhf\n",
       "8hCBGpbRl8t5gEgx+Sxlb6YzgT58xq4s5AtcyakUUEIFGzOVUXzAC3RnBgv4EvtkpkQv5176cAun\n",
       "MiHRGZMkqVEcEcuW37Mt8zmbgziakRzI/lzKFC5nQx5mJAfQgz8xmTEAvMZl9e5PZWNeYyE7A1BN\n",
       "ITV0YRn5LGZb8lnKCnqwH+dwAcexjO1ZxmYAREooYAl78lM24FUq6ArAC/yYIiq5mGOpoTcQ+D7f\n",
       "oQ83EangW5zHUM6imgL+XPu5PenOW1zC0XTjYwp5j69xEWfzy3oqdhRMktQuGMSyZRZ70oXHaq9V\n",
       "2pWFVPFVTuJPAAznUarYDWCd+1PZhmmsYDsW0wUoopiPeY4vsZQ9KGQmBaxkMxZRQg29eYoKNsn0\n",
       "rKSYhQBsxDvUZEZgV7IjBVQD0JN3gUgxNXzI4VTRlce5mae5iRqKKKc/AHksoitzM6+XAflZPAOS\n",
       "JLUJg1h2rf1shap69jW0P4ViqinkY95kPwpYQGAT3uQX1LABvXlyjbaRwOoRqUBV7f5AzeeOuqrN\n",
       "9owD8vglD1PJVpQwl2/xIw7mLDbkA05lbKZ/ZW2fzRhLNQN4iyuYTxGBaur+XY6UtObXlySprRjE\n",
       "smVz/sFyvsUEegIwgZ4UMIn7OQyABzmCAl4GWOf++oJctmzAq8zjcLblQg7iJ1RRQQn/Ykdeo4aF\n",
       "VBJYQR5zOYgB3MyRPA/AWZzBvkwDoIRn2ZJFlPA6m3AFAK+xJ4EVnMOxbM31VPMpvVnIznzC3vyI\n",
       "dzLnK48FnMIdABzJ8xTwJttzERtRwUZ8zHK+SA3wV/pSxU7ZP0GSJDWdF+tny9FM51fcxNM8ytNU\n",
       "U8gb7MLFTOYXjOZM8pjHrpk7Bte1v3F3I7aNPkzhM77L3kylFyv5EyvZkCnswDymcBOPcDsQ6M7f\n",
       "Gc6Ldeqta9X2PlzDc1zJ5ZxMD8bX7j+aCdzGltzNvQAElvENLias53sfxutMZxZPcT95vE0BU1vz\n",
       "q0uS1FZCjLl9XXMIIcYYc2earonCoDCwLZ+sH8vi9DY5NnVqf5Ut2aWBh7jmilfpyi6832CbNj5n\n",
       "kqTOram5xRExrV8FFbyaueMxl1VQkboESZKawiCm9duDWalLkCSpIzKItbVyKtpsfcNyR4AkSWrP\n",
       "vEZM6xR6hwGU1j4nv2MopyLOiTNTlyFJ6piamlsMYpIkSa2kqbnF54hJkiQlYhCTJElKxCAmSZKU\n",
       "iHdNKomcuBHAC/clSYkZxJRGKUVttuJAY7XVY0UkSWokpyYlSZISMYip47ubPbiXr6YuQ5Kkz3Nq\n",
       "Uh3fPPYknyXA5NSlSJJUl0FMaZXRhUe5jWo2A/LpxqOsYGcu5jRu5UBmcytnsy2LKeBenmck/8NY\n",
       "BjCVy4lsTGA5X+LHHMEM/kEpL3IV1fQDoD+X0ptPWMYJQDVjOILNuYRTmJj0O0uSlGEQU1p/Y1/y\n",
       "+ZSLORGA19mAv3ACAIvYnXzeYSw7U0MBBbwKwFSuYQcu5DA+4D525m2u4AiO5UXG0Jc7OJmJPEVf\n",
       "JvIAJzOEt/gt+SzhfG5P9j0lSaqHQUxp9eUd5jKSa7iIXozjFCbyGDN5lK2pZCdKuZ157E4kn65M\n",
       "oIwuVPE1pnIbUzPHiJnHYFTydf7DNoyp3d+NMrpktlwmS5KUcwxiSutw3qc/B/Iv9mMWF3Id/6CE\n",
       "l5nJfkAlO/F3xvNLInlsy2hWkk9gISM4qJ6jBb7Ht+hD5Rp7H8vKN5Ekqcm8a1JpPUdvNmElZ/En\n",
       "NuHXrGQHSpnAYk6jmEnsyXxq2IgatuIY/s2OLCGPD7mFbwJQCTzEdgAU8gIP8N3aYz/I9gDks4Rq\n",
       "Nsj6d5MkaT0cEVNa7zGIfzKCQA1QxVZcyGDe5T42ZiMmAFDI21TTq7bPLvyIKVzFaM4BCunKn4F3\n",
       "2JcRPM/ljGYcUEARLwEXsRXjmMrtjOFA+nMJJ3uxviQpN4QYY+oaGhRCiDFGr+/pYMKgMDAXnqwf\n",
       "y+L0pDVIkjqUpuYWpyYlSZIScWpSzdbChbu34CWWtWpBTdc1DKrzf1pcBFySlGUGMTXf6oW7X6Qf\n",
       "hU0MZAvpzjJKW6WOSirpzewm99uSBexRZ3rURcAlSVlmEFPLFVLE4CaPbr3fap//El3ZpRWPJ0lS\n",
       "lniNmLLvAY7kYQ5u1WNexh+4nx3W2n8Tx3AVl7XqZ0mS1EoMYmo7N3MGv84sV1TXd3iUY3my0ce5\n",
       "kjt4KvOssHVb1+2/uX1bsCSpU3NqUq3j9xzMpxwHFFDCm5zEVSziVIp4lV/yTQIrGcp5bMt87uYM\n",
       "CljKifyOF9iW17mYSDEFfMQhjGIOPfkXV3M2x2eOXszbXMYwjuQeTmMxX6eGErryOqdzeW0NH3EU\n",
       "Y7iOSAFbcj7/y+tr1FjfouAnMylLZ6jDaOFNGk3nTRSSOjCDmFruQ77AZxzAGZzMnZzCp5zIdexK\n",
       "pJACPmEZ+XTnE/7FEdTwKB/xHbbkDu7mED7mJ+TxAdVsQFeKeJhfUEE3aticJ9iZbzKFGjammllc\n",
       "zoNECtmFkRzM2/ya0fyZr7Mpk4FApIQRHMQ97Mb73ADsR901JutbFByGpDlp7djqmzSyxZsoJHVg\n",
       "Tk2q5T7ja1SwHTfzB+bxPbozj015lmoiJbwHQAnvsZK+a/SroZhqSjiF0ziaE1jCzkT6cjHfoYTX\n",
       "eIcfUEmghlLy+ZiLGU5vnmISd/JLHmY5u7KQrTJHi/TizwCcwitEujOF7mt83qpFwS9jDE8ziXuI\n",
       "dGM6JW1+fiRJWgdHxNQ6evA4VZRTTHd+xG0AXM5Jte8HIrGe4F/ASvqwgj6sII9lFLIEgI0YzxzO\n",
       "5gn2JrCULXmM+RSxhOMILOLbnMpzHE9NA1NkBdR8bk/9i4IrdzzB5kzmXkayX+pSJCkbWjwiFkIY\n",
       "GkIoCyFMDyFcuI42N2Xefz2EsHNT+qod2IRJLGY/qikBAu/Tg9fYrPb9QDUxM0W4tE5wymMlgZU8\n",
       "zY4AREroyqsA5FNBHrN5n5+Rx2cALK7tW8MiSljIAXWqCMzlUADuYVcCC9nhc9Nn61oUXJKkRFo0\n",
       "IhZCyAduBvYHZgETQwhjY4zv1GlzMLBNjHFgCGF34FZgj8b0VTvxBT4kn1v4kO+zhL6MZS+25kaq\n",
       "yAegiI9ZyuYUA5PYf42+xbzCNM6jjBKgkF24D4BIoICZwJcJLGYmB3EEkynkn0SG8TeupYQ36xwp\n",
       "EljJGJ4mks9WnF9n/6o7J9e1KLia7zrOZRlHEJhHPh9TwlT68A/e5SoiJeQzk6Gcz1dZxINsX+/+\n",
       "+9mBGdzAqij+QuqvJEnZ1NKpyd2Ad2OMHwCEEB4CDgPqhqlDgXsBYowTQgg9QwibAVs2oq/ai6MY\n",
       "B4zjFk6lnEN4gzPpxlMAfI37+DtXs4QaejAHyKOQBVQB+cznbH4GwBgeo2dmFCsQqWYTSvkLK/gq\n",
       "gZVczgNAPl/jFA7m7drPfomuXMLR9dZ1No8AjwAwmPkM5gdtdAY6n9+yI8sZxmnsx3wK+QNPA1OZ\n",
       "zo3052JO4RWu5QKe5Xy+yiim80v6c9Fa+2fwC/rzM05hItdwceqvJUnZ1NIg1g/4T53tj4DdG9Gm\n",
       "H9C3EX3V3vyAu4G719q/N8cBcDdnEnmPPXiBLVgMPF7bZgSH1L5exr6U0IV9eZAtM9ecKbfMZVdK\n",
       "eJo+VNKHSooZRw1diWzIKbwCwCAe4TVu53U2INJjrf1T6J7ZPxGAATzKNL6R7DtJUpa1NIg19mGZ\n",
       "Yf1NGugcwqg6m+NjjONbcjwldCq3smp6umE/5IK2L0YtFFnfv+24jvebul+SclQIYQgteBRSS4PY\n",
       "LKB/ne3+rBrZaqjN5pk2hY3oC0CMcVQL61RbKKci84ynrhQnrOMDuvJBKzxrqpyKVqim8+jNRN7n\n",
       "Gj7iVyyigJXszwbcT2AB97ArpzCRaRxFEf9iR5bwl3r278xiHmMhv2FXTmYiH3JE6q8lSU2RGRwa\n",
       "v3o7hHBpU/q3NIhNAgaGELYAPgaOBYZ/rs1Y4EfAQyGEPYAFMcbZIYR5jeirHLb6aeehd6hgWhaf\n",
       "tP55Pnk9jROYyrU8w908Sx6fUUAZ+SxkIOfyLlcxmi7kM5ODOQ9gnfu35jxmcANjiBTzIi5LJakT\n",
       "CTG27GdeCGEYcCOQD9wVY7wyhHAGQIzxtkybm4GhwFLglBjjq+vqW8/xY4zR6QopR4RBYWDtk/XL\n",
       "6MIgljOdEh7ij2zDjxnOW636gQ/SLZbF6a16TElqI03NLS0OYm3NICblljWC2BXcTDUDiZTQjYe5\n",
       "gFta/QMNYpLakabmFp+sL6n5LuJHqUuQpPbMICapaf57k0b2Pk+SOiinJiVJklpJU3NLi9ealCRJ\n",
       "UvMYxCRJkhLxGjGpHqF3GEBpwmejreYz0iSpQzOISfUppaj2EQ0pZfOieElS1hnEJEmSWqh2JqUw\n",
       "87xFaNSshteISQ15gs0ZzXPN7n8tF3Ane7ViRZKkXLR6JqUEGM5ShrO0MZe4OCImtZVlBH7M9anL\n",
       "kCTlLoOYtH4FXMGvqGIHCpjGcZzD3/ganzCCSD6FvM7J/JQ+VPJzJtCFv7CSvSnlFhaxLxsyjh/w\n",
       "JD9nAl35PSs4gEgBX+YMjmAG/6CU8dxCDb0pZDIV7M1QDmJ3FqT+4pKktuXUpLQ+NWxNP37DSIYQ\n",
       "WMIfOYNZ/IKvcAYj2R/I50FOzLSO5FPOCIbyQ8YCMfNn1XsFzGMEQ+nBfUzj+wD8g/PpwouM5Bts\n",
       "zONE+mX/S0qSUjCISesT+JiTmAxAbx5lBXuSz0wO4wMAevEIy9mjtv32jF3nsb7CkwCU8gbV9Aeg\n",
       "kl3Zjr8AcDovEBwJk6TOwiAmrV+s8yoQWATUXb4iEOq02Yhl6zzSBpl1E/OoBvLXOK4kqdMxiEnr\n",
       "E+nHfewCwFy+TTGvU01/xjIgs+9IuvBSs49fyETKOBSAO9ibSM+WFy1Jag8MYlJDApE8ZjCLkxnN\n",
       "eCLdOZrRXcEXAAAXuklEQVTb6cd5vM7tjOZZoJoT+G2mR2zocGscd3XbvbiB5ezNaJ5jHt8iMIet\n",
       "WNIm30eSlFNCjI37vZFKU1cxl1pDGBQGZu3J+p9QSDeq6UEN9/JVPuQKRnAQAA/SLZbF6Q3W2pbL\n",
       "MbnEkiQ1Su3vjTHMYi92Yl/mNuZnuI+vkFJ7hX5M5bbM9WeVbMX/Nal/Wy7HlOUllhodKg2IknJV\n",
       "V2Bf5ja2uUFMSu0wPuCwzAhYZ9fYUOkanJI6CIOYVJ9yKnLil3155i7LtnQZf2BLRnE8b/JzJvgw\n",
       "WUnKHoOYVI9ONu0V1/FaktTGDGJSR3E9Z5LHCs7jHq5mFJVsxyUcy53syRyOoxe/51P+Dygmnw84\n",
       "gvMYxPLUZUtSh7B6JmUF/718ohGzGj6+QuooevEyy9kdgAp2JNKVcvKZz+4U8w6zOZfjOJYRDKWY\n",
       "qTzOGYkrlqQOI86JM2NZnE4lxLI4PZbF6Y2ZXXFETOoohvEGt/IV3qAbgZUU8jqPsyMr2Y1uPE01\n",
       "2/JQZimlSBGFTEpcsSR1egYxqaPoRRV5fMizHEMxE+nGO8xhT6rZgi78h+W8yEX8MHWZjXIjJ7GY\n",
       "7wAQKCdSSiGv81N+krgySWpVBjGpIynmFRZxJv04j20p43l+TgGvsQOTeZYrGMsADmUmZXShjM04\n",
       "nPdTl1yvc7kXuDd1GZLU1gxiUkeyERNYyll8g8lsxQrGs5wuTGBP5lPGubzOLbyWeWBqb66GHA1i\n",
       "ktRJuMSR1M616XJMjVieozU1+rtkuS5Jaqym5hbvmpQkSUrEICZJkpSI14hJ7V1bLseUjSWWJKkT\n",
       "8xoxSTkj9A4DKM3cTNCQcio62TJUktqJpuYWg5gkSVIr8WJ9SZKkdsIgJkmSlIhBTJIkKRGDmCRJ\n",
       "UiIGMUmSpER8jpiUQxr9+IbW5KMgJCkZg5iUS0oparN1I9elrR4GK0laL4OYWizJKE5djuhIktop\n",
       "g5haLsUoTl2O6DTerRxIL97jKN5NXYokyYv1pfZrUTP+/S5gGHPYtg2qkSQ1gyNiyk2O3MB1nMsy\n",
       "jiAwj3w+poSpLGN/CnmLCnajG39iU17mfS4l0o1AOXtyLvsyl1/yHRZxPJFCCviA4zibF/kyFRzA\n",
       "XPZgDOewE6dxCB+m/pqS1JkZxJSbFjCMyDjopEHst+zIcoZxGvsxn0L+wNPAVAAiBYzgYMrJ5//x\n",
       "R/bnZAYzn//HobzMT9mXC9iDJ9mdBwC4lh/zF4ZzHvdwJc+wIeP4AU8l/HaSpAyDmFrP9ZxJHis4\n",
       "j3u4mlFUsh2XcCx3sidzOI48FlPJTkRK6MIT/JjrAbiGi1jBAUAVJbzApjy1xsjNznyPGvKYyuVE\n",
       "NiawnC/xY45gRtov3IbmsislPE0fKulDJcWMq31vU8YC8CzbUM0XeZaHeBaI5JPHbADeYhDPcCGR\n",
       "7kS6UczzdY7e6MVoJUltyyCm1tOLl/mIM4B7qGBHoIBy8pnP7nTjJf6HJ9iVhSwij5t4mIcZxBeY\n",
       "zQqGMpK9AXidDdiRJWuN3FzGw+zAhRzGB9zHzrzNFRzBsem+bJuLrCswFbEs0yKQzzRGcNhabf7D\n",
       "jQziZI6ljJs4mqUM/tyxJUk5wIv11XqG8QZVfIU36EZgJUVM5nF2ZCW70Y9X+CeHMoa/ciNPU80X\n",
       "+YyBbM9CYAVXcj23MpS+rKhzxFVB5G26UsXXmMptjOFpZnIVNfRO8h2zpTcTWcEBfEQRb9OVlexf\n",
       "591V52VvZhDZmPvYBYC5FPAIAzNtutGPOcylgEUcyerwlcdSquievS8iSWqII2JqPb2oIo8PeZZj\n",
       "KGYi3XiHOexJNVvQhRUs4gwOYRg7s5gruYFqSuhBDafxTcayF+V8i9s4hUtqR7pWhYdK8ggsZAQH\n",
       "pftyWXYCU7mWZ7ibZ8njMwooI59FrDonq85LHyoZyOnMYAyj6Q4UsCG3A9PZkGt4jid4nnkUMYUa\n",
       "ugKwKX/hQ65lDKeyE6d7sb4kpWUQU+sq5hUWcSb9OI9tKeN5fk4Br7GY7sAyvsxixrMJFXyDDfgX\n",
       "ZXRhDl05g+eZwiTG8hKw5sjNjixhLB9yC9/kBzxBJfAo23Ec76T7ollwCLcyiBuYTgkP8Ud6M5Vz\n",
       "eHCNNsN5Gzhyrb7n8lvgt2vtP5lJwL5tU7AkqakMYmpdGzGBpZzFN5jMVqxgPMvpwgSO4x2u5E2u\n",
       "5EXy+JhCXgHgEzbgH9zDeIqBQCmXAmuO3OzMaezCj5jCVYzmHKCQrvwZOngQ+yPXUs1AIiV042GG\n",
       "81bqkiRJrSvEmNvX7YYQYozRu7xyWBgUBqZ+sn4si9OTfX4rSnIuO9D5k6TUmppbHBGTckk5FVlf\n",
       "sqmciqx+niSplkFMyiEuXi5JnYtBTC2XYhTn858vSVI75DVikiRJraSpucUHukqSJCViEJMkSUrE\n",
       "a8QkNUroHQZQSlGrHrScCm9QkNSZGcQkNU4pRa3+jLOUN3lIUg5walKSJCkRg5ikpnmCzRnNc1nv\n",
       "K0kdkEFMkiQpEa8Rk9QcBVzBr6hiBwqYxnGcwx85k+XsT6SEIibxUy4E4H52YAY3AJESXkhbtiTl\n",
       "FkfEJDVdDVvTj98wkiEElvBnTmIf7mYE32Qk+xEp4dfsD8AMfkF/LmIkByauWpJyjiNikpou8DEn\n",
       "MRmA3jzKp3yXifyHv/IDIiVEerKUaUzmFSI9OIWJAAzgUabxjZSlS1IucURMUnPEOq8CEJnLFezB\n",
       "9xjJ/nTlASLF5BE/18vlyiSpDoOYpKaL9OM+dgFgLt+mC68AsC3zeZuuLOdbAOzMYgIL+Q27AvAh\n",
       "R6QpWJJyk1OTkpomEMljBrM4mdHcQAHTOJz7eJQNuZe/EZhLIVNq22/NeczgBsYQKeZF+NwomSR1\n",
       "YiHG3P6Z2NRVzCW1jTAoDGyLJ+vHsji9VY8pSQk1Nbc4NSlJkpSIU5OScsZ6FxZ3kXBJHYxBTFLj\n",
       "lFPR6ot0l1Oxxvb6FhZ3kXBJHYxBTFKjOBIlSa3Pa8QkSZISMYhJyj0/52UAnmBzLuORxNVIUpsx\n",
       "iEmSJCXiNWKSck8enwGQTw15zE9cTaOt967PhnhHqNQpGcQk5Z4RmSWShvIxQzk9cTWNt767Phvi\n",
       "HaFSp+TUpCRJUiIGMUmSpESaHcRCCKUhhHEhhH+HEJ4JIfRcR7uhIYSyEML0EMKFdfaPCiF8FEKY\n",
       "kvkztLm1SJIktUctuUbsp8C4GOM1mYD108yfWiGEfOBmYH9gFjAxhDA2xvgOEIEbYow3tKAGScpN\n",
       "l3MXNfQlUsyG3Mk5PMAoptONO1jO/gRWsA+nsDfzUpcqKZ2WTE0eCtybeX0vcHg9bXYD3o0xfhBj\n",
       "rAQeAg6r836jVyeXpHblQM5nBMM4gYNZyHeZQE+gCz2YzEgOpJgJvMLxqcuUlFZLRsQ2jTHOzrye\n",
       "DWxaT5t+wH/qbH8E7F5n+6wQwonAJOCCGOOCFtQjSbljPN/jKQ4CoIY+TGNLoIIzeA6AbkxlEXsn\n",
       "rFBSDmgwiIUQxgGb1fPWxXU3YowxhBDraVffvtVuBUZnXo8Brge+u446RtXZHB9jHN/AcSW1V+tb\n",
       "WPzzi4TnqrsYzAr24iQOYQAruYxHqKIYqKptE6gB8pPVKKlVhBCGAEOa27/BIBZjPKCBD54dQtgs\n",
       "xvhpCKEPMKeeZrOA/nW2+7NqVIwYY237EMKdwGMN1DGqoToldQwd5oGmlXQnjwUMYCV/YBuq2CV1\n",
       "SZLaRmZwaPzq7RDCpU3p35JrxMYCJ2VenwT8uZ42k4CBIYQtQghFwLGZfmTC22rfBt5oQS2SlDsO\n",
       "43mggNGMZxo/o4DJmXfqzhJEGp41kNQJhBib93MghFAK/B74AvABcEyMcUEIoS9wR4zxm5l2w4Ab\n",
       "WTUEf1eM8crM/vuAnVj1g+h94Iw615zV/ZwYY/Sifkk5LwwKA1vyZP1YFqe3ckmSsqypuaXZQSxb\n",
       "DGJKqUVrB7aUaw+2OwYxSU3NLa41KTWkJWsHtpRrD0pSh2cQk6TWsr67PtfXV1KnYxCTsukGvsfh\n",
       "/I6tWAHAKKYzioGJq1IrcSpZUlO56LeULYvIYzHfYzZd6uzN7Ys0JUltyhExqTGu50zyWMF53MPV\n",
       "jKKS7biEY7mTPZnDcfTgWco5CwiU8Bw/4Qpg1YhXF+5jJV+nK08S2ZRxPMJzzOMSjgXgWn7i2oOS\n",
       "1Dk5IiY1Ri9eZnlmea4KdiTSlXLymc/uFPEe87iYAziaczmACnbkVg7M9OxCd15lJAfyf9xIYDYH\n",
       "cVRtCIOurj0oSZ2XQUxqjGG8QRVf4Q26EVhJEZN5nB1ZyW7ks5Ai/slg5tODGnrwJxaxR6ZnNafx\n",
       "RANHXnPtwco1VqKQJHVwBjGpMXpRRR4f8izHUMxEuvMKc9iTaragCx8B/31mTKzzv7CSwgaP7NqD\n",
       "ktSJGcSkxirmFRZxJhvxMl9mAks5kQLe4ItMoYLBvMRGmQvyD2dDXq73GIElfMYGWa5ckpSjDGJS\n",
       "Y23EBCK9+AaT2Zt5BJbThQnsy1w25grG8Qg3Mo4iXuf7jMv0WvOuyO7cz2Qe4DIerud91x6UpE7G\n",
       "JY6kBrRoyZqWcskbSWp3mppbHBGTJElKxCAmSZKUiA90lRrSkrUDW+OzJUkdmteISZIktRKvEZMk\n",
       "SWonDGKSJEmJGMQkSZISMYhJkiQlYhCTJElKxCAmSZKUiEFMkiQpEYOYJElSIgYxSZKkRAxikiRJ\n",
       "iRjEJEmSEjGISZIkJWIQkyRJSsQgJkmSlIhBTJIkKRGDmCRJUiIGMUmSpEQMYpIkSYkYxCRJkhIx\n",
       "iEmSJCViEJMkSUrEICZJkpSIQUySJCkRg5gkSVIiBjFJkqREDGKSJEmJGMQkSZISMYhJkiQlYhCT\n",
       "JElKxCAmSZKUiEFMkiQpEYOYJElSIgYxSZKkRAxikiRJiRjEJEmSEjGISZIkJWIQkyRJSsQgJkmS\n",
       "lIhBTJIkKRGDmCRJUiIGMUmSpEQMYpIkSYkYxCRJkhIxiEmSJCViEJMkSUrEICZJkpSIQUySJCkR\n",
       "g5gkSVIiBjFJkqREDGKSJEmJGMQkSZISMYhJkiQlYhCTJElKpCB1AZJyR+gdBlBK0Ro7y6mIc+LM\n",
       "RCVJUodmEJP0X6UUMZyla+x7kG6JqpGkDs+pSUmSpEQMYpLWdi0XcANnpC5Dkjo6g5ik+sTUBUhS\n",
       "Z2AQkyRJSsQgJkmSlIhBTNK6OD0pSW3Mx1dIWtuPuSF1CZLUGTgiJkmSlIhBTNLabuQEfsWRqcuQ\n",
       "pI7OqUlJazuX36UuQZI6A0fEJEmSEml2EAshlIYQxoUQ/h1CeCaE0HMd7e4OIcwOIbzRnP6SJEkd\n",
       "VYixeXeohxCuAT6LMV4TQrgQ2CjG+NN62n0dWALcF2PcoRn9Y4wxNKtISU0SeocBlFK0xs5yKuKc\n",
       "ODNRSZLUrjQ1t7QkiJUB+8QYZ4cQNgPGxxgHraPtFsBjnwtijepvEJMkSe1FU3NLS64R2zTGODvz\n",
       "ejawaZb7S5IktWsN3jUZQhgHbFbPWxfX3YgxxhBCs5/Cvb7+IYRRdTbHxxjHN/ezJEmSWksIYQgw\n",
       "pNn9Wzg1OSTG+GkIoQ/wfDOmJtfb36lJSZLUXmRzanIscFLm9UnAn7PcX5IkqV1ryYhYKfB74AvA\n",
       "B8AxMcYFIYS+wB0xxm9m2j0I7ANsDMwBRsYY71lX/3o+xxExSZLULmTtrslsMYhJkqT2IptTk5Ik\n",
       "SWoBg5gkSVIiBjFJkqREDGKSJEmJGMQkSZISMYhJkiQlYhCTJElKxCAmSZKUiEFMkiQpEYOYJElS\n",
       "IgYxSZKkRAxikiRJiRjEJEmSEjGISZIkJWIQkyRJSsQgJkmSlIhBTJIkKRGDmCRJUiIGMUmSpEQM\n",
       "YpIkSYkYxCRJkhIxiEmSJCViEJMkSUrEICZJkpSIQUySJCkRg5gkSVIiBjFJkqREDGKSJEmJGMQk\n",
       "SZISMYhJkiQlYhCTJElKxCAmSZKUiEFMkiQpEYOYJElSIgYxSZKkRAxikiRJiRjEJEmSEjGISZIk\n",
       "JVKQugBJ6uhC7zCAUorW2aCcijgnzsxiSZJyhEFMktpaKUUMZ+k633+QblmsRlIOcWpSkiQpEUfE\n",
       "JClbHmVr3uZ6IhuQxwL24zQGMz91WZLScURMkrIlALvwI0ayP0VM4p/8b+qSJKXliJgkZcsRzKh9\n",
       "HSkmj/KE1UjKAY6IZVkIYUjqGjoSz2fr8ny2rnWez9vZh5Xsy348kN2K2jf/frYuz2frau75NIhl\n",
       "35DUBXQwQ1IX0MEMSV1ABzNkrT3LCHzCdezESezIkuyX1K4NSV1ABzMkdQEdzJDmdDKISVI2/Z3N\n",
       "CCziUHxumCSDmCRl1VbMZzNGpy5DUm4IMcbUNTQohJDbBUrS+hQCJZnXNcBKoEud91cAldkuSlJb\n",
       "iTGGxrbN+SAmSe1dGBQGru/J+rEsTs9iSZJyhFOTkiRJiRjEJEmSEvGBrpLU1sqpaHBh73IqsliN\n",
       "pBziiFgbCiGUhhDGhRD+HUJ4JoTQs4G2+SGEKSGEx7JZY3vTmHMaQugfQng+hPBWCOHNEMLZKWrN\n",
       "ZSGEoSGEshDC9BDChetoc1Pm/ddDCDtnu8b2ZL3ncy57MY0/MI1HmcZvmEaXWBan1/6ZE32URR2N\n",
       "+fuZabdrCKEqhHBENutrbxr5731I5nfQmyGE8VkusV1Z3/kMIWwYQngshPBa5nye3NDxDGJt66fA\n",
       "uBjjtsBzme11OQd4G/DuiYY15pxWAufFGLcH9gB+GELYLos15rQQQj5wMzAU+BIw/PPnJ4RwMLBN\n",
       "jHEgcDpwa9YLbScacz6B94C9Y4xfAcYAt2e3yvajkedzdburgb+yahVP1aOR/957Av8POCTG+GXg\n",
       "qKwX2k408u/nD4E3Y4w7seohr9eHENY5A2kQa1uHAvdmXt8LHF5foxDC5sDBwJ34A2V91ntOY4yf\n",
       "xhhfy7xeArwD9M1ahblvN+DdGOMHMcZK4CHgsM+1qT3P8f+3d/cgctRxGMe/DxKRIIeKkGhykigh\n",
       "YJGggoqxCSrEFyxVJCbGFCJErBRfwMZCO1MIIkdAxEIkEb0iRAIWNjFwGLxCLSKKieLF+IZIigQf\n",
       "i/kLqzd7+4dzd27w+VQ7O7PDj2dnd34z/5ld+zhwmaQ1ky2zN0bmafuY7d/K5HFg/YRr7JOa7RPg\n",
       "SeAg8OMki+uhmjwfBg7ZPg1g++yEa+yTmjz/BKbK4yngJ9sXhq0wjdh4rbG9UB4vAMN2ZK8CT9O8\n",
       "ebG02kwBkLQBuIFm5xeNdcCpgenT5blRy6R5aFeT56C9wOGxVtRvI/OUtI5m5/f3mdqMJAxXs31u\n",
       "Aq4ol3TMSXpkYtX1T02erwHXS/oe+IxmxGuoXKy/TJKOAmtbZr0wOGHbbT9OK+k+4IztE/kD1sZy\n",
       "Mx1Yz6U0R8xPlTNj0ajdaf377Gx2du2qc5G0HXgM2Da+cnqvJs/9wLPlO0BkJGEpNXmuAm4E7gBW\n",
       "A8ckfWLnt+1a1OS5A/jU9nZJ1wFHJW21/XvbwmnElsn2XcPmSVqQtNb2D5KuAs60LHYbcH+5JucS\n",
       "YErSW7Z3jankFe8/yBRJq4BDwNu23x9TqX31HTA9MD1Nc1S31DLry3OxWE2eSNoCzAA7bP8yodr6\n",
       "qCbPm4B3mh6MK4G7JZ23PTuZEnulJs9TwFnb54Bzkj4GtgJpxBaryfNR4GUA219J+hrYDMy1rTBD\n",
       "k+M1C+wuj3cDixoC28/bnra9EXgI+Oj/3IRVGJlpOUI+AHxue/8Ea+uLOWCTpA2SLgYepMl10Cyw\n",
       "C0DSrcCvA0PC8U8j85R0DfAesNP2yQ5q7JORedq+1vbG8r15EHgiTdhQNZ/3D4Dby937q4FbaG4e\n",
       "i8Vq8vwWuBOgXFu7meaGnVY5IzZerwDvStoLfAM8ACDpamDG9r0tr8nwz9JqMt0G7ATmJZ0or3vO\n",
       "9pEO6l1xbF+QtA/4ELgIOGD7C0mPl/lv2D4s6R5JJ4E/gD0dlryi1eQJvAhcDrxezuKct31zVzWv\n",
       "ZJV5RqXKz/uXko4A8zTXKs/YTiPWonL7fAl4U9I8zbD5M7Z/HrbO/NdkREREREcyNBkRERHRkTRi\n",
       "ERERER1JIxYRERHRkTRiERERER1JIxYRERHRkTRiERERER1JIxYRERHRkb8A4izqWyrfLUgAAAAA\n",
       "SUVORK5CYII=\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12f75080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the word vectors you trained\n",
    "\n",
    "_, wordVectors0, _ = load_saved_params()\n",
    "wordVectors = (wordVectors0[:nWords,:] + wordVectors0[nWords:,:])\n",
    "visualizeWords = [\"the\", \"a\", \"an\", \",\", \".\", \"?\", \"!\", \"``\", \"''\", \"--\", \"good\", \"great\", \"cool\", \"brilliant\", \"wonderful\", \"well\", \"amazing\", \"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"waste\", \"dumb\", \"annoying\"]\n",
    "visualizeIdx = [tokens[word] for word in visualizeWords]\n",
    "visualizeVecs = wordVectors[visualizeIdx, :]\n",
    "temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n",
    "covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n",
    "U,S,V = np.linalg.svd(covariance)\n",
    "coord = temp.dot(U[:,0:2]) \n",
    "\n",
    "for i in xrange(len(visualizeWords)):\n",
    "    plt.text(coord[i,0], coord[i,1], visualizeWords[i], bbox=dict(facecolor='green', alpha=0.1))\n",
    "    \n",
    "plt.xlim((np.min(coord[:,0]), np.max(coord[:,0])))\n",
    "plt.ylim((np.min(coord[:,1]), np.max(coord[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis\n",
    "\n",
    "Now, with the word vectors you trained, we are going to perform a simple sentiment analysis.\n",
    "\n",
    "For each sentence in the Stanford Sentiment Treebank dataset, we are going to use the average of all the word vectors in that sentence as its feature, and try to predict the sentiment level of the said sentence. The sentiment level of the phrases are represented as real values in the original dataset, here we'll just use five classes:\n",
    "\n",
    "    \"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"\n",
    "    \n",
    "which are represented by 0 to 4 in the code, respectively.\n",
    "\n",
    "For this part, you will learn to train a softmax regressor with SGD, and perform train/dev validation to improve generalization of your regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, implement some helper functions\n",
    "\n",
    "def getSentenceFeature(tokens, wordVectors, sentence):\n",
    "    \"\"\" Obtain the sentence feature for sentiment analysis by averaging its word vectors \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement computation for the sentence features given a         #\n",
    "    # sentence.                                                       #\n",
    "    # Inputs:                                                         #\n",
    "    #   - tokens: a dictionary that maps words to their indices in    #\n",
    "    #             the word vector list                                #\n",
    "    #   - wordVectors: word vectors for all tokens                    #\n",
    "    #   - sentence: a list of words in the sentence of interest       #\n",
    "    # Output:                                                         #\n",
    "    #   - sentVector: feature vector for the sentence                 #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    idx = [tokens[word] for word in sentence]\n",
    "    sentVector = np.mean(wordVectors[idx, :], axis = 0)\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return sentVector\n",
    "\n",
    "def softmaxRegression(features, labels, weights, regularization = 0.0, nopredictions = False):\n",
    "    \"\"\" Softmax Regression \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement softmax regression with weight regularization.        #\n",
    "    # Inputs:                                                         #\n",
    "    #   - features: feature vectors, each row is a feature vector     #\n",
    "    #   - labels: labels corresponding to the feature vectors         #\n",
    "    #   - weights: weights of the regressor                           #\n",
    "    #   - regularization: L2 regularization constant                  #\n",
    "    # Output:                                                         #\n",
    "    #   - cost: cost of the regressor                                 #\n",
    "    #   - grad: gradient of the regressor cost with respect to its    #\n",
    "    #           weights                                               #\n",
    "    #   - pred: label predictions of the regressor (you might find    #\n",
    "    #           np.argmax helpful)                                    #\n",
    "    ###################################################################\n",
    "    \n",
    "    prob = softmax(features.dot(weights))\n",
    "    if len(features.shape) > 1:\n",
    "        N = features.shape[0]\n",
    "    else:\n",
    "        N = 1\n",
    "    # A vectorized implementation of    1/N * sum(cross_entropy(x_i, y_i)) + 1/2*|w|^2\n",
    "    cost = np.sum(-np.log(prob[range(N), labels])) / N \n",
    "    cost += 0.5 * regularization * np.sum(weights ** 2)\n",
    "    \n",
    "    ### YOUR CODE HERE: compute the gradients and predictions\n",
    "    labels_matrix = np.zeros_like(prob)\n",
    "    labels_matrix[range(N), labels] = 1\n",
    "    grad = features.T.dot(prob - labels_matrix) / N + regularization * weights\n",
    "    pred = np.argmax(prob, axis = 1)\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    if nopredictions:\n",
    "        return cost, grad\n",
    "    else:\n",
    "        return cost, grad, pred\n",
    "\n",
    "def precision(y, yhat):\n",
    "    \"\"\" Precision for classifier \"\"\"\n",
    "    assert(y.shape == yhat.shape)\n",
    "    return np.sum(y == yhat) * 100.0 / y.size\n",
    "\n",
    "def softmax_wrapper(features, labels, weights, regularization = 0.0):\n",
    "    cost, grad, _ = softmaxRegression(features, labels, weights, regularization)\n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for softmax regression ====\n",
      "Gradient check passed!\n",
      "\n",
      "=== For autograder ===\n",
      "(1.9092154575518125, array([[ 0.13173772,  0.01001059,  0.06655011,  0.14794558,  0.07499602],\n",
      "       [-0.10829203, -0.07491409,  0.00833132,  0.09103345,  0.02177026],\n",
      "       [ 0.02546566, -0.07095996,  0.0082149 , -0.00280093, -0.07882097],\n",
      "       [ 0.12213369,  0.23722807, -0.03415194, -0.05804006, -0.03605968],\n",
      "       [ 0.16897372,  0.07964471, -0.06675108,  0.00196775, -0.25084671],\n",
      "       [ 0.00798661, -0.03201922,  0.28859272,  0.11965083,  0.09338442],\n",
      "       [-0.1494036 , -0.16463204, -0.23312905, -0.142476  ,  0.0861008 ],\n",
      "       [ 0.10750012,  0.06379092, -0.0373871 ,  0.10858575, -0.11358543],\n",
      "       [-0.25326338,  0.17415493,  0.07478631,  0.09250239,  0.0198438 ],\n",
      "       [-0.05522526, -0.0028503 ,  0.11080386, -0.05959559,  0.10286727]]), array([0, 1, 0, 1, 0, 0, 1, 0, 0, 1], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# Gradient check always comes first\n",
    "random.seed(314159)\n",
    "np.random.seed(265)\n",
    "dummy_weights = 0.1 * np.random.randn(dimVectors, 5)\n",
    "dummy_features = np.zeros((10, dimVectors))\n",
    "dummy_labels = np.zeros((10,), dtype=np.int32)    \n",
    "for i in xrange(10):\n",
    "    words, dummy_labels[i] = dataset.getRandomTrainSentence()\n",
    "    dummy_features[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "print \"==== Gradient check for softmax regression ====\"\n",
    "gradcheck_naive(lambda weights: softmaxRegression(dummy_features, dummy_labels, weights, 1.0, nopredictions = True), dummy_weights)\n",
    "\n",
    "print \"\\n=== For autograder ===\"\n",
    "print softmaxRegression(dummy_features, dummy_labels, dummy_weights, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 100th iteration, cost function is 1.5702587946\n",
      "After 200th iteration, cost function is 1.5700570043\n",
      "After 300th iteration, cost function is 1.56986049817\n",
      "After 400th iteration, cost function is 1.56966906437\n",
      "After 500th iteration, cost function is 1.56948250179\n",
      "After 600th iteration, cost function is 1.56930061944\n",
      "After 700th iteration, cost function is 1.56912323589\n",
      "After 800th iteration, cost function is 1.56895017876\n",
      "After 900th iteration, cost function is 1.5687812842\n",
      "After 1000th iteration, cost function is 1.56861639644\n",
      "After 1100th iteration, cost function is 1.56845536734\n",
      "After 1200th iteration, cost function is 1.56829805596\n",
      "After 1300th iteration, cost function is 1.56814432818\n",
      "After 1400th iteration, cost function is 1.56799405632\n",
      "After 1500th iteration, cost function is 1.56784711879\n",
      "After 1600th iteration, cost function is 1.56770339978\n",
      "After 1700th iteration, cost function is 1.56756278889\n",
      "After 1800th iteration, cost function is 1.56742518092\n",
      "After 1900th iteration, cost function is 1.56729047551\n",
      "After 2000th iteration, cost function is 1.56715857695\n",
      "After 2100th iteration, cost function is 1.56702939386\n",
      "After 2200th iteration, cost function is 1.56690283902\n",
      "After 2300th iteration, cost function is 1.56677882912\n",
      "After 2400th iteration, cost function is 1.56665728457\n",
      "After 2500th iteration, cost function is 1.56653812926\n",
      "After 2600th iteration, cost function is 1.56642129044\n",
      "After 2700th iteration, cost function is 1.56630669849\n",
      "After 2800th iteration, cost function is 1.56619428681\n",
      "After 2900th iteration, cost function is 1.5660839916\n",
      "After 3000th iteration, cost function is 1.56597575176\n",
      "After 3100th iteration, cost function is 1.56586950875\n",
      "After 3200th iteration, cost function is 1.56576520644\n",
      "After 3300th iteration, cost function is 1.56566279099\n",
      "After 3400th iteration, cost function is 1.56556221076\n",
      "After 3500th iteration, cost function is 1.56546341614\n",
      "After 3600th iteration, cost function is 1.56536635954\n",
      "After 3700th iteration, cost function is 1.5652709952\n",
      "After 3800th iteration, cost function is 1.56517727914\n",
      "After 3900th iteration, cost function is 1.56508516907\n",
      "After 4000th iteration, cost function is 1.56499462432\n",
      "After 4100th iteration, cost function is 1.56490560573\n",
      "After 4200th iteration, cost function is 1.5648180756\n",
      "After 4300th iteration, cost function is 1.5647319976\n",
      "After 4400th iteration, cost function is 1.56464733675\n",
      "After 4500th iteration, cost function is 1.56456405929\n",
      "After 4600th iteration, cost function is 1.56448213267\n",
      "After 4700th iteration, cost function is 1.56440152548\n",
      "After 4800th iteration, cost function is 1.5643222074\n",
      "After 4900th iteration, cost function is 1.56424414914\n",
      "After 5000th iteration, cost function is 1.56416732241\n",
      "After 5100th iteration, cost function is 1.56409169984\n",
      "After 5200th iteration, cost function is 1.56401725499\n",
      "After 5300th iteration, cost function is 1.56394396226\n",
      "After 5400th iteration, cost function is 1.5638717969\n",
      "After 5500th iteration, cost function is 1.56380073491\n",
      "After 5600th iteration, cost function is 1.56373075308\n",
      "After 5700th iteration, cost function is 1.5636618289\n",
      "After 5800th iteration, cost function is 1.56359394055\n",
      "After 5900th iteration, cost function is 1.56352706687\n",
      "After 6000th iteration, cost function is 1.56346118734\n",
      "After 6100th iteration, cost function is 1.56339628205\n",
      "After 6200th iteration, cost function is 1.56333233165\n",
      "After 6300th iteration, cost function is 1.56326931737\n",
      "After 6400th iteration, cost function is 1.56320722096\n",
      "After 6500th iteration, cost function is 1.56314602468\n",
      "After 6600th iteration, cost function is 1.56308571131\n",
      "After 6700th iteration, cost function is 1.56302626407\n",
      "After 6800th iteration, cost function is 1.56296766666\n",
      "After 6900th iteration, cost function is 1.5629099032\n",
      "After 7000th iteration, cost function is 1.56285295824\n",
      "After 7100th iteration, cost function is 1.56279681674\n",
      "After 7200th iteration, cost function is 1.56274146404\n",
      "After 7300th iteration, cost function is 1.56268688585\n",
      "After 7400th iteration, cost function is 1.56263306825\n",
      "After 7500th iteration, cost function is 1.56257999769\n",
      "After 7600th iteration, cost function is 1.56252766091\n",
      "After 7700th iteration, cost function is 1.56247604501\n",
      "After 7800th iteration, cost function is 1.5624251374\n",
      "After 7900th iteration, cost function is 1.56237492577\n",
      "After 8000th iteration, cost function is 1.56232539812\n",
      "After 8100th iteration, cost function is 1.56227654273\n",
      "After 8200th iteration, cost function is 1.56222834816\n",
      "After 8300th iteration, cost function is 1.56218080321\n",
      "After 8400th iteration, cost function is 1.56213389695\n",
      "After 8500th iteration, cost function is 1.5620876187\n",
      "After 8600th iteration, cost function is 1.56204195801\n",
      "After 8700th iteration, cost function is 1.56199690467\n",
      "After 8800th iteration, cost function is 1.56195244869\n",
      "After 8900th iteration, cost function is 1.56190858029\n",
      "After 9000th iteration, cost function is 1.5618652899\n",
      "After 9100th iteration, cost function is 1.56182256818\n",
      "After 9200th iteration, cost function is 1.56178040595\n",
      "After 9300th iteration, cost function is 1.56173879425\n",
      "After 9400th iteration, cost function is 1.56169772429\n",
      "After 9500th iteration, cost function is 1.56165718747\n",
      "After 9600th iteration, cost function is 1.56161717536\n",
      "After 9700th iteration, cost function is 1.56157767971\n",
      "After 9800th iteration, cost function is 1.56153869243\n",
      "After 9900th iteration, cost function is 1.56150020559\n",
      "After 10000th iteration, cost function is 1.56146221142\n",
      "Regularization constant: 0.000000, Dev precision (%): 29.427793\n",
      "After 100th iteration, cost function is 1.57047901827\n",
      "After 200th iteration, cost function is 1.5702757248\n",
      "After 300th iteration, cost function is 1.57007898664\n",
      "After 400th iteration, cost function is 1.56988851868\n",
      "After 500th iteration, cost function is 1.56970405104\n",
      "After 600th iteration, cost function is 1.56952532818\n",
      "After 700th iteration, cost function is 1.56935210802\n",
      "After 800th iteration, cost function is 1.56918416121\n",
      "After 900th iteration, cost function is 1.5690212703\n",
      "After 1000th iteration, cost function is 1.56886322914\n",
      "After 1100th iteration, cost function is 1.56870984214\n",
      "After 1200th iteration, cost function is 1.56856092369\n",
      "After 1300th iteration, cost function is 1.5684162976\n",
      "After 1400th iteration, cost function is 1.5682757965\n",
      "After 1500th iteration, cost function is 1.56813926139\n",
      "After 1600th iteration, cost function is 1.56800654112\n",
      "After 1700th iteration, cost function is 1.56787749198\n",
      "After 1800th iteration, cost function is 1.56775197724\n",
      "After 1900th iteration, cost function is 1.56762986676\n",
      "After 2000th iteration, cost function is 1.56751103663\n",
      "After 2100th iteration, cost function is 1.56739536884\n",
      "After 2200th iteration, cost function is 1.56728275089\n",
      "After 2300th iteration, cost function is 1.56717307554\n",
      "After 2400th iteration, cost function is 1.56706624051\n",
      "After 2500th iteration, cost function is 1.56696214818\n",
      "After 2600th iteration, cost function is 1.56686070535\n",
      "After 2700th iteration, cost function is 1.56676182301\n",
      "After 2800th iteration, cost function is 1.56666541609\n",
      "After 2900th iteration, cost function is 1.56657140328\n",
      "After 3000th iteration, cost function is 1.56647970681\n",
      "After 3100th iteration, cost function is 1.56639025223\n",
      "After 3200th iteration, cost function is 1.56630296831\n",
      "After 3300th iteration, cost function is 1.56621778679\n",
      "After 3400th iteration, cost function is 1.56613464229\n",
      "After 3500th iteration, cost function is 1.56605347212\n",
      "After 3600th iteration, cost function is 1.56597421616\n",
      "After 3700th iteration, cost function is 1.56589681672\n",
      "After 3800th iteration, cost function is 1.56582121843\n",
      "After 3900th iteration, cost function is 1.56574736809\n",
      "After 4000th iteration, cost function is 1.56567521462\n",
      "After 4100th iteration, cost function is 1.56560470889\n",
      "After 4200th iteration, cost function is 1.56553580367\n",
      "After 4300th iteration, cost function is 1.56546845353\n",
      "After 4400th iteration, cost function is 1.56540261471\n",
      "After 4500th iteration, cost function is 1.56533824512\n",
      "After 4600th iteration, cost function is 1.56527530419\n",
      "After 4700th iteration, cost function is 1.56521375281\n",
      "After 4800th iteration, cost function is 1.56515355331\n",
      "After 4900th iteration, cost function is 1.56509466934\n",
      "After 5000th iteration, cost function is 1.56503706581\n",
      "After 5100th iteration, cost function is 1.56498070889\n",
      "After 5200th iteration, cost function is 1.56492556588\n",
      "After 5300th iteration, cost function is 1.56487160521\n",
      "After 5400th iteration, cost function is 1.56481879636\n",
      "After 5500th iteration, cost function is 1.56476710984\n",
      "After 5600th iteration, cost function is 1.56471651713\n",
      "After 5700th iteration, cost function is 1.56466699064\n",
      "After 5800th iteration, cost function is 1.56461850366\n",
      "After 5900th iteration, cost function is 1.56457103035\n",
      "After 6000th iteration, cost function is 1.56452454567\n",
      "After 6100th iteration, cost function is 1.56447902538\n",
      "After 6200th iteration, cost function is 1.56443444599\n",
      "After 6300th iteration, cost function is 1.56439078472\n",
      "After 6400th iteration, cost function is 1.56434801949\n",
      "After 6500th iteration, cost function is 1.56430612889\n",
      "After 6600th iteration, cost function is 1.56426509214\n",
      "After 6700th iteration, cost function is 1.56422488908\n",
      "After 6800th iteration, cost function is 1.56418550014\n",
      "After 6900th iteration, cost function is 1.5641469063\n",
      "After 7000th iteration, cost function is 1.56410908912\n",
      "After 7100th iteration, cost function is 1.56407203066\n",
      "After 7200th iteration, cost function is 1.5640357135\n",
      "After 7300th iteration, cost function is 1.5640001207\n",
      "After 7400th iteration, cost function is 1.56396523579\n",
      "After 7500th iteration, cost function is 1.56393104276\n",
      "After 7600th iteration, cost function is 1.56389752604\n",
      "After 7700th iteration, cost function is 1.56386467047\n",
      "After 7800th iteration, cost function is 1.56383246131\n",
      "After 7900th iteration, cost function is 1.5638008842\n",
      "After 8000th iteration, cost function is 1.56376992517\n",
      "After 8100th iteration, cost function is 1.56373957062\n",
      "After 8200th iteration, cost function is 1.56370980731\n",
      "After 8300th iteration, cost function is 1.56368062231\n",
      "After 8400th iteration, cost function is 1.56365200307\n",
      "After 8500th iteration, cost function is 1.56362393732\n",
      "After 8600th iteration, cost function is 1.56359641313\n",
      "After 8700th iteration, cost function is 1.56356941886\n",
      "After 8800th iteration, cost function is 1.56354294315\n",
      "After 8900th iteration, cost function is 1.56351697494\n",
      "After 9000th iteration, cost function is 1.56349150345\n",
      "After 9100th iteration, cost function is 1.56346651814\n",
      "After 9200th iteration, cost function is 1.56344200874\n",
      "After 9300th iteration, cost function is 1.56341796524\n",
      "After 9400th iteration, cost function is 1.56339437786\n",
      "After 9500th iteration, cost function is 1.56337123705\n",
      "After 9600th iteration, cost function is 1.56334853351\n",
      "After 9700th iteration, cost function is 1.56332625814\n",
      "After 9800th iteration, cost function is 1.56330440206\n",
      "After 9900th iteration, cost function is 1.56328295661\n",
      "After 10000th iteration, cost function is 1.56326191332\n",
      "Regularization constant: 0.000010, Dev precision (%): 29.609446\n",
      "After 100th iteration, cost function is 1.57091175765\n",
      "After 200th iteration, cost function is 1.57069796414\n",
      "After 300th iteration, cost function is 1.57049350336\n",
      "After 400th iteration, cost function is 1.57029789108\n",
      "After 500th iteration, cost function is 1.57011067222\n",
      "After 600th iteration, cost function is 1.56993141888\n",
      "After 700th iteration, cost function is 1.56975972856\n",
      "After 800th iteration, cost function is 1.56959522245\n",
      "After 900th iteration, cost function is 1.56943754395\n",
      "After 1000th iteration, cost function is 1.56928635713\n",
      "After 1100th iteration, cost function is 1.56914134545\n",
      "After 1200th iteration, cost function is 1.5690022105\n",
      "After 1300th iteration, cost function is 1.56886867081\n",
      "After 1400th iteration, cost function is 1.56874046075\n",
      "After 1500th iteration, cost function is 1.56861732957\n",
      "After 1600th iteration, cost function is 1.56849904043\n",
      "After 1700th iteration, cost function is 1.56838536951\n",
      "After 1800th iteration, cost function is 1.56827610521\n",
      "After 1900th iteration, cost function is 1.56817104739\n",
      "After 2000th iteration, cost function is 1.56807000664\n",
      "After 2100th iteration, cost function is 1.56797280365\n",
      "After 2200th iteration, cost function is 1.56787926857\n",
      "After 2300th iteration, cost function is 1.56778924043\n",
      "After 2400th iteration, cost function is 1.56770256662\n",
      "After 2500th iteration, cost function is 1.56761910239\n",
      "After 2600th iteration, cost function is 1.56753871035\n",
      "After 2700th iteration, cost function is 1.56746126009\n",
      "After 2800th iteration, cost function is 1.5673866277\n",
      "After 2900th iteration, cost function is 1.56731469545\n",
      "After 3000th iteration, cost function is 1.56724535141\n",
      "After 3100th iteration, cost function is 1.56717848909\n",
      "After 3200th iteration, cost function is 1.56711400719\n",
      "After 3300th iteration, cost function is 1.56705180924\n",
      "After 3400th iteration, cost function is 1.56699180339\n",
      "After 3500th iteration, cost function is 1.56693390208\n",
      "After 3600th iteration, cost function is 1.56687802189\n",
      "After 3700th iteration, cost function is 1.56682408322\n",
      "After 3800th iteration, cost function is 1.56677201018\n",
      "After 3900th iteration, cost function is 1.56672173029\n",
      "After 4000th iteration, cost function is 1.56667317437\n",
      "After 4100th iteration, cost function is 1.56662627635\n",
      "After 4200th iteration, cost function is 1.56658097306\n",
      "After 4300th iteration, cost function is 1.56653720414\n",
      "After 4400th iteration, cost function is 1.56649491187\n",
      "After 4500th iteration, cost function is 1.56645404103\n",
      "After 4600th iteration, cost function is 1.56641453877\n",
      "After 4700th iteration, cost function is 1.5663763545\n",
      "After 4800th iteration, cost function is 1.56633943976\n",
      "After 4900th iteration, cost function is 1.56630374815\n",
      "After 5000th iteration, cost function is 1.56626923518\n",
      "After 5100th iteration, cost function is 1.56623585823\n",
      "After 5200th iteration, cost function is 1.5662035764\n",
      "After 5300th iteration, cost function is 1.56617235048\n",
      "After 5400th iteration, cost function is 1.56614214285\n",
      "After 5500th iteration, cost function is 1.56611291738\n",
      "After 5600th iteration, cost function is 1.56608463941\n",
      "After 5700th iteration, cost function is 1.56605727563\n",
      "After 5800th iteration, cost function is 1.56603079407\n",
      "After 5900th iteration, cost function is 1.56600516398\n",
      "After 6000th iteration, cost function is 1.56598035584\n",
      "After 6100th iteration, cost function is 1.56595634124\n",
      "After 6200th iteration, cost function is 1.56593309289\n",
      "After 6300th iteration, cost function is 1.56591058451\n",
      "After 6400th iteration, cost function is 1.56588879084\n",
      "After 6500th iteration, cost function is 1.56586768757\n",
      "After 6600th iteration, cost function is 1.5658472513\n",
      "After 6700th iteration, cost function is 1.5658274595\n",
      "After 6800th iteration, cost function is 1.56580829048\n",
      "After 6900th iteration, cost function is 1.56578972335\n",
      "After 7000th iteration, cost function is 1.56577173798\n",
      "After 7100th iteration, cost function is 1.56575431499\n",
      "After 7200th iteration, cost function is 1.56573743569\n",
      "After 7300th iteration, cost function is 1.56572108207\n",
      "After 7400th iteration, cost function is 1.56570523677\n",
      "After 7500th iteration, cost function is 1.56568988305\n",
      "After 7600th iteration, cost function is 1.56567500478\n",
      "After 7700th iteration, cost function is 1.56566058638\n",
      "After 7800th iteration, cost function is 1.56564661283\n",
      "After 7900th iteration, cost function is 1.56563306965\n",
      "After 8000th iteration, cost function is 1.56561994286\n",
      "After 8100th iteration, cost function is 1.56560721896\n",
      "After 8200th iteration, cost function is 1.56559488492\n",
      "After 8300th iteration, cost function is 1.56558292819\n",
      "After 8400th iteration, cost function is 1.56557133661\n",
      "After 8500th iteration, cost function is 1.56556009847\n",
      "After 8600th iteration, cost function is 1.56554920244\n",
      "After 8700th iteration, cost function is 1.56553863761\n",
      "After 8800th iteration, cost function is 1.56552839339\n",
      "After 8900th iteration, cost function is 1.5655184596\n",
      "After 9000th iteration, cost function is 1.56550882638\n",
      "After 9100th iteration, cost function is 1.56549948419\n",
      "After 9200th iteration, cost function is 1.56549042384\n",
      "After 9300th iteration, cost function is 1.56548163643\n",
      "After 9400th iteration, cost function is 1.56547311336\n",
      "After 9500th iteration, cost function is 1.56546484632\n",
      "After 9600th iteration, cost function is 1.56545682726\n",
      "After 9700th iteration, cost function is 1.56544904843\n",
      "After 9800th iteration, cost function is 1.5654415023\n",
      "After 9900th iteration, cost function is 1.56543418161\n",
      "After 10000th iteration, cost function is 1.56542707933\n",
      "Regularization constant: 0.000030, Dev precision (%): 29.609446\n",
      "After 100th iteration, cost function is 1.57234776495\n",
      "After 200th iteration, cost function is 1.572025376\n",
      "After 300th iteration, cost function is 1.57172853266\n",
      "After 400th iteration, cost function is 1.57145510646\n",
      "After 500th iteration, cost function is 1.57120315604\n",
      "After 600th iteration, cost function is 1.57097090978\n",
      "After 700th iteration, cost function is 1.57075675019\n",
      "After 800th iteration, cost function is 1.57055919977\n",
      "After 900th iteration, cost function is 1.57037690825\n",
      "After 1000th iteration, cost function is 1.57020864102\n",
      "After 1100th iteration, cost function is 1.57005326867\n",
      "After 1200th iteration, cost function is 1.56990975757\n",
      "After 1300th iteration, cost function is 1.56977716125\n",
      "After 1400th iteration, cost function is 1.5696546127\n",
      "After 1500th iteration, cost function is 1.56954131731\n",
      "After 1600th iteration, cost function is 1.56943654647\n",
      "After 1700th iteration, cost function is 1.56933963182\n",
      "After 1800th iteration, cost function is 1.56924995999\n",
      "After 1900th iteration, cost function is 1.56916696784\n",
      "After 2000th iteration, cost function is 1.56909013809\n",
      "After 2100th iteration, cost function is 1.56901899544\n",
      "After 2200th iteration, cost function is 1.56895310293\n",
      "After 2300th iteration, cost function is 1.56889205872\n",
      "After 2400th iteration, cost function is 1.56883549311\n",
      "After 2500th iteration, cost function is 1.56878306583\n",
      "After 2600th iteration, cost function is 1.56873446359\n",
      "After 2700th iteration, cost function is 1.56868939784\n",
      "After 2800th iteration, cost function is 1.56864760268\n",
      "After 2900th iteration, cost function is 1.56860883305\n",
      "After 3000th iteration, cost function is 1.56857286297\n",
      "After 3100th iteration, cost function is 1.56853948402\n",
      "After 3200th iteration, cost function is 1.56850850388\n",
      "After 3300th iteration, cost function is 1.56847974506\n",
      "After 3400th iteration, cost function is 1.56845304368\n",
      "After 3500th iteration, cost function is 1.56842824838\n",
      "After 3600th iteration, cost function is 1.56840521934\n",
      "After 3700th iteration, cost function is 1.56838382737\n",
      "After 3800th iteration, cost function is 1.568363953\n",
      "After 3900th iteration, cost function is 1.56834548582\n",
      "After 4000th iteration, cost function is 1.56832832369\n",
      "After 4100th iteration, cost function is 1.56831237211\n",
      "After 4200th iteration, cost function is 1.56829754366\n",
      "After 4300th iteration, cost function is 1.56828375739\n",
      "After 4400th iteration, cost function is 1.56827093839\n",
      "After 4500th iteration, cost function is 1.56825901727\n",
      "After 4600th iteration, cost function is 1.56824792976\n",
      "After 4700th iteration, cost function is 1.56823761632\n",
      "After 4800th iteration, cost function is 1.56822802179\n",
      "After 4900th iteration, cost function is 1.56821909502\n",
      "After 5000th iteration, cost function is 1.56821078861\n",
      "After 5100th iteration, cost function is 1.56820305861\n",
      "After 5200th iteration, cost function is 1.56819586424\n",
      "After 5300th iteration, cost function is 1.5681891677\n",
      "After 5400th iteration, cost function is 1.5681829339\n",
      "After 5500th iteration, cost function is 1.56817713029\n",
      "After 5600th iteration, cost function is 1.56817172666\n",
      "After 5700th iteration, cost function is 1.56816669497\n",
      "After 5800th iteration, cost function is 1.56816200918\n",
      "After 5900th iteration, cost function is 1.56815764511\n",
      "After 6000th iteration, cost function is 1.56815358033\n",
      "After 6100th iteration, cost function is 1.56814979396\n",
      "After 6200th iteration, cost function is 1.56814626665\n",
      "After 6300th iteration, cost function is 1.5681429804\n",
      "After 6400th iteration, cost function is 1.56813991847\n",
      "After 6500th iteration, cost function is 1.56813706534\n",
      "After 6600th iteration, cost function is 1.56813440654\n",
      "After 6700th iteration, cost function is 1.56813192867\n",
      "After 6800th iteration, cost function is 1.56812961922\n",
      "After 6900th iteration, cost function is 1.56812746659\n",
      "After 7000th iteration, cost function is 1.56812545999\n",
      "After 7100th iteration, cost function is 1.56812358937\n",
      "After 7200th iteration, cost function is 1.5681218454\n",
      "After 7300th iteration, cost function is 1.56812021939\n",
      "After 7400th iteration, cost function is 1.56811870326\n",
      "After 7500th iteration, cost function is 1.56811728948\n",
      "After 7600th iteration, cost function is 1.56811597106\n",
      "After 7700th iteration, cost function is 1.56811474149\n",
      "After 7800th iteration, cost function is 1.5681135947\n",
      "After 7900th iteration, cost function is 1.56811252506\n",
      "After 8000th iteration, cost function is 1.56811152731\n",
      "After 8100th iteration, cost function is 1.56811059656\n",
      "After 8200th iteration, cost function is 1.56810972827\n",
      "After 8300th iteration, cost function is 1.56810891818\n",
      "After 8400th iteration, cost function is 1.56810816236\n",
      "After 8500th iteration, cost function is 1.56810745712\n",
      "After 8600th iteration, cost function is 1.56810679905\n",
      "After 8700th iteration, cost function is 1.56810618495\n",
      "After 8800th iteration, cost function is 1.56810561186\n",
      "After 8900th iteration, cost function is 1.56810507699\n",
      "After 9000th iteration, cost function is 1.56810457779\n",
      "After 9100th iteration, cost function is 1.56810411184\n",
      "After 9200th iteration, cost function is 1.5681036769\n",
      "After 9300th iteration, cost function is 1.56810327089\n",
      "After 9400th iteration, cost function is 1.56810289187\n",
      "After 9500th iteration, cost function is 1.56810253802\n",
      "After 9600th iteration, cost function is 1.56810220765\n",
      "After 9700th iteration, cost function is 1.56810189919\n",
      "After 9800th iteration, cost function is 1.56810161116\n",
      "After 9900th iteration, cost function is 1.56810134222\n",
      "After 10000th iteration, cost function is 1.56810109106\n",
      "Regularization constant: 0.000100, Dev precision (%): 29.064487\n",
      "After 100th iteration, cost function is 1.57583227261\n",
      "After 200th iteration, cost function is 1.57473188974\n",
      "After 300th iteration, cost function is 1.57382637659\n",
      "After 400th iteration, cost function is 1.57308103884\n",
      "After 500th iteration, cost function is 1.57246739541\n",
      "After 600th iteration, cost function is 1.57196205868\n",
      "After 700th iteration, cost function is 1.57154581788\n",
      "After 800th iteration, cost function is 1.5712028885\n",
      "After 900th iteration, cost function is 1.5709202975\n",
      "After 1000th iteration, cost function is 1.57068737962\n",
      "After 1100th iteration, cost function is 1.57049536439\n",
      "After 1200th iteration, cost function is 1.57033703764\n",
      "After 1300th iteration, cost function is 1.57020646381\n",
      "After 1400th iteration, cost function is 1.57009875804\n",
      "After 1500th iteration, cost function is 1.57000989928\n",
      "After 1600th iteration, cost function is 1.56993657668\n",
      "After 1700th iteration, cost function is 1.56987606357\n",
      "After 1800th iteration, cost function is 1.56982611386\n",
      "After 1900th iteration, cost function is 1.56978487693\n",
      "After 2000th iteration, cost function is 1.56975082769\n",
      "After 2100th iteration, cost function is 1.56972270905\n",
      "After 2200th iteration, cost function is 1.56969948461\n",
      "After 2300th iteration, cost function is 1.56968029976\n",
      "After 2400th iteration, cost function is 1.56966444966\n",
      "After 2500th iteration, cost function is 1.5696513529\n",
      "After 2600th iteration, cost function is 1.56964052977\n",
      "After 2700th iteration, cost function is 1.56963158442\n",
      "After 2800th iteration, cost function is 1.56962419014\n",
      "After 2900th iteration, cost function is 1.56961807724\n",
      "After 3000th iteration, cost function is 1.56961302309\n",
      "After 3100th iteration, cost function is 1.56960884382\n",
      "After 3200th iteration, cost function is 1.56960538762\n",
      "After 3300th iteration, cost function is 1.56960252906\n",
      "After 3400th iteration, cost function is 1.56960016457\n",
      "After 3500th iteration, cost function is 1.56959820854\n",
      "After 3600th iteration, cost function is 1.56959659025\n",
      "After 3700th iteration, cost function is 1.56959525125\n",
      "After 3800th iteration, cost function is 1.56959414325\n",
      "After 3900th iteration, cost function is 1.56959322629\n",
      "After 4000th iteration, cost function is 1.56959246738\n",
      "After 4100th iteration, cost function is 1.56959183922\n",
      "After 4200th iteration, cost function is 1.56959131923\n",
      "After 4300th iteration, cost function is 1.56959088876\n",
      "After 4400th iteration, cost function is 1.56959053235\n",
      "After 4500th iteration, cost function is 1.56959023726\n",
      "After 4600th iteration, cost function is 1.5695899929\n",
      "After 4700th iteration, cost function is 1.56958979054\n",
      "After 4800th iteration, cost function is 1.56958962295\n",
      "After 4900th iteration, cost function is 1.56958948415\n",
      "After 5000th iteration, cost function is 1.56958936917\n",
      "After 5100th iteration, cost function is 1.56958927393\n",
      "After 5200th iteration, cost function is 1.56958919503\n",
      "After 5300th iteration, cost function is 1.56958912967\n",
      "After 5400th iteration, cost function is 1.56958907551\n",
      "After 5500th iteration, cost function is 1.56958903064\n",
      "After 5600th iteration, cost function is 1.56958899345\n",
      "After 5700th iteration, cost function is 1.56958896264\n",
      "After 5800th iteration, cost function is 1.5695889371\n",
      "After 5900th iteration, cost function is 1.56958891594\n",
      "After 6000th iteration, cost function is 1.5695888984\n",
      "After 6100th iteration, cost function is 1.56958888386\n",
      "After 6200th iteration, cost function is 1.5695888718\n",
      "After 6300th iteration, cost function is 1.56958886181\n",
      "After 6400th iteration, cost function is 1.56958885353\n",
      "After 6500th iteration, cost function is 1.56958884666\n",
      "After 6600th iteration, cost function is 1.56958884096\n",
      "After 6700th iteration, cost function is 1.56958883624\n",
      "After 6800th iteration, cost function is 1.56958883233\n",
      "After 6900th iteration, cost function is 1.56958882908\n",
      "After 7000th iteration, cost function is 1.56958882639\n",
      "After 7100th iteration, cost function is 1.56958882416\n",
      "After 7200th iteration, cost function is 1.5695888223\n",
      "After 7300th iteration, cost function is 1.56958882077\n",
      "After 7400th iteration, cost function is 1.56958881949\n",
      "After 7500th iteration, cost function is 1.56958881844\n",
      "After 7600th iteration, cost function is 1.56958881756\n",
      "After 7700th iteration, cost function is 1.56958881683\n",
      "After 7800th iteration, cost function is 1.56958881623\n",
      "After 7900th iteration, cost function is 1.56958881573\n",
      "After 8000th iteration, cost function is 1.56958881531\n",
      "After 8100th iteration, cost function is 1.56958881497\n",
      "After 8200th iteration, cost function is 1.56958881468\n",
      "After 8300th iteration, cost function is 1.56958881444\n",
      "After 8400th iteration, cost function is 1.56958881425\n",
      "After 8500th iteration, cost function is 1.56958881408\n",
      "After 8600th iteration, cost function is 1.56958881395\n",
      "After 8700th iteration, cost function is 1.56958881384\n",
      "After 8800th iteration, cost function is 1.56958881374\n",
      "After 8900th iteration, cost function is 1.56958881367\n",
      "After 9000th iteration, cost function is 1.5695888136\n",
      "After 9100th iteration, cost function is 1.56958881355\n",
      "After 9200th iteration, cost function is 1.5695888135\n",
      "After 9300th iteration, cost function is 1.56958881347\n",
      "After 9400th iteration, cost function is 1.56958881344\n",
      "After 9500th iteration, cost function is 1.56958881341\n",
      "After 9600th iteration, cost function is 1.56958881339\n",
      "After 9700th iteration, cost function is 1.56958881337\n",
      "After 9800th iteration, cost function is 1.56958881336\n",
      "After 9900th iteration, cost function is 1.56958881335\n",
      "After 10000th iteration, cost function is 1.56958881334\n",
      "Regularization constant: 0.000300, Dev precision (%): 26.430518\n",
      "After 100th iteration, cost function is 1.58259646404\n",
      "After 200th iteration, cost function is 1.57700994135\n",
      "After 300th iteration, cost function is 1.57398172581\n",
      "After 400th iteration, cost function is 1.57234001886\n",
      "After 500th iteration, cost function is 1.57144986132\n",
      "After 600th iteration, cost function is 1.57096713714\n",
      "After 700th iteration, cost function is 1.57070532447\n",
      "After 800th iteration, cost function is 1.57056330753\n",
      "After 900th iteration, cost function is 1.57048626226\n",
      "After 1000th iteration, cost function is 1.57044445933\n",
      "After 1100th iteration, cost function is 1.57042177526\n",
      "After 1200th iteration, cost function is 1.57040946443\n",
      "After 1300th iteration, cost function is 1.57040278247\n",
      "After 1400th iteration, cost function is 1.57039915528\n",
      "After 1500th iteration, cost function is 1.5703971861\n",
      "After 1600th iteration, cost function is 1.57039611694\n",
      "After 1700th iteration, cost function is 1.57039553637\n",
      "After 1800th iteration, cost function is 1.57039522108\n",
      "After 1900th iteration, cost function is 1.57039504984\n",
      "After 2000th iteration, cost function is 1.57039495683\n",
      "After 2100th iteration, cost function is 1.57039490631\n",
      "After 2200th iteration, cost function is 1.57039487886\n",
      "After 2300th iteration, cost function is 1.57039486394\n",
      "After 2400th iteration, cost function is 1.57039485584\n",
      "After 2500th iteration, cost function is 1.57039485144\n",
      "After 2600th iteration, cost function is 1.57039484904\n",
      "After 2700th iteration, cost function is 1.57039484774\n",
      "After 2800th iteration, cost function is 1.57039484703\n",
      "After 2900th iteration, cost function is 1.57039484665\n",
      "After 3000th iteration, cost function is 1.57039484644\n",
      "After 3100th iteration, cost function is 1.57039484633\n",
      "After 3200th iteration, cost function is 1.57039484627\n",
      "After 3300th iteration, cost function is 1.57039484623\n",
      "After 3400th iteration, cost function is 1.57039484621\n",
      "After 3500th iteration, cost function is 1.5703948462\n",
      "After 3600th iteration, cost function is 1.5703948462\n",
      "After 3700th iteration, cost function is 1.5703948462\n",
      "After 3800th iteration, cost function is 1.57039484619\n",
      "After 3900th iteration, cost function is 1.57039484619\n",
      "After 4000th iteration, cost function is 1.57039484619\n",
      "After 4100th iteration, cost function is 1.57039484619\n",
      "After 4200th iteration, cost function is 1.57039484619\n",
      "After 4300th iteration, cost function is 1.57039484619\n",
      "After 4400th iteration, cost function is 1.57039484619\n",
      "After 4500th iteration, cost function is 1.57039484619\n",
      "After 4600th iteration, cost function is 1.57039484619\n",
      "After 4700th iteration, cost function is 1.57039484619\n",
      "After 4800th iteration, cost function is 1.57039484619\n",
      "After 4900th iteration, cost function is 1.57039484619\n",
      "After 5000th iteration, cost function is 1.57039484619\n",
      "After 5100th iteration, cost function is 1.57039484619\n",
      "After 5200th iteration, cost function is 1.57039484619\n",
      "After 5300th iteration, cost function is 1.57039484619\n",
      "After 5400th iteration, cost function is 1.57039484619\n",
      "After 5500th iteration, cost function is 1.57039484619\n",
      "After 5600th iteration, cost function is 1.57039484619\n",
      "After 5700th iteration, cost function is 1.57039484619\n",
      "After 5800th iteration, cost function is 1.57039484619\n",
      "After 5900th iteration, cost function is 1.57039484619\n",
      "After 6000th iteration, cost function is 1.57039484619\n",
      "After 6100th iteration, cost function is 1.57039484619\n",
      "After 6200th iteration, cost function is 1.57039484619\n",
      "After 6300th iteration, cost function is 1.57039484619\n",
      "After 6400th iteration, cost function is 1.57039484619\n",
      "After 6500th iteration, cost function is 1.57039484619\n",
      "After 6600th iteration, cost function is 1.57039484619\n",
      "After 6700th iteration, cost function is 1.57039484619\n",
      "After 6800th iteration, cost function is 1.57039484619\n",
      "After 6900th iteration, cost function is 1.57039484619\n",
      "After 7000th iteration, cost function is 1.57039484619\n",
      "After 7100th iteration, cost function is 1.57039484619\n",
      "After 7200th iteration, cost function is 1.57039484619\n",
      "After 7300th iteration, cost function is 1.57039484619\n",
      "After 7400th iteration, cost function is 1.57039484619\n",
      "After 7500th iteration, cost function is 1.57039484619\n",
      "After 7600th iteration, cost function is 1.57039484619\n",
      "After 7700th iteration, cost function is 1.57039484619\n",
      "After 7800th iteration, cost function is 1.57039484619\n",
      "After 7900th iteration, cost function is 1.57039484619\n",
      "After 8000th iteration, cost function is 1.57039484619\n",
      "After 8100th iteration, cost function is 1.57039484619\n",
      "After 8200th iteration, cost function is 1.57039484619\n",
      "After 8300th iteration, cost function is 1.57039484619\n",
      "After 8400th iteration, cost function is 1.57039484619\n",
      "After 8500th iteration, cost function is 1.57039484619\n",
      "After 8600th iteration, cost function is 1.57039484619\n",
      "After 8700th iteration, cost function is 1.57039484619\n",
      "After 8800th iteration, cost function is 1.57039484619\n",
      "After 8900th iteration, cost function is 1.57039484619\n",
      "After 9000th iteration, cost function is 1.57039484619\n",
      "After 9100th iteration, cost function is 1.57039484619\n",
      "After 9200th iteration, cost function is 1.57039484619\n",
      "After 9300th iteration, cost function is 1.57039484619\n",
      "After 9400th iteration, cost function is 1.57039484619\n",
      "After 9500th iteration, cost function is 1.57039484619\n",
      "After 9600th iteration, cost function is 1.57039484619\n",
      "After 9700th iteration, cost function is 1.57039484619\n",
      "After 9800th iteration, cost function is 1.57039484619\n",
      "After 9900th iteration, cost function is 1.57039484619\n",
      "After 10000th iteration, cost function is 1.57039484619\n",
      "Regularization constant: 0.001000, Dev precision (%): 25.431426\n",
      "After 100th iteration, cost function is 1.58198969621\n",
      "After 200th iteration, cost function is 1.57281228656\n",
      "After 300th iteration, cost function is 1.57132382277\n",
      "After 400th iteration, cost function is 1.57108238214\n",
      "After 500th iteration, cost function is 1.57104321379\n",
      "After 600th iteration, cost function is 1.57103685885\n",
      "After 700th iteration, cost function is 1.57103582766\n",
      "After 800th iteration, cost function is 1.57103566031\n",
      "After 900th iteration, cost function is 1.57103563315\n",
      "After 1000th iteration, cost function is 1.57103562875\n",
      "After 1100th iteration, cost function is 1.57103562803\n",
      "After 1200th iteration, cost function is 1.57103562791\n",
      "After 1300th iteration, cost function is 1.5710356279\n",
      "After 1400th iteration, cost function is 1.57103562789\n",
      "After 1500th iteration, cost function is 1.57103562789\n",
      "After 1600th iteration, cost function is 1.57103562789\n",
      "After 1700th iteration, cost function is 1.57103562789\n",
      "After 1800th iteration, cost function is 1.57103562789\n",
      "After 1900th iteration, cost function is 1.57103562789\n",
      "After 2000th iteration, cost function is 1.57103562789\n",
      "After 2100th iteration, cost function is 1.57103562789\n",
      "After 2200th iteration, cost function is 1.57103562789\n",
      "After 2300th iteration, cost function is 1.57103562789\n",
      "After 2400th iteration, cost function is 1.57103562789\n",
      "After 2500th iteration, cost function is 1.57103562789\n",
      "After 2600th iteration, cost function is 1.57103562789\n",
      "After 2700th iteration, cost function is 1.57103562789\n",
      "After 2800th iteration, cost function is 1.57103562789\n",
      "After 2900th iteration, cost function is 1.57103562789\n",
      "After 3000th iteration, cost function is 1.57103562789\n",
      "After 3100th iteration, cost function is 1.57103562789\n",
      "After 3200th iteration, cost function is 1.57103562789\n",
      "After 3300th iteration, cost function is 1.57103562789\n",
      "After 3400th iteration, cost function is 1.57103562789\n",
      "After 3500th iteration, cost function is 1.57103562789\n",
      "After 3600th iteration, cost function is 1.57103562789\n",
      "After 3700th iteration, cost function is 1.57103562789\n",
      "After 3800th iteration, cost function is 1.57103562789\n",
      "After 3900th iteration, cost function is 1.57103562789\n",
      "After 4000th iteration, cost function is 1.57103562789\n",
      "After 4100th iteration, cost function is 1.57103562789\n",
      "After 4200th iteration, cost function is 1.57103562789\n",
      "After 4300th iteration, cost function is 1.57103562789\n",
      "After 4400th iteration, cost function is 1.57103562789\n",
      "After 4500th iteration, cost function is 1.57103562789\n",
      "After 4600th iteration, cost function is 1.57103562789\n",
      "After 4700th iteration, cost function is 1.57103562789\n",
      "After 4800th iteration, cost function is 1.57103562789\n",
      "After 4900th iteration, cost function is 1.57103562789\n",
      "After 5000th iteration, cost function is 1.57103562789\n",
      "After 5100th iteration, cost function is 1.57103562789\n",
      "After 5200th iteration, cost function is 1.57103562789\n",
      "After 5300th iteration, cost function is 1.57103562789\n",
      "After 5400th iteration, cost function is 1.57103562789\n",
      "After 5500th iteration, cost function is 1.57103562789\n",
      "After 5600th iteration, cost function is 1.57103562789\n",
      "After 5700th iteration, cost function is 1.57103562789\n",
      "After 5800th iteration, cost function is 1.57103562789\n",
      "After 5900th iteration, cost function is 1.57103562789\n",
      "After 6000th iteration, cost function is 1.57103562789\n",
      "After 6100th iteration, cost function is 1.57103562789\n",
      "After 6200th iteration, cost function is 1.57103562789\n",
      "After 6300th iteration, cost function is 1.57103562789\n",
      "After 6400th iteration, cost function is 1.57103562789\n",
      "After 6500th iteration, cost function is 1.57103562789\n",
      "After 6600th iteration, cost function is 1.57103562789\n",
      "After 6700th iteration, cost function is 1.57103562789\n",
      "After 6800th iteration, cost function is 1.57103562789\n",
      "After 6900th iteration, cost function is 1.57103562789\n",
      "After 7000th iteration, cost function is 1.57103562789\n",
      "After 7100th iteration, cost function is 1.57103562789\n",
      "After 7200th iteration, cost function is 1.57103562789\n",
      "After 7300th iteration, cost function is 1.57103562789\n",
      "After 7400th iteration, cost function is 1.57103562789\n",
      "After 7500th iteration, cost function is 1.57103562789\n",
      "After 7600th iteration, cost function is 1.57103562789\n",
      "After 7700th iteration, cost function is 1.57103562789\n",
      "After 7800th iteration, cost function is 1.57103562789\n",
      "After 7900th iteration, cost function is 1.57103562789\n",
      "After 8000th iteration, cost function is 1.57103562789\n",
      "After 8100th iteration, cost function is 1.57103562789\n",
      "After 8200th iteration, cost function is 1.57103562789\n",
      "After 8300th iteration, cost function is 1.57103562789\n",
      "After 8400th iteration, cost function is 1.57103562789\n",
      "After 8500th iteration, cost function is 1.57103562789\n",
      "After 8600th iteration, cost function is 1.57103562789\n",
      "After 8700th iteration, cost function is 1.57103562789\n",
      "After 8800th iteration, cost function is 1.57103562789\n",
      "After 8900th iteration, cost function is 1.57103562789\n",
      "After 9000th iteration, cost function is 1.57103562789\n",
      "After 9100th iteration, cost function is 1.57103562789\n",
      "After 9200th iteration, cost function is 1.57103562789\n",
      "After 9300th iteration, cost function is 1.57103562789\n",
      "After 9400th iteration, cost function is 1.57103562789\n",
      "After 9500th iteration, cost function is 1.57103562789\n",
      "After 9600th iteration, cost function is 1.57103562789\n",
      "After 9700th iteration, cost function is 1.57103562789\n",
      "After 9800th iteration, cost function is 1.57103562789\n",
      "After 9900th iteration, cost function is 1.57103562789\n",
      "After 10000th iteration, cost function is 1.57103562789\n",
      "Regularization constant: 0.003000, Dev precision (%): 25.522252\n",
      "After 100th iteration, cost function is 1.57307599077\n",
      "After 200th iteration, cost function is 1.57255222433\n",
      "After 300th iteration, cost function is 1.57255105281\n",
      "After 400th iteration, cost function is 1.57255105019\n",
      "After 500th iteration, cost function is 1.57255105019\n",
      "After 600th iteration, cost function is 1.57255105019\n",
      "After 700th iteration, cost function is 1.57255105019\n",
      "After 800th iteration, cost function is 1.57255105019\n",
      "After 900th iteration, cost function is 1.57255105019\n",
      "After 1000th iteration, cost function is 1.57255105019\n",
      "After 1100th iteration, cost function is 1.57255105019\n",
      "After 1200th iteration, cost function is 1.57255105019\n",
      "After 1300th iteration, cost function is 1.57255105019\n",
      "After 1400th iteration, cost function is 1.57255105019\n",
      "After 1500th iteration, cost function is 1.57255105019\n",
      "After 1600th iteration, cost function is 1.57255105019\n",
      "After 1700th iteration, cost function is 1.57255105019\n",
      "After 1800th iteration, cost function is 1.57255105019\n",
      "After 1900th iteration, cost function is 1.57255105019\n",
      "After 2000th iteration, cost function is 1.57255105019\n",
      "After 2100th iteration, cost function is 1.57255105019\n",
      "After 2200th iteration, cost function is 1.57255105019\n",
      "After 2300th iteration, cost function is 1.57255105019\n",
      "After 2400th iteration, cost function is 1.57255105019\n",
      "After 2500th iteration, cost function is 1.57255105019\n",
      "After 2600th iteration, cost function is 1.57255105019\n",
      "After 2700th iteration, cost function is 1.57255105019\n",
      "After 2800th iteration, cost function is 1.57255105019\n",
      "After 2900th iteration, cost function is 1.57255105019\n",
      "After 3000th iteration, cost function is 1.57255105019\n",
      "After 3100th iteration, cost function is 1.57255105019\n",
      "After 3200th iteration, cost function is 1.57255105019\n",
      "After 3300th iteration, cost function is 1.57255105019\n",
      "After 3400th iteration, cost function is 1.57255105019\n",
      "After 3500th iteration, cost function is 1.57255105019\n",
      "After 3600th iteration, cost function is 1.57255105019\n",
      "After 3700th iteration, cost function is 1.57255105019\n",
      "After 3800th iteration, cost function is 1.57255105019\n",
      "After 3900th iteration, cost function is 1.57255105019\n",
      "After 4000th iteration, cost function is 1.57255105019\n",
      "After 4100th iteration, cost function is 1.57255105019\n",
      "After 4200th iteration, cost function is 1.57255105019\n",
      "After 4300th iteration, cost function is 1.57255105019\n",
      "After 4400th iteration, cost function is 1.57255105019\n",
      "After 4500th iteration, cost function is 1.57255105019\n",
      "After 4600th iteration, cost function is 1.57255105019\n",
      "After 4700th iteration, cost function is 1.57255105019\n",
      "After 4800th iteration, cost function is 1.57255105019\n",
      "After 4900th iteration, cost function is 1.57255105019\n",
      "After 5000th iteration, cost function is 1.57255105019\n",
      "After 5100th iteration, cost function is 1.57255105019\n",
      "After 5200th iteration, cost function is 1.57255105019\n",
      "After 5300th iteration, cost function is 1.57255105019\n",
      "After 5400th iteration, cost function is 1.57255105019\n",
      "After 5500th iteration, cost function is 1.57255105019\n",
      "After 5600th iteration, cost function is 1.57255105019\n",
      "After 5700th iteration, cost function is 1.57255105019\n",
      "After 5800th iteration, cost function is 1.57255105019\n",
      "After 5900th iteration, cost function is 1.57255105019\n",
      "After 6000th iteration, cost function is 1.57255105019\n",
      "After 6100th iteration, cost function is 1.57255105019\n",
      "After 6200th iteration, cost function is 1.57255105019\n",
      "After 6300th iteration, cost function is 1.57255105019\n",
      "After 6400th iteration, cost function is 1.57255105019\n",
      "After 6500th iteration, cost function is 1.57255105019\n",
      "After 6600th iteration, cost function is 1.57255105019\n",
      "After 6700th iteration, cost function is 1.57255105019\n",
      "After 6800th iteration, cost function is 1.57255105019\n",
      "After 6900th iteration, cost function is 1.57255105019\n",
      "After 7000th iteration, cost function is 1.57255105019\n",
      "After 7100th iteration, cost function is 1.57255105019\n",
      "After 7200th iteration, cost function is 1.57255105019\n",
      "After 7300th iteration, cost function is 1.57255105019\n",
      "After 7400th iteration, cost function is 1.57255105019\n",
      "After 7500th iteration, cost function is 1.57255105019\n",
      "After 7600th iteration, cost function is 1.57255105019\n",
      "After 7700th iteration, cost function is 1.57255105019\n",
      "After 7800th iteration, cost function is 1.57255105019\n",
      "After 7900th iteration, cost function is 1.57255105019\n",
      "After 8000th iteration, cost function is 1.57255105019\n",
      "After 8100th iteration, cost function is 1.57255105019\n",
      "After 8200th iteration, cost function is 1.57255105019\n",
      "After 8300th iteration, cost function is 1.57255105019\n",
      "After 8400th iteration, cost function is 1.57255105019\n",
      "After 8500th iteration, cost function is 1.57255105019\n",
      "After 8600th iteration, cost function is 1.57255105019\n",
      "After 8700th iteration, cost function is 1.57255105019\n",
      "After 8800th iteration, cost function is 1.57255105019\n",
      "After 8900th iteration, cost function is 1.57255105019\n",
      "After 9000th iteration, cost function is 1.57255105019\n",
      "After 9100th iteration, cost function is 1.57255105019\n",
      "After 9200th iteration, cost function is 1.57255105019\n",
      "After 9300th iteration, cost function is 1.57255105019\n",
      "After 9400th iteration, cost function is 1.57255105019\n",
      "After 9500th iteration, cost function is 1.57255105019\n",
      "After 9600th iteration, cost function is 1.57255105019\n",
      "After 9700th iteration, cost function is 1.57255105019\n",
      "After 9800th iteration, cost function is 1.57255105019\n",
      "After 9900th iteration, cost function is 1.57255105019\n",
      "After 10000th iteration, cost function is 1.57255105019\n",
      "Regularization constant: 0.010000, Dev precision (%): 25.522252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11cf6048>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAlYAAAHkCAYAAAD8T2H8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAIABJREFUeJzt3XuUpVV95vHvT0BEQFEEQWguKoyXiMJEzSwwtqKiKAiD\n",
       "iHgDxjEqRoizVuJlnITErInxkjheEieJdrWoKN5RQEGgETQoOty7BVQu3Sg0tEEBFRv6N3+ct6Uo\n",
       "qrpOnXrP2e/l+1mrltWnznvOA77UenrvffaOzESSJEmL96DSASRJkrrCYiVJklQTi5UkSVJNLFaS\n",
       "JEk1sVhJkiTVxGIlSZJUk00Wq4h4SER8LyIujYgrI+Kk6vFHRsTZEXFNRJwVEdtNJK0kSVKDxXz7\n",
       "WEXEQzPz1xGxOXAhcCJwBHBbZr43It4GPCIz3z7+uJIkSc0171RgZv66+vbBwBZAAocCy6vHlwOH\n",
       "jSWdJElSi8xbrCLiQRFxKXALcFZmfh94dGbeUj3lFuDRY8woSZLUCsOMWG3IzKcBuwLPjIg/mPHz\n",
       "ZDCKJUmS1GubD/vEzPxlRJwHHATcEhE7ZebNEbEzsHa2ayLCwiVJklojM2OxLzDnF/AoYLvq+62A\n",
       "bwMHA+8F3lY9/nbgPXNcn5t6/bZ/ASd1+f3rev1RX2eU6xZyzTDPres5bf4q/c/nfb64a7zPJ3sf\n",
       "NPX9vc+Hfk4u9t/RfCNWOwPLI2IzBtOGn8vMMyLiIuDUiHgdcD3w8nlep6tWdPz963r9UV9nlOsW\n",
       "cs0wzx0lQ9es6Pj71/X6o77OKNct5JphnjtKhq5Z0fH3r+v1R32dUa5byDXDPHeUDAs273YLi3rx\n",
       "iMzFDqlJDRcRJ2XmSaVzSOPkfa4+qKO3uPO6tHgrSgeQJmBF6QBSGzhiJUmShCNWkiRJjWKxkiRJ\n",
       "qonFSpIkqSYWK0mSpJpYrCRJkmpisZIkSaqJxUqSJKkmFitJkqSaWKwkSZJqYrGSJEmqicVKkiSp\n",
       "JhYrSZKkmlisJEmSamKxkiRJqonFSpIkqSYWK0mSpJpYrCRJkmpisZIkSaqJxUqSJKkmFitJkqSa\n",
       "WKwkSZJqYrGSJEmqicVKkiSpJhYrSZKkmlisJEmSamKxkiRJqonFSpIkqSabj/sNIthh3O/RY7/O\n",
       "5K7SISRJ0kBk5vhePCIhbxvbG2gL4N+A92VyS+kwkiS1WURkZsZiXmPsI1aZjliNSwSPAd4GrIxg\n",
       "GRYsSZKKco1Vi2Xys0xOBJ7CYPRqZQTvj+DRhaNJktRLFqsOsGBJktQMFqsOsWBJklSWxaqDphWs\n",
       "fYAHA6ssWJIkjZ/FqsMyuSmTExiMYFmwJEkaM4tVD1iwJEmaDItVj8xSsFZG8L4IdiwcTZKkTrBY\n",
       "9dC0grUPsCWDESwLliRJi2Sx6jELliRJ9bJYaWbBegjwIwuWJEkLZ7HS71UF6y0M1mBZsCRJWiCL\n",
       "lR7AgiVJ0mgsVprTtILlFKEkSUOwWGlemayZUbBWRfBeC5YkSfdnsdLQphWspwJbYcGSJOl+LFZa\n",
       "MAuWJEmzs1hpZDMK1kMZrMGyYEmSestipUWrCtafMliD9VAcwZIk9ZTFSrWZVrAcwZIk9ZLFSrWb\n",
       "ZQTLgiVJ6gWLlcbGgiVJ6huLlcZujjVYfx/BDoWjSZJUK4uVJmZawXoasA2DESwLliSpMyxWmrhM\n",
       "VmfyZixYkqSOsVipmFkK1tUWLElSm1msVNy0gvVUHMGSJLWYxUqN4RShJKntLFZqHAuWJKmtLFZq\n",
       "rDkK1nssWJKkprJYqfFmFKyHYcGSJDWUxUqtURWs47FgSZIaymKl1rFgSZKaymKl1ppWsPbFgiVJ\n",
       "agCLlVovkxstWJKkJrBYqTMsWJKk0ixW6pwZBevhDI7KeU8EjyocTZLUcRYrdVZVsN7EYJH7xoL1\n",
       "dxYsSdK4WKzUedMK1r7AdliwJEljYrFSb1iwJEnjZrFS71iwJEnjYrFSb1mwJEl1s1ip96YVrP2A\n",
       "R2DBkiSNyGIlVTK5IZM3YsGSJI3IYiXNMEfB+t8WLEnSfCxW0hxmFKxHYsGSJM3DYiXNw4IlSRqW\n",
       "xUoa0hwF690R/nckSRqIzBzfi0dkZsbY3kAqKILdgTOBEzM5u3QeSdLi1NFbNvk37YhYEhHnRcRV\n",
       "EXFlRJxQPf7UiPj3iLg8Ik6LiG0XE0Jqo0xuAP4ZOK50FklSM2xyxCoidgJ2ysxLI2Ib4IfAYcAn\n",
       "gf+RmRdExHHAnpn5l7Nc74iVOi2C7YGfArtncnvpPJKk0Y19xCozb87MS6vv7wRWAbsAe2XmBdXT\n",
       "vgUcsZgQUltlsg44GziqdBZJUnlDL7qNiD0YHP3xPeCqiHhp9aMjgSW1J5PaYxlOB0qSGLJYVdOA\n",
       "XwBOzMw7gP8GHB8RPwC2AX43vohS430T2D2CJ5YOIkkqa/P5nhARWwBfBD6VmV8ByMyrgYOqn+8N\n",
       "vHgT15807Y8rMnPFIvJKjZPJPRGcDBwLvK1wHEnSkCJiKbC01tecZ/F6AMuBdZn51mmP75CZt0bE\n",
       "g4Ap4NzMnJrlehevqxcieBKD9Ya7ZXJP6TySpIUb++J1YH/g1cBzIuKS6utFwNERcTWDxexrZitV\n",
       "Up9kshJYDbygdBZJUjluECrVJII3AgdmcmTpLJKkhaujt1ispJpEsB1wA/DYahsGSVKLTGIqUNKQ\n",
       "qg1CzwCOLp1FklSGxUqql3taSVKPWaykep0D7BjBPqWDSJImz2Il1SiTexmcpXls4SiSpAJcvC7V\n",
       "LIK9gAuBXTNZXzqPJGk4Ll6XGiiTa4FrgYNLZ5EkTZbFShoPF7FLUg85FSiNQQTbMtiJfe9M1pbO\n",
       "I0man1OBUkNlcgdwGvCq0lkkSZNjsZLGZxlwXASO2kpST1ispPE5H9gW2Ld0EEnSZFispDHJZAOw\n",
       "HBexS1JvuHhdGqMI9gQuBnbJ5O7SeSRJc3PxutRwmVwHXAEcUjqLJGn8LFbS+LmnlST1hFOB0phF\n",
       "sDWwBnhSJj8vnUeSNDunAqUWyOQu4EvAa0pnkSSNl8VKmoxlwLHuaSVJ3WaxkibjO8AWwDNKB5Ek\n",
       "jY/FSpqATBKYwkXsktRpLl6XJiSCJcBlDPa0+k3pPJKk+3PxutQimawGfgAcVjqLJGk8LFbSZC0D\n",
       "ji0dQpI0Hk4FShMUwVbATcBTqxEsSVJDOBUotUy1tupU4LWls0iS6mexkibPPa0kqaMsVtLkfR+4\n",
       "B9i/dBBJUr0sVtKEVXtauYhdkjrIxetSARE8BrgK2LU6S1CSVJiL16WWyuRnwHeBI0pnkSTVx2Il\n",
       "leN0oCR1jFOBUiERbMlgT6unZ3Jd6TyS1HdOBUotlsndwCm4p5UkdYYjVlJBEfxn4AvA4zLZUDqP\n",
       "JPWZI1ZS+/0/4A7g2aWDSJIWz2IlFeSeVpLULU4FSoVFsCNwDbAkkztK55GkvnIqUOqATNYCK4CX\n",
       "F44iSVoki5XUDFM4HShJrWexkprhdGDvCPYqHUSSNDqLldQAmawHPg0cUzqLJGl0Ll6XGiKCfRiM\n",
       "XO2Ryb2l80hS37h4XeqQTC4H1gIHls4iSRqNxUpqFve0kqQWcypQapAItgd+wmA68PbSeSSpT5wK\n",
       "lDomk3XA2cBRpbNIkhbOYiU1zxRwXOkQkqSFs1hJzfNNYLcInlg6iCRpYSxWUsNkcg9wMi5il6TW\n",
       "cfG61EDVaNU5wG5V0ZIkjZmL16WOymQVcCPwgtJZJEnDs1hJzTWFi9glqVWcCpQaKoLtgOuBx1Xb\n",
       "MEiSxsipQKnDqg1CzwCOLp1FkjQci5XUbFM4HShJrWGxkprtHGDHCPYpHUSSND+LldRgmdwLfBL3\n",
       "tJKkVnDxutRwEewFXAjsmsn60nkkqatcvC71QCbXAtcAB5fOIknaNIuV1A5TuIhdkhrPqUCpBSLY\n",
       "FlgN7J3J2tJ5JKmLnAqUeiKTO4CvAq8qnUWSNDeLldQeU8BxETgKLEkNZbGS2uN8YFtg39JBJEmz\n",
       "s1hJLZHJBmA5LmKXpMZy8brUIhHsCVwM7JLJ3aXzSFKXuHhd6plMrgMuBw4pnUWS9EAWK6l9pnA6\n",
       "UJIayalAqWUi2BpYAzwpk5+XziNJXeFUoNRDmdwFfAl4TekskqT7s1hJ7bQM97SSpMaxWEnt9B1g\n",
       "c+AZpYNIku5jsZJaKJPEReyS1DguXpdaKoIlwGUM9rT6Tek8ktR2Ll6XeiyT1cAPgMNKZ5EkDVis\n",
       "pHZbhtOBktQYTgVKLRbBVsBNwFOrESxJ0oicCpR6rlpbdSrw2tJZJEkWK6kLlgHHuqeVJJVnsZLa\n",
       "7/vAPcD+pYNIUt9tslhFxJKIOC8iroqIKyPihOrxp0XERRFxSURcHBFPn0xcSTNVe1q5iF2SGmCT\n",
       "i9cjYidgp8y8NCK2AX7I4KPd/wf4QGZ+MyJeBPxFZj5nlutdvC5NQAQ7AyuBXauzBCVJCzT2xeuZ\n",
       "eXNmXlp9fyewCtgF2AA8vHradgw+lSSpkEx+DnwXOKJ0Fknqs6G3W4iIPYDzgScDuwLfBIJBOfsv\n",
       "mfmAj3o7YiVNTgQvA96cyQNGjyVJ86ujt2w+5BttA3wBODEz74yI44E/y8wvR8SRwCeA589x7UnT\n",
       "/rgiM1csJrCkOX0N+FgEe2ZyXekwktR0EbEUWFrra843YhURWwBfB87MzA9Wj92emdtV3wdwe2Y+\n",
       "fJZrHbGSJiiCDwPrMjmpdBZJapuxr7GqStPHgZUbS1XlZxHx7Or75wLXLCaEpNosA46JcCsVSSph\n",
       "vk8FHgB8G7gc2PjEdwK/YvDJwM2B3wDHZ+Yls1zviJU0QdUmoZcBJ2ZyXuk8ktQmdfQWzwqUOiaC\n",
       "twL7ZnrMjSQthMVK0gNEsCOD6fndMvlV6TyS1BYewizpATJZC6wAjiwcRZJ6x2IlddMy4NjSISSp\n",
       "byxWUjedAewdwV6lg0hSn1ispA7KZD3waRy1kqSJcvG61FER7AOcDuyRyb2l80hS07l4XdKcMrkc\n",
       "WAscWDqLJPWFxUrqNhexS9IEORUodVgE2wM/YTAdeHvpPJLUZE4FStqkTNYBZwOvKJ1FkvrAYiV1\n",
       "n9OBkjQhFiup+84CdovgiaWDSFLXWaykjsvkHuBkHLWSpLFz8brUA9Vo1TkMDma+p3QeSWoiF69L\n",
       "Gkomq4AbgYNKZ5GkLrNYSf3hInZJGjOnAqWeiGA74HrgcdU2DJKkaZwKlDS0aoPQM4BXls4iSV1l\n",
       "sZL6xelASRoji5XUL+cCO0awT+kgktRFFiupRzK5F1iOo1aSNBYuXpd6JoK9gAuBXTNZXzqPJDWF\n",
       "i9clLVgm1wLXAC8unUWSusZiJfWTi9glaQycCpR6KIJtgdXA3pmsLZ1HkprAqUBJI8nkDuCrwKtK\n",
       "Z5GkLrFYSf01BRwXgaPKklQTi5XUX+cD2wL7lQ4iSV1hsZJ6KpMNDEatji2bRJK6w8XrUo9FsCdw\n",
       "MbBLJneXziNJJbl4XdKiZHIdcDlwSOksktQFFitJU8BxpUNIUhc4FSj1XARbA2uAJ2fys9J5JKkU\n",
       "pwIlLVomdwFfBF5dOosktZ3FShK4p5Uk1cJiJQngO8BmwDNLB5GkNrNYSSKTxD2tJGnRXLwuCYAI\n",
       "lgCXMdjT6jel80jSpLl4XVJtMlnNYLPQw0pnkaS2slhJmm4K97SSpJE5FSjp9yLYisGeVvtmcmPp\n",
       "PJI0SU4FSqpVtbbqVOA1pbNIUhtZrCTNNAUc655WkrRwFitJM30fWA/sXzqIJLWNxUrS/Uzb08pF\n",
       "7JK0QC5el/QAEewMrAR2rc4SlKTOc/G6pLHI5OcMjrk5onQWSWoTi5WkuUzhdKAkLYhTgZJmFcGW\n",
       "wE3A0zO5rnQeSRo3pwIljU0mdwOnAMeUziJJbeGIlaQ5RbAf8CXgsZlsKJ1HksbJEStJ43YJ8Evg\n",
       "2aWDSFIbWKwkzck9rSRpYZwKlLRJEewIXAPslsmvSueRpHFxKlDS2GWyFlgBHFk4iiQ1nsVK0jCW\n",
       "4XSgJM3LYiVpGGcAe0WwV+kgktRkFitJ88pkPfBp4NjCUSSp0Vy8LmkoETwFOBPYPZN7S+eRpLq5\n",
       "eF3SxGRyBXAzcGDpLJLUVBYrSQsxhYvYJWlOTgVKGloE2wM/ZTAdeHvpPJJUJ6cCJU1UJuuAs4BX\n",
       "lM4iSU1ksZK0UO5pJUlzsFhJWqizgCURPKl0EElqGouVpAXJ5B7gZNzTSpIewMXrkhYsgicC5wJL\n",
       "qqIlSa3n4nVJRWSyCrgBOKh0FklqEouVpFG5iF2SZnAqUNJIItiOwajVY6ttGCSp1ZwKlFRMtUHo\n",
       "6cArS2eRpKawWElajGX46UBJ+j2LlaTFOBfYMYJ9SgeRpCawWEkaWSb3AstxEbskAS5el7RIETwe\n",
       "+C6waya/K51Hkkbl4nVJxWXyY+Bq4ODSWSSpNIuVpDq4iF2ScCpQUg0i2BZYDeydydrSeSRpFE4F\n",
       "SmqETO4Avgq8unQWSSrJYiWpLsuA4yJwlFpSb22yWEXEkog4LyKuiogrI+KE6vHPRcQl1dd1EXHJ\n",
       "ZOJKarBvA9sA+5UOIkmlbD7Pz9cDb83MSyNiG+CHEXF2Zh618QkR8X7g9nGGlNR8mWyIYIrBnlY/\n",
       "LBxHkorY5IhVZt6cmZdW398JrAIes/HnERHAy4FTxhlSUmssB14RwZalg0hSCUOvsYqIPYB9ge9N\n",
       "e/hZwC2Z+ZN6Y0lqo0yuBy4HDi0cRZKKmG8qEIBqGvALwInVyNVGRwOfmefak6b9cUVmrlhgRknt\n",
       "snFPq88XziFJmxQRS4Gltb7mfPtYRcQWwNeBMzPzg9Me3xxYA+yXmT+b41r3sZJ6JoKtGfxueHIm\n",
       "s/5ukKQmGvs+VtUaqo8DK6eXqsrzgFVzlSpJ/ZTJXcAXgdeUziJJkzbfGqv9GWz495xp2yu8sPrZ\n",
       "UbhoXdLslgHHuqeVpL7xSBtJtasK1dXAazO5qHQeSRqGR9pIaqRMEpjCg5kl9YwjVpLGIoIlwGXA\n",
       "Lpn8pnQeSZqPI1aSGiuT1cDFwOGls0jSpFisJI3Txj2tJKkXnAqUNDYRbMVgT6t9M7mxdB5J2hSn\n",
       "AiU1WrW26lTc00pST1isJI3bFO5pJaknLFaSxu37wHrggNJBJGncLFaSxqra08pF7JJ6wcXrksYu\n",
       "gp2BlcCu1VmCktQ4Ll6X1AqZ/Bz4DnBE6SySNE4WK0mTsgw4rnQISRonpwIlTUQEWzLY0+qZmfy0\n",
       "dB5JmsmpQEmtkcndwCnAa0tnkaRxccRK0sREsB/wJeCxmWwonUeSpnPESlLbXAL8ElhaOIckjYXF\n",
       "StLEuKeVpK5zKlDSREWwA3AtsFsmvyqdR5I2cipQUutkcitwHnBk6SySVDeLlaQSpnBPK0kdZLGS\n",
       "VMIZwF4R7F06iCTVyWIlaeIyWQ98CjimdBZJqpOL1yUVEcFTgDOB3TO5t3QeSXLxuqTWyuQK4Gbg\n",
       "wNJZJKkuFitJJU3hInZJHeJUoKRiIngk8FNgz0z+o3QeSf3mVKCkVsvkF8BZwFGls0hSHSxWkkqb\n",
       "wulASR1hsZJU2lnArhE8qXQQSVosi5WkojK5BzgZD2aW1AEuXpdUXARPYHB+4JKqaEnSxLl4XVIn\n",
       "ZPIj4HrgoMJRJGlRLFaSmmIKF7FLajmnAiU1QgQPB24AHpfJutJ5JPWPU4GSOiOTXwKnA68snUWS\n",
       "RmWxktQkUzgdKKnFLFaSmuRc4FERPLV0EEkahcVKUmNkci/wSdzTSlJLuXhdUqNE8Hjgu8Cumfyu\n",
       "dB5J/eHidUmdk8mPgR8BB5fOIkkLZbGS1ERTuIhdUgs5FSipcSLYFlgN/KdMbimdR1I/OBUoqZMy\n",
       "uQP4KvCq0lkkaSEsVpKaahlwXASOektqDYuVpKb6NrA1sF/pIJI0LIuVpEbKZAOwHBexS2oRi5Wk\n",
       "Jvs48LIIjikdRJKGsXnpAJI0l0zWRPAc4BsRPCqTD5TOJEmbYrGS1GiZrIrgAOCsCHYA3pHJ+PaJ\n",
       "kaRFcCpQUuNlshp4FvAc4F8j/EuhpGayWElqhUxuAw4EdgM+H8FDCkeSpAewWElqjUzuBA4Bfgec\n",
       "GcHDCkeSpPuxWElqlUzuBl4JrAJWRPDowpEk6fcsVpJaJ5N7gTcDpwEXRrBn4UiSBPipQEktVX0y\n",
       "8KQIbgUuiOBFmVxROpekfrNYSWq1TD4awTrgWxEckcmFpTNJ6i+nAiW1XiafBV4DfDmCF5fOI6m/\n",
       "LFaSOiGTs4CXAB+P4DWl80jqJ6cCJXVGJt+L4LncdwTOP5bOJKlfLFaSOiWTlTOOwPmfHoEjaVKc\n",
       "CpTUOZncCBwAPA/4F4/AkTQpFitJnTTtCJw9gFM9AkfSJFisJHVWJncwWNB+D3CGR+BIGjeLlaRO\n",
       "q47AORq4Gjgvgh0LR5LUYRYrSZ1XHYFzPPB1Bkfg7FE2kaSuckGnpF6oPhn4VxHcxqBcvTCTK0vn\n",
       "ktQtFitJvZLJh6tydU4Eh2fy3dKZJHWHU4GSeieTU4BjgK9GcHDpPJK6w2IlqZcy+QZwCPCJCF5d\n",
       "Oo+kbnAqUFJvZXLRjCNwPlg6k6R2s1hJ6rXqCJxncd8ROO/yCBxJo3IqUFLvZXIDgyNwXgB8LILN\n",
       "CkeS1FIWK0kCMrkVeC7wOOBzEWxZOJKkFrJYSVKlOgLnxUAyOAJn28KRJLWMxUqSpqmOwHkFcC0e\n",
       "gSNpgSxWkjRDdQTOm4AzgQsi2L1wJEkt4acCJWkW1ScD/1cEt3LfEThXlc4lqdksVpK0CZl8aMYR\n",
       "OP9eOpOk5nIqUJLmkclngOOA0yJ4Uek8kprLYiVJQ8jkTOBQYCqCV5XOI6mZNlmsImJJRJwXEVdF\n",
       "xJURccK0n70lIlZVj//9+KNKUlnVNOBzgfdEcMJ8z5fUP/OtsVoPvDUzL42IbYAfRsTZwE4M/ua2\n",
       "T2auj4gdxh1Ukpogk6tmHIHzlx6BI2mjTRarzLwZuLn6/s6IWAXsArwe+LvMXF/97NZxB5Wkpsjk\n",
       "+ggOYLAdww4RvLnaokFSzw29xioi9gD2Bb4H7A38cURcFBErIuIPxxNPkpopk7XAc4C9gM96BI4k\n",
       "GLJYVdOAXwBOzMw7GIx0PSIz/wj4c+DU8UWUpGbK5FcMjsAJ4HSPwJE07z5WEbEF8EXgU5n5lerh\n",
       "NcCXADLz4ojYEBHbZ+a6Wa4/adofV2TmikWnlqSGyOS3ERwF/BNwbgQHVwc6S2q4iFgKLK31NTPn\n",
       "XnMZEQEsB9Zl5lunPf4G4DGZ+VcRsTfwrczcbZbrMzOjzsCS1EQRBPBu4EjgBZncUDiSpAWqo7fM\n",
       "N2K1P/Bq4PKIuKR67B3AJ4BPRMQVwO+A1y4mhCS1XfXJwHd5BI7Ub5scsVr0iztiJamHIng18AHg\n",
       "pZlcVDqPpOHU0VvceV2SapbJpxgcgfO1CF5YOo+kybFYSdIYZHIGcBiwPIKjS+eRNBnzfipQkjSa\n",
       "TL4TwfOAMyPYPpOPlM4kabwsVpI0RplcUe3SvvEInJM8AkfqLqcCJWnMMrkeOAB4CfDRCDYrm0jS\n",
       "uFisJGkCph2B8wTgFI/AkbrJYiVJE1IdgXMwsBnwdY/AkbrHYiVJE5TJb4GXA9cD50TwqLKJJNXJ\n",
       "YiVJE5bJvcCfAN9isEv7A44Ek9ROfipQkgqoPhn4zmlH4ByUyarSuSQtjsVKkgrK5B8jWAecF8FL\n",
       "M/le6UySRudUoCQVlskngdcxOALnBaXzSBqdxUqSGiCT04HDgZMjeEXpPJJG41SgJDXELEfgfLR0\n",
       "JkkLY7GSpAapjsB5FvcdgfPXHoEjtYdTgZLUMJlcx+AInEOBj3gEjtQeFitJaqBMbmFwBM6TgU9H\n",
       "8ODCkSQNwWIlSQ2VyS+BFwJbMjgCZ5vCkSTNw2IlSQ1WHYFzJHAjHoEjNZ7FSpIaLpN7gNcD5wIX\n",
       "RLCkcCRJc/BTgZLUAtUnA98x4wicH5XOJen+LFaS1CKZ/EMEtwErIjg0k++XziTpPk4FSlLLVEfg\n",
       "vB44PYLnl84j6T4WK0lqoUy+xuAInE9F8PLSeSQNOBUoSS2VyYXViNWZETwqk38qnUnqO4uVJLVY\n",
       "JpfPOALnbzwCRyrHqUBJarlMfsrgCJzDgA9F+LtdKsX/+CSpAzK5GVgKPAWPwJGKsVhJUkdMOwJn\n",
       "K+C0CLYuHEnqHYuVJHVIdQTOy4CbGByBs33hSFKvWKwkqWOqI3D+O3A+HoEjTZSfCpSkDqo+Gfg2\n",
       "j8CRJstiJUkdlsn7qyNwzquOwLm4dCapy5wKlKSOy2QKeANwRgTPKxxH6jSLlST1QCanAUcAn4ng\n",
       "yNJ5pK5yKlCSeiKTb1dH4JwRwfaZfKx0JqlrLFaS1COZXDbjCJy/9QgcqT4WK0nqmUx+GsEBwDeA\n",
       "HSL4s0w2lM7VR9UO+TsCjyidRfWIzPH9RSUiMjNjbG8gSRpZBNsBpwFrgGMz+V3hSJ0QwRbADsBO\n",
       "wKOn/e+jZ3nsYcBtwC/Aclte/MFie4vFSpJ6LIKtgM8CWwJHZHJX4UiNFMHmDMrSbOVo5mMPB9YB\n",
       "NwO3VF83z/jfjd+vc7SwOeroLRYrSeq5qjT8K/AE4MWZ/KJwpImIYDMeWJbm+v4RDEaVZitHMx+7\n",
       "LZN7J/nPonpYrCRJtYgggPcCBwMHZbKmcKSRVGVpe4abhnsk8B/MXZamf39bdVSQOqyO3uLidUnS\n",
       "xiNw/jyCtdx3BM7VpXMBRPAgBmVpmGm47YFfMvtI0pUzHrvVsqS6WawkSb+XyfuqI3BWRHBIJj8Y\n",
       "x/tUZemRzD2aNP37HYBfMftI0soZj92ayfpxZJaG4VSgJOkBIjgU+Dfg6EzOGfKaYLAWaZhpuB2A\n",
       "OxluzdJaP7GoSXCNlSRpbCL4Y+ALwInAJcxfmHYE7mL+T8JtLEt3T/AfR5qXxUqSNFYRPI3BdgzB\n",
       "/Iu812by20JRpUWzWEmSJNWkjt7yoLrCSJIk9Z3FSpIkqSYWK0mSpJpYrCRJkmpisZIkSaqJxUqS\n",
       "JKkmFitJkqSaWKwkSZJqYrGSJEmqicVKkiSpJhYrSZKkmlisJEmSamKxkiRJqonFSpIkqSYWK0mS\n",
       "pJpYrCRJkmpisZIkSaqJxUqSJKkmFitJkqSaWKwkSZJqYrGSJEmqicVKkiSpJhYrSZKkmlisJEmS\n",
       "amKxkiSvtcPYAAAEK0lEQVRJqonFSpIkqSYWK0mSpJpYrCRJkmpisZIkSaqJxUqSJKkmFitJkqSa\n",
       "WKwkSZJqYrGSJEmqySaLVUQsiYjzIuKqiLgyIk6oHj8pItZExCXV1wsnE1dqnohYWjqDNG7e59Jw\n",
       "5huxWg+8NTOfDPwR8OaIeCKQwD9k5r7V1zfGHVRqsKWlA0gTsLR0AKkNNlmsMvPmzLy0+v5OYBWw\n",
       "S/XjGHO2xiv9N7hxv39drz/q64xy3UKuGea5pf8/boLS/w68zxd3jff5cEr/O/A+X9w1TbrPh15j\n",
       "FRF7APsCF1UPvSUiLouIj0fEdmPI1gZLO/7+db3+qK8zynULuWaY546SoWuWdvz963r9UV9nlOsW\n",
       "cs0wzx0lQ9cs7fj71/X6o77OKNct5JphnjtKhgWLzJz/SRHbACuAv83Mr0TEjsCt1Y/fDeycma+b\n",
       "5br5X1ySJKkhMnNRM3LzFquI2AL4OnBmZn5wlp/vAXwtM5+ymCCSJEltN9+nAgP4OLByeqmKiJ2n\n",
       "Pe1w4IrxxJMkSWqPTY5YRcQBwLeByxl8EhDgncDRwNOqx64D3pCZt4w3qiRJUrMNtcZKkiRJ83Pn\n",
       "dUmSpJpMvFhFxNKIuCAi/jkinj3p95cmJSK2joiLI+LFpbNI4xART6h+l38+It5YOo80DhHx0oj4\n",
       "l4j4bEQ8f77nlxix2gDcAWwJrCnw/tKk/AXwudIhpHHJzB9l5puAo4D9S+eRxiEzv5qZfwK8kcG9\n",
       "vkkjF6uI+ERE3BIRV8x4/IUR8aOIuDYi3jbLpRdk5sHA24G/HvX9pUkY9T6v/lazkvv2e5MaaxG/\n",
       "z4mIQxhsyXPGJLJKo1rMfV55F/CRed9n1MXrEfEs4E7gkxv3sIqIzYCrgecBNwEXM/gE4R8C+wHv\n",
       "y8yfVc99MPDpzDxypADSBIx6nwPHA1sDTwJ+AxyeflJEDbXY3+fV87+emS+ZdHZpWIv4ff5z4D3A\n",
       "WZl5znzvs/moATPzgmpz0OmeAfw4M6+vAn8WeGlmvgc4uXrscOAgYDvgw6O+vzQJo97nDP5mQ0Qc\n",
       "A9xqqVKTLeL3+bOB/8pgacfpk8orjWIR9/kJwIHAwyLi8Zn5fzf1PiMXqznsAqye9uc1wDOnPyEz\n",
       "vwx8ueb3lSZp3vt8o8xcPpFEUv2G+X1+PnD+JENJNRvmPv8Q8KFhX7Duxev+rVx94H2uPvA+Vx/U\n",
       "fp/XXaxuApZM+/MS/OSfusf7XH3gfa4+qP0+r7tY/QDYKyL2qBanHwWcVvN7SKV5n6sPvM/VB7Xf\n",
       "54vZbuEU4LvA3hGxOiKOy8x7gD8Fvsngo+afy8xViwkoleR9rj7wPlcfTOo+96xASZKkmnhWoCRJ\n",
       "Uk0sVpIkSTWxWEmSJNXEYiVJklQTi5UkSVJNLFaSJEk1sVhJkiTVxGIlSZJUE4uVJElSTf4/uc6m\n",
       "htcTxc4AAAAASUVORK5CYII=\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11efdd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Try different regularizations and pick the best!\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "regularizations = [0.0, 0.00001, 0.00003, 0.0001, 0.0003, 0.001, 0.003, 0.01]\n",
    "precisions = []\n",
    "### END YOUR CODE\n",
    "\n",
    "for regularization in regularizations:\n",
    "    random.seed(3141)\n",
    "    np.random.seed(59265)\n",
    "    weights = np.random.randn(dimVectors, 5)\n",
    "\n",
    "    trainset = dataset.getTrainSentences()\n",
    "    nTrain = len(trainset)\n",
    "    trainFeatures = np.zeros((nTrain, dimVectors))\n",
    "    trainLabels = np.zeros((nTrain,), dtype=np.int32)\n",
    "\n",
    "    for i in xrange(nTrain):\n",
    "        words, trainLabels[i] = trainset[i]\n",
    "        trainFeatures[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "\n",
    "    # We will do batch optimization\n",
    "    weights = sgd(lambda weights: softmax_wrapper(trainFeatures, trainLabels, weights, regularization), weights, 3.0, 10000, PRINT_EVERY=100)\n",
    "\n",
    "    # Prepare dev set features\n",
    "    devset = dataset.getDevSentences()\n",
    "    nDev = len(devset)\n",
    "    devFeatures = np.zeros((nDev, dimVectors))\n",
    "    devLabels = np.zeros((nDev,), dtype=np.int32)\n",
    "\n",
    "    for i in xrange(nDev):\n",
    "        words, devLabels[i] = devset[i]\n",
    "        devFeatures[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "\n",
    "    _, _, pred = softmaxRegression(devFeatures, devLabels, weights)\n",
    "    precisions.append(precision(devLabels, pred))\n",
    "    \n",
    "    print \"Regularization constant: %f, Dev precision (%%): %f\" % (regularization, precisions[-1])\n",
    "    \n",
    "plt.xscale('log')\n",
    "plt.plot(regularizations, precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "1e-05\t29.6094\n"
     ]
    }
   ],
   "source": [
    "# Write down the best regularization and accuracy you found\n",
    "# sanity check: your accuracy should be around or above 30%\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "best_idx = np.argmax(precisions)\n",
    "BEST_REGULARIZATION = regularizations[best_idx]\n",
    "BEST_ACCURACY = precisions[best_idx]\n",
    "\n",
    "### END YOUR CODE\n",
    "\n",
    "print \"=== For autograder ===\\n%g\\t%g\" % (BEST_REGULARIZATION, BEST_ACCURACY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "Test precision (%): 23.031674\n"
     ]
    }
   ],
   "source": [
    "# Test your findings on the test set\n",
    "\n",
    "testset = dataset.getTestSentences()\n",
    "nTest = len(testset)\n",
    "testFeatures = np.zeros((nTest, dimVectors))\n",
    "testLabels = np.zeros((nTest,), dtype=np.int32)\n",
    "\n",
    "for i in xrange(nTest):\n",
    "    words, testLabels[i] = testset[i]\n",
    "    testFeatures[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "    \n",
    "_, _, pred = softmaxRegression(testFeatures, testLabels, weights)\n",
    "print \"=== For autograder ===\\nTest precision (%%): %f\" % precision(testLabels, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Credit\n",
    "\n",
    "Train your own classifier for sentiment analysis! We will not provide any starter code for this part, but you can feel free to reuse the code you've written before, or write some new code for this task. Also feel free to refer to the code we provided you with to see how we scaffolded training for you.\n",
    "\n",
    "Try to contain all of your code in one code block. You could start by using multiple blocks, then paste code together and remove unnecessary blocks. Report, as the last two lines of the output of your block, the dev set accuracy and test set accuracy you achieved, in the format we used above.\n",
    "\n",
    "*Note: no credits will be given for this part if you use the dev or test sets for training, or if you fine-tune your regularization or other hyperparameters on the test set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "### END YOU CODE\n",
    "\n",
    "\n",
    "_, _, pred = softmaxRegression(devFeatures, devLabels, weights)\n",
    "print \"=== For autograder ===\\nDev precision (%%): %f\" % precision(devLabels, pred)\n",
    "_, _, pred = softmaxRegression(testFeatures, testLabels, weights)\n",
    "print \"Test precision (%%): %f\" % precision(testLabels, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
