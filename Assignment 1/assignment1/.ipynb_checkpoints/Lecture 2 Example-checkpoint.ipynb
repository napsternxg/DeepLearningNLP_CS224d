{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [\"I like deep learning.\", \"I like NLP.\", \"I enjoy flying.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tk = nltk.tokenize.WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'enjoy': 6, 'NLP': 5, 'like': 1, 'I': 0, 'flying': 7, 'deep': 2, '.': 4, 'learning': 3}\n",
      "[[1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 0, 0, 1, 1, 0, 0], [1, 0, 0, 0, 1, 0, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate the vocabulary and the scentence tokens.\n",
    "\"\"\"\n",
    "vocab = dict()\n",
    "vocab_list = []\n",
    "i = 0\n",
    "sent_tokens_vector = []\n",
    "for k in sentences:\n",
    "    words = tk.tokenize(k)\n",
    "    sent_tokens_vector.append(words)\n",
    "    for word in words:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = i\n",
    "            vocab_list.append(word)\n",
    "            i += 1\n",
    "print vocab\n",
    "sent_vectors = [[0]*len(vocab) for _ in sentences]\n",
    "for i, sent_tokens in enumerate(sent_tokens_vector):\n",
    "    for word in sent_tokens:\n",
    "        sent_vectors[i][vocab[word]] = 1\n",
    "print sent_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     I       like       deep   learning          .        NLP      enjoy     flying\n",
      "         I         0.0        2.0        0.0        0.0        0.0        0.0        1.0        0.0\n",
      "      like         2.0        0.0        1.0        0.0        0.0        1.0        0.0        0.0\n",
      "      deep         0.0        1.0        0.0        1.0        0.0        0.0        0.0        0.0\n",
      "  learning         0.0        0.0        1.0        0.0        1.0        0.0        0.0        0.0\n",
      "         .         0.0        0.0        0.0        1.0        0.0        1.0        0.0        1.0\n",
      "       NLP         0.0        1.0        0.0        0.0        1.0        0.0        0.0        0.0\n",
      "     enjoy         1.0        0.0        0.0        0.0        0.0        0.0        0.0        1.0\n",
      "    flying         0.0        0.0        0.0        0.0        1.0        0.0        1.0        0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate count co-occurence matrix\n",
    "\"\"\"\n",
    "co_occur_mat = np.zeros([len(vocab)]*2)\n",
    "for sent_vector in sent_tokens_vector:\n",
    "    for i, val in enumerate(sent_vector[:-1]):\n",
    "        word_id = vocab[val]\n",
    "        next_word_id = vocab[sent_vector[i+1]]\n",
    "        co_occur_mat[word_id][next_word_id] += 1\n",
    "        co_occur_mat[next_word_id][word_id] = co_occur_mat[word_id][next_word_id]\n",
    "\n",
    "\n",
    "\n",
    "print \"{:>10} \".format(\"\"),\" \".join(\"{:>10}\".format(k) for k in vocab_list)\n",
    "for i, row in enumerate(co_occur_mat):\n",
    "    print \"{:>10} \".format(vocab_list[i]),\" \".join(\"{:>10}\".format(k) for k in row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         I ~       deep = 2.0; [ 0.  2.  0.  0.  0.  0.  1.  0.]\t[ 0.  1.  0.  1.  0.  0.  0.  0.]\n",
      "         I ~        NLP = 2.0; [ 0.  2.  0.  0.  0.  0.  1.  0.]\t[ 0.  1.  0.  0.  1.  0.  0.  0.]\n",
      "         I ~     flying = 1.0; [ 0.  2.  0.  0.  0.  0.  1.  0.]\t[ 0.  0.  0.  0.  1.  0.  1.  0.]\n",
      "      like ~   learning = 1.0; [ 2.  0.  1.  0.  0.  1.  0.  0.]\t[ 0.  0.  1.  0.  1.  0.  0.  0.]\n",
      "      like ~          . = 1.0; [ 2.  0.  1.  0.  0.  1.  0.  0.]\t[ 0.  0.  0.  1.  0.  1.  0.  1.]\n",
      "      like ~      enjoy = 2.0; [ 2.  0.  1.  0.  0.  1.  0.  0.]\t[ 1.  0.  0.  0.  0.  0.  0.  1.]\n",
      "      deep ~          . = 1.0; [ 0.  1.  0.  1.  0.  0.  0.  0.]\t[ 0.  0.  0.  1.  0.  1.  0.  1.]\n",
      "      deep ~        NLP = 1.0; [ 0.  1.  0.  1.  0.  0.  0.  0.]\t[ 0.  1.  0.  0.  1.  0.  0.  0.]\n",
      "  learning ~        NLP = 1.0; [ 0.  0.  1.  0.  1.  0.  0.  0.]\t[ 0.  1.  0.  0.  1.  0.  0.  0.]\n",
      "  learning ~     flying = 1.0; [ 0.  0.  1.  0.  1.  0.  0.  0.]\t[ 0.  0.  0.  0.  1.  0.  1.  0.]\n",
      "         . ~      enjoy = 1.0; [ 0.  0.  0.  1.  0.  1.  0.  1.]\t[ 1.  0.  0.  0.  0.  0.  0.  1.]\n",
      "       NLP ~     flying = 1.0; [ 0.  1.  0.  0.  1.  0.  0.  0.]\t[ 0.  0.  0.  0.  1.  0.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Print word similarity\n",
    "\"\"\"\n",
    "for i, word in enumerate(vocab_list):\n",
    "    for j, word2 in enumerate(vocab_list[i+1:]):\n",
    "        score = np.dot(co_occur_mat[i], co_occur_mat[i+1+j])\n",
    "        if score > 1e-10: # Only print similar words in the vocab.\n",
    "            print \"{:>10} ~ {:>10} = {:>3}; {}\\t{}\".format(word, word2,score, co_occur_mat[i], co_occur_mat[i+1+j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8L, 8L)\n",
      "(8L,)\n",
      "(8L, 8L)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Singular value decomposition of co-occurence matrix.\n",
    "\"\"\"\n",
    "\n",
    "U, s, Vh = np.linalg.svd(co_occur_mat, full_matrices=False)\n",
    "print U.shape\n",
    "print s.shape\n",
    "print Vh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAA48AAALTCAYAAAC2Ujc3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAIABJREFUeJzt3X+Q7Xdd3/HXm3tBDZICBukYQoOQpqACMRAQ/HE1FAMq\n",
       "VEuhEVB+VIKdiG1RsWpNqD9aZ7S1osaIkEKnkiLEgQ6YSCt3oBACCfnBjyRNUNokKCMaKFF+JPDu\n",
       "H3uubpZ773uz9+6e3ezjMbNzz/d7PufkvfnOvfN97vecs9XdAQAAgMO5x7IHAAAAYPsTjwAAAIzE\n",
       "IwAAACPxCAAAwEg8AgAAMBKPAAAAjMQjsKtV1aur6uNV9YHDrPm1qrqhqq6uqlO2cj4AgO1CPAK7\n",
       "3QVJzjjUnVX11CQP6+6TkrwoyXlbNRgAwHYiHoFdrbvfmeTWwyx5WpLXLNZeluS+VfXArZgNAGA7\n",
       "EY8Ah3d8kptWbd+c5EFLmgUAYGnEI8Cs1mz3UqYAAFiivcseAGCbuyXJCau2H7TYdydVJSgBgB2j\n",
       "u9f+cHwkHgEO781Jzk5yYVU9Psknu/vjB1u4kX+EWb6qOre7z132HGyM47ezOX47l2O3s230h97i\n",
       "EdjVqup1Sb4tyXFVdVOSc5LcM0m6+/zufmtVPbWqbkzyV0mev7xpAQCWRzwCu1p3n7mONWdvxSwA\n",
       "ANuZD8wBYLfbv+wBOCL7lz0AR2T/sgdgw/YvewC2XnX7jAeAI1VV7T2PAMBOsNHzFlceAQAAGIlH\n",
       "AAAARuIRAACAkXgEAABgJB4BAAAYiUcAAABG4hEAAICReAQAAGAkHgEAABiJRwAAAEbiEQAAgJF4\n",
       "BAAAYCQeAQAAGIlHAAAARuIRAACAkXgEAABgJB4BAAAYiUcAAABG4hEAAICReAQAAGAkHgEAABiJ\n",
       "RwAAAEbiEQAAgJF4BAAAYCQeAQAAGIlHAAAARuIRAACAkXgEAABgJB4BAAAYiUcAAABG4hEAAICR\n",
       "eAQAAGAkHgEAABiJRwAAAEbiEQAAgJF4BAAAYCQeAQAAGIlHAAAARuIRAACAkXgEAABgJB4BAAAY\n",
       "iUcAAABG4hEAAICReAQAAGAkHgEAABiJRwAAAEbiEQAAgJF4BAAAYCQeAQAAGIlHAAAARuIRAACA\n",
       "kXgEAABgJB4BAAAYiUcAAABG4hEAAICReAQAAGAkHgEAABiJRwAAAEbiEQAAgJF4BAAAYCQeAQAA\n",
       "GIlHAAAARuIRAACAkXgEAABgJB4BAAAYiUcAAABG4hEAAICReAQAAGAkHgEAABiJRwAAAEbiEQAA\n",
       "gJF4BAAAYCQeAQAAGIlHAAAARuIRAACAkXgEAABgJB4BAAAYiUcAAABG4hHY1arqjKq6rqpuqKqX\n",
       "HeT+46rq4qq6qqo+WFXPW8KYAABLV9297BkAlqKq9iS5PsmTktyS5H1Jzuzua1etOTfJl3X3v66q\n",
       "4xbrH9jdd6x5ru7u2rLhAQA2aKPnLa48ArvZaUlu7O6PdvftSS5M8vQ1a/40ybGL28cm+Yu14QgA\n",
       "sBvsXfYAAEt0fJKbVm3fnORxa9a8MskfVdXHktwnyTO3aDYAgG3FlUdgN1vP6/Z/KslV3f01SR6d\n",
       "5Deq6j6bOxYAwPbjyiOwm92S5IRV2ydk5erjak9I8gtJ0t0fqao/SXJyksvXPtni/ZEH7O/u/Udz\n",
       "WACAjaiqfUn2HfHz+MAcYLeqqr1Z+QCc05N8LMl786UfmPMfknyqu19eVQ9MckWSR3b3X655Lh+Y\n",
       "AwDsCBs9b3HlEdi1uvuOqjo7ySVJ9iR5VXdfW1VnLe4/P8kvJrmgqq7Oykv9f2JtOAIA7AauPAIc\n",
       "Ba48AgA7hV/VAQAAwKYRjwAAAIzEIwAAACPxCAAAwEg8AgAAMBKPAAAAjMQjAAAAI/EIAADASDwC\n",
       "AAAwEo8AAACMxCMAAAAj8QgAAMBIPAIAADASjwAAAIzEIwAAACPxCAAAwEg8AgAAMBKPAAAAjMQj\n",
       "AAAAI/EIAADASDwCAAAwEo8AAACMxCMAAAAj8QgAAMBIPAIAADASjwAAAIzEIwAAACPxCAAAwEg8\n",
       "AgAAMBKPAAAAjMQjAAAAI/EIAADASDwCAAAwEo8AAACMxCMAAAAj8QgAAMBIPAIAADASjwAAAIzE\n",
       "IwAAACPxCAAAwEg8AgAAMBKPAAAAjMQjAAAAI/EIAADASDwCAAAwEo8AAACMxCMAAAAj8QgAAMBI\n",
       "PAIAADASjwAAAIzEIwAAACPxCAAAwEg8AgAAMBKPAAAAjMQjAAAAI/EIAADASDwCAAAwEo8AAACM\n",
       "xCMAAAAj8QgAAMBIPAIAADASjwAAAIzEIwAAACPxCAAAwOiw8VhVty3+/Jqq+r3F7edV1Su2YjgA\n",
       "AAC2h73D/Z0k3f2xJP9k9T4AAAB2j3W9bLWqTqyqDxzYXLX/u6rq3VX1VVX15MXtK6rq9VV1702Z\n",
       "GAAAgC234fc8VtX3JnlZkqdkJSh/Osnp3X1qkiuS/KujMiEAAABLN71s9VC+I8ljkvzD7r6tqr47\n",
       "ySOSvLuqkuReSd59dEYEAABg2TYSj53kI0kekuTkrFxlTJK3dff3H63BAAAA2D428rLVSvJ/kjwj\n",
       "yWur6hFJLkvyxKp6aJJU1b2r6qSjNyYAAADLNMVjH+R2J+nuvj7Js5P8XpKvTPK8JK+rqquz8pLV\n",
       "k4/uqAAAACxLdfvNGwBHqqq6u2teCQCwXBs9b9nwp60CAACwe4hHAAAARuIRAACAkXgEAABgJB4B\n",
       "AAAYiUcAAABG4hEAAICReAQAAGAkHoFdrarOqKrrquqGqnrZIdbsq6orq+qDVbV/i0cEANgWqruX\n",
       "PQPAUlTVniTXJ3lSkluSvC/Jmd197ao1903yriTf2d03V9Vx3f2JgzxXd3dt0egAABu20fMWVx6B\n",
       "3ey0JDd290e7+/YkFyZ5+po135/kjd19c5IcLBwBAHYD8QjsZscnuWnV9s2LfaudlOT+VfX2qrq8\n",
       "qp67ZdMBAGwje5c9AMASred1+/dM8o1JTk9yTJJLq+o93X3D2oVVde6qzf3dvf9oDAkAcCSqal+S\n",
       "fUf6POIR2M1uSXLCqu0TsnL1cbWbknyiuz+T5DNV9Y4kj0ryJfHY3edu0pwAABu2+IH2/gPbVXXO\n",
       "Rp7Hy1aB3ezyJCdV1YlVda8kz0ry5jVr3pTkm6tqT1Udk+RxST68xXMCACydK4/ArtXdd1TV2Uku\n",
       "SbInyau6+9qqOmtx//ndfV1VXZzkmiRfTPLK7haPAMCu41d1ABwFflUHALBT+FUdAAAAbBrxCAAA\n",
       "wEg8AgAAMBKPAAAAjMQjAAAAI/EIAADASDwCAAAwEo8AAACMxCMAAAAj8QgAAMBIPAIAADASjwAA\n",
       "AIzEIwAAACPxCAAAwEg8AgAAMBKPAAAAjMQjAAAAI/EIAADASDwCAAAwEo8AAACMxCMAAAAj8QgA\n",
       "AMBIPAIAADASjwAAAIzEIwAAACPxCAAAwEg8AgAAMBKPAAAAjMQjAAAAI/EIAADASDwCAAAwEo8A\n",
       "AACMxCMAAAAj8QgAAMBIPAIAADASjwAAAIzEIwAAACPxCAAAwEg8AgAAMBKPAAAAjMQjAAAAI/EI\n",
       "AADASDwCAAAwEo8AAACMxCMAAAAj8QgAAMBIPAIAADASjwAAAIzEIwAAACPxCAAAwEg8AgAAMBKP\n",
       "AAAAjMQjAAAAI/EIAADASDwCAAAwEo8AAACMxCMAAAAj8QgAAMBIPAIAADASjwAAAIzEIwAAACPx\n",
       "CAAAwEg8AgAAMBKPAAAAjMQjAAAAI/EIAADASDwCAAAwEo8AAACMxCMAAAAj8QgAAMBIPAIAADAS\n",
       "jwAAAIzEIwAAACPxCAAAwEg8AgAAMBKPAAAAjMQjAAAAI/EIAADASDwCAAAwEo8AAACMxCOwq1XV\n",
       "GVV1XVXdUFUvO8y6x1bVHVX1fVs5HwDAdiEegV2rqvYk+fUkZyR5RJIzq+rhh1j3S0kuTlJbOiQA\n",
       "wDYhHoHd7LQkN3b3R7v79iQXJnn6Qdb9SJI3JPnzrRwOAGA7EY/AbnZ8kptWbd+82Pc3qur4rATl\n",
       "eYtdvTWjAQBsL+IR2M3WE4K/muQnu7uz8pJVL1sFAHalvcseAGCJbklywqrtE7Jy9XG1U5NcWFVJ\n",
       "clySp1TV7d395rVPVlXnrtrc3937j+q0AAAbUFX7kuw74udZ+WE6wO5TVXuTXJ/k9CQfS/LeJGd2\n",
       "97WHWH9Bkv/e3Rcd5L7ublclAYBtb6PnLa48ArtWd99RVWcnuSTJniSv6u5rq+qsxf3nL3VAAIBt\n",
       "xJVHgKPAlUcAYKfY6HmLD8wBAABgJB4BAAAYiUcAAABG4hEAAICReAQAAGAkHgEAABiJRwAAAEbi\n",
       "EQAAgJF4BAAAYCQeAQAAGIlHAAAARuIRAACAkXgEAABgJB4BAAAYiUcAAABG4hEAAICReAQAAGAk\n",
       "HgEAABiJRwAAAEbiEQAAgJF4BAAAYCQeAQAAGIlHAAAARuIRAACAkXgEAABgJB4BAAAYiUcAAABG\n",
       "4hEAAICReAQAAGAkHgEAABiJRwAAAEbiEQAAgJF4BAAAYCQeAQAAGIlHAAAARuIRAACAkXgEAABg\n",
       "JB4BAAAYiUcAAABG4hEAAICReAQAAGAkHgEAABiJRwAAAEbiEQAAgJF4hCNQVWdV1XOXPQcAAGy2\n",
       "6u5lzwCw41VVd3ctew4AgMlGz1tceYQ1quo5VXVZVV1ZVb9VVXuq6raq+vmquqqqLq2qr16sPbeq\n",
       "Xrq4/eiqek9VXV1VF1XVfavqoVV1xarnPmn1NgAA7BTiEVapqocneWaSJ3T3KUm+kOTZSY5Jcml3\n",
       "PzrJO5L80OIhvfhKktcm+fHuflSSDyQ5p7s/kuRTVfWoxZrnJ3n1lnwzAABwFIlHuLPTk5ya5PKq\n",
       "ujLJdyR5SJLPd/dbFmuuSHLi6gdV1bFJ/k53v3Ox6zVJvnVx+3eSPL+q7pGVMP3dTf0OAABgE4hH\n",
       "+FKv6e5TFl8P7+6XJ7l91f1fTLJ3eI7VryG/KMlTknx3ksu7+9ajOy4AAGw+8Qh39j+TPKOqHpAk\n",
       "VXX/qvp704O6+/8lubWqvnmx67lJ9i/u+2ySS5Kcl+SCzRgaAAA223T1BHaV7r62qn4myR8uXmb6\n",
       "+SRn52/f15jc+X2Oq/1gkt+qqmOSfCQr72884HeTfG+SP9yUwQEAYJP5VR1wBKrqFVl5KeprhnU/\n",
       "luQ+3X3O1kzGVvOrOgCAnWKj5y2uPMIGVdXPJXlskp8d1v1+Vj505zu2Yi4AANgMrjwCHAWuPAIA\n",
       "O8VGz1t8YA4AAAAj8QgAAMBIPAIAADASjwAAAIzEIwAAACPxCAAAwEg8AgAAMBKPAAAAjMQjAAAA\n",
       "I/EIAADASDwCAAAwEo8AAACMxCMAAAAj8QgAAMBIPAIAADASjwAAAIzEIwAAACPxCAAAwEg8AgAA\n",
       "MBKPAAAAjMQjAAAAI/EIAADASDwCAAAwEo8AAACMxCMAAAAj8QjselV1RlVdV1U3VNXLDnL/s6vq\n",
       "6qq6pqreVVWPXMacAADLVN297BkAlqaq9iS5PsmTktyS5H1Jzuzua1et+aYkH+7uT1XVGUnO7e7H\n",
       "r3me7u7awtEBADZko+ctrjwCu91pSW7s7o929+1JLkzy9NULuvvS7v7UYvOyJA/a4hkBAJZOPAK7\n",
       "3fFJblq1ffNi36G8MMlbN3UiAIBtaO+yBwBYsnW/dr+qvj3JC5I8cfPGAQDYnsQjsNvdkuSEVdsn\n",
       "ZOXq450sPiTnlUnO6O5bD/ZEVXXuqs393b3/6I0JALAxVbUvyb4jfh4fmAPsZlW1NysfmHN6ko8l\n",
       "eW++9ANzHpzkj5I8p7vfc4jn8YE5AMCOsNHzFlcegV2tu++oqrOTXJJkT5JXdfe1VXXW4v7zk/xs\n",
       "kvslOa+qkuT27j5tWTMDACyDK48AR4ErjwDATuFXdQAAALBpxCMAAAAj8QgAAMBIPAIAADASjwAA\n",
       "AIzEIwAAACPxCAAAwEg8AgAAMBKPAAAAjMQjAAAAI/EIAADASDwCAAAwEo8AAACMxCMAAAAj8QiD\n",
       "qnpLVf3dZc8BAADLVN297BkAdryq6u6uZc8BADDZ6HmLK48AAACMxCMAAAAj8QgAAMBIPAIAADAS\n",
       "jwAAAIzEIwAAACPxCAAAwEg8AgAAMBKPAAAAjMQjAAAAI/EIAADASDwCAAAwEo8AAACMxCMAAAAj\n",
       "8QgAAMBIPAIAADASjwAAAIzEIwAAACPxCAAAwEg8AgAAMBKPAAAAjMQjAAAAI/EIAADASDwCAAAw\n",
       "Eo8AAACMxCMAAAAj8QgAAMBIPAIAADASjwAAAIzEIwAAACPxCAAAwEg8AgAAMBKPAAAAjMQjAAAA\n",
       "I/EIAADASDwCAAAwEo8AAACMxCMAAAAj8QgAAMBIPAIAADASjwAAAIzEIwAAACPxCAAAwEg8AgAA\n",
       "MBKPAAAAjMQjAAAAI/EI21RV3bYF/42zquq5m/3fAQBg56vuXvYMwEFU1ae7+z5H4Xnu0d1fPBoz\n",
       "cWhV1d1dy54DAGCy0fMWVx5hB6iqH6+q91bV1VV17qr9v19Vl1fVB6vqh1btv62qfrmqrkryTYvt\n",
       "n6+qq6rq0qr66sW6c6vqpYvb+6vq31fVZVV1fVV982L/MVX1+qr6UFVdVFXvqapTt/b/AAAAyyYe\n",
       "YZurqicneVh3n5bklCSnVtW3LO5+QXc/Jsljk7ykqu632H9Mkvd096O7+12L7Uu7+9FJ3pHkQGj2\n",
       "4uvA7T3d/bgk/yLJOYv9/zzJX3T31yX5N0lOXfUYAAB2CfEI29+Tkzy5qq5MckWSk5M8bHHfjy6u\n",
       "Ll6a5IQkJy32fyHJG1c9x+e7+y2L21ckOfEQ/62LFn++f9WaJya5MEm6+0NJrjmC7wUAgB1q77IH\n",
       "ANbl33X3b6/eUVX7kpye5PHd/dmqenuSL1/c/dm+8xuab191+4s59N/9zy3+/MKaNd7LBwCwy7ny\n",
       "CNvfJUleUFX3TpKqOr6qHpDk2CS3LsLxHyR5/AaeuzKH4buSPHPx335Ekm/YwH8HAIAdzpVH2L46\n",
       "Sbr7bVX18CSXVlWSfDrJc5JcnOTFVfXhJNdn5aWrd3rsIbbXvs/xUO9fPLD/N5O8pqo+lOS6JB9K\n",
       "8qmNfEMAAOxcflUHcFhVdY8k9+zuz1XVQ5O8Lcnf7+47ljzatuJXdQAAO8VGz1tceQQm907yR1V1\n",
       "z6y8xPWHhSMAwO7jyiPAUeDKIwCwU2z0vMUH5gAAADASjwAAAIzEIwAAACPxCAAAwEg8AgAAMBKP\n",
       "AAAAjMQjAAAAI/EIAADASDwCu1pVnVFV11XVDVX1skOs+bXF/VdX1SlbPSObq6r2LXsGNs7x29kc\n",
       "v53LsdudxCOwa1XVniS/nuSMJI9IcmZVPXzNmqcmeVh3n5TkRUnO2/JB2Wz7lj0AR2TfsgfgiOxb\n",
       "9gBs2L5lD8DWE4/AbnZakhu7+6PdfXuSC5M8fc2apyV5TZJ092VJ7ltVD9zaMQEAlk88ArvZ8Ulu\n",
       "WrV982LftOZBmzwXAMC2s3fZAwAsUa9zXa3ncVW13udjm6mqc5Y9Axvn+O1sjt/O5djtPuIR2M1u\n",
       "SXLCqu0TsnJl8XBrHrTYdyfdvTYwAQDuVrxsFdjNLk9yUlWdWFX3SvKsJG9es+bNSX4gSarq8Uk+\n",
       "2d0f39oxAQCWz5VHYNfq7juq6uwklyTZk+RV3X1tVZ21uP/87n5rVT21qm5M8ldJnr/EkQEAlqa6\n",
       "vUUHAACAw/OyVYC7oKrOqKrrquqGqnrZIdb82uL+q6vqlK2ekYObjl1VPXtxzK6pqndV1SOXMScH\n",
       "t56/e4t1j62qO6rq+7ZyPg5tnf9u7quqK6vqg1W1f4tH5DDW8W/ncVV1cVVdtTh+z1vCmBxEVb26\n",
       "qj5eVR84zJq7dM4iHgHWqar2JPn1JGckeUSSM6vq4WvWPDXJw7r7pCQvSnLelg/Kl1jPsUvyx0m+\n",
       "tbsfmeTnkvz21k7Joazz+B1Y90tJLs6XfkoyS7DOfzfvm+Q3knxPd399kmds+aAc1Dr/7p2d5Mru\n",
       "fnSSfUl+paq8NW57uCArx+6gNnLOIh4B1u+0JDd290e7+/YkFyZ5+po1T0vymiTp7suS3LeqHri1\n",
       "Y3IQ47Hr7ku7+1OLzcvi93luJ+v5u5ckP5LkDUn+fCuH47DWc+y+P8kbu/vmJOnuT2zxjBzaeo7f\n",
       "nyY5dnH72CR/0d13bOGMHEJ3vzPJrYdZcpfPWcQjwPodn+SmVds3L/ZNa0TI8q3n2K32wiRv3dSJ\n",
       "uCvG41dVx2flpPbAT859qMP2sJ6/eycluX9Vvb2qLq+q527ZdEzWc/xemeTrqupjSa5O8qNbNBtH\n",
       "7i6fs7ikDLB+6z0ZXftyOSexy7fuY1BV357kBUmeuHnjcBet5/j9apKf7O6uqoqXrW4X6zl290zy\n",
       "jUlOT3JMkkur6j3dfcOmTsZ6rOf4/VSSq7p7X1U9NMnbqupR3f3pTZ6No+MunbOIR4D1uyXJCau2\n",
       "T8jKT+kOt+ZBi30s13qOXRYfkvPKJGd09+Fe6sPWWs/xOzXJhSvdmOOSPKWqbu/utb+7la21nmN3\n",
       "U5JPdPdnknymqt6R5FFJxOPyref4PSHJLyRJd3+kqv4kyclZ+V3KbG93+ZzFy1YB1u/yJCdV1YlV\n",
       "da8kz0qy9sT0zUl+IEmq6vFJPtndH9/aMTmI8dhV1YOTXJTkOd194xJm5NDG49fdX9vdD+nuh2Tl\n",
       "fY8/LBy3hfX8u/mmJN9cVXuq6pgkj0vy4S2ek4Nbz/G7LsmTkmTxfrmTs/IBZGx/d/mcxZVHgHXq\n",
       "7juq6uwklyTZk+RV3X1tVZ21uP/87n5rVT21qm5M8ldJnr/EkVlYz7FL8rNJ7pfkvMXVq9u7+7Rl\n",
       "zczfWufxYxta57+b11XVxUmuSfLFJK/sbvG4Dazz794vJrmgqq7OyoWpn+juv1za0PyNqnpdkm9L\n",
       "clxV3ZTknKy8THzD5yzV7a04AAAAHJ6XrQIAADASjwAAAIzEIwAAACPxCAAAwEg8AgAAMBKPAAAA\n",
       "jMQjAAAAI/EIAADASDwCAAAwEo8AAACMxCMAAAAj8QgAAMBIPAIAADASjwAAAIzEIwAAACPxCAAA\n",
       "wEg8AgAAMBKPAAAAjMQjAAAAI/EIAADASDwCAAAwEo8AAACMxCMAAAAj8QgAAMBIPAIAADASjwAA\n",
       "AIzEIwAAACPxCAAAwEg8AgAAMBKPAAAAjMQjAAAAI/EIAADASDwCAAAwEo8AAACMxCMAAAAj8QgA\n",
       "AMBIPAIAADASjwAAAIzEIwAAACPxCAAAwEg8AgAAMBKPAAAAjMQjAAAAI/EIAADASDwCAAAwEo8A\n",
       "AACMxCMAAAAj8QgAAMBIPAIAADASjwAAAIzEI+xAVfWSqvpwVd1cVa8Y1r68qk7fqtkAALh7qu5e\n",
       "9gzAXVRV1yZ50uLrMd39I0seCQCAuzlXHmGHqarfSvK1Sf4gyf0W+76yqv64qvYuto89sF1V/7mq\n",
       "/vFi/0er6tyquqKqrqmqkxf7H1BVb6uqD1bVKxfr7r+kbxEAgG1IPMIO090vTvKxJPuS3LrYd1uS\n",
       "/Um+a7HsnyZ5Y3ffkaQXX1n8+efdfWqS85L82GL/OUn+R3d/fZI3JHnwpn8jAADsKOIRdq5afB3w\n",
       "O0mev7j9vCQXHOJxFy3+fH+SExe3n5jkwiTp7kuyiFIAADhAPMLO9jdvWu7udyc5sar2JdnT3R8+\n",
       "xGM+t/jzC0n2rtpfB1kLAABJxCPsdGuD77VJ/muSV9/F53lXkmcmSVU9OYv3UgIAwAHiEXamXvN1\n",
       "wO9mJfxedxeeI0lenuTJVfWBJM9I8mdJPn3UpgUAYMfzqzrgbqSqnpHke7r7B+/i4+6V5Avd/YWq\n",
       "+qYkv9Hd37gpQwIAsCPtnZcAO0FVvSLJdyZ56gYe/uAkr6+qeyT5fJIfOpqzAQCw87nyCAAAwMh7\n",
       "HgEAABiJRwAAAEbiEQAAgJF4BAAAYCQeAQAAGIlHAAAARuIRAACAkXgEAABgJB4BAAAYiUcAAABG\n",
       "4hEAAICReAQAAGAkHgEAABiJRwAAAEbiEQAAgJF4BAAAYCQeAQAAGIlHAAAARuIRAACAkXgEAABg\n",
       "JB4BAAAYiUcAAABG4hEAAICReAQAAGAkHgEAABiJRwAAAEbiEQAAgJF4BAAAYCQeuVurqnOr6qXL\n",
       "ngMAAHY68cjdXS97AAAAuDsQj9ztVNVPV9X1VfXOJCcv9j20qv6gqi6vqndU1YH9D6iqN1TVexdf\n",
       "T1jsP7eq/ktVvbuq/ndV/bMlfksAALB0e5c9ABxNVXVqkmcleVSSeyZ5f5Irkpyf5MXdfWNVPS7J\n",
       "byY5Pcl/SvIfu/tdVfXgJBcnecTi6b4+yeOTfGWSK6vqLd39p1v6DQEAwDYhHrm7+ZYkF3X3Z5N8\n",
       "tqrenOTLkzwhye9V1YF191r8+aQkD1+1/z5Vde+svNz1Td39uSSfq6q3JzktyZu25tsAAIDtRTxy\n",
       "d9NJas2+eyT5ZHefcpD1leRx3f35O+2stU+RJPniUZkQAAB2IO955O7mHUn+UVV9eVXdJ8n3JPnr\n",
       "JH9SVc9IklrxyMX6P0zykgMPrqpHH7iZ5OlV9WVV9VVJ9iV53xZ9DwAAsO2IR+5WuvvKJP8tydVJ\n",
       "3prkvVm5GvnsJC+sqquSfDDJ0xYPeUmSx1TV1VX1oSQvOvBUSa5J8vYklyb5t939Z1v2jQAAwDZT\n",
       "3X6TAaxVVeckua27f2XZswAAwHbgyiMcmp+sAADAgiuPAAAAjFx5BAAAYCQeAQAAGIlHAAAARuIR\n",
       "AACAkXgEAABgJB4BAAAYiUcAAABG4hEAAICReAQAAGAkHgEAABiJR7a9qvpiVf3yqu0fq6pzFrfP\n",
       "raqXHuQxX6iqK6vqA1X1+qr6iq2cGQAA7m7EIzvB55N8b1V91WK7V93XB1mfJH/d3ad09zcsHv/i\n",
       "zRwQAADu7sQjO8HtSX47yb/c4OP/V5KHHb1xAABg9xGP7BS/meTZVXXsXXlQVe1N8pQk12zKVAAA\n",
       "sEvsXfYAsB7d/emqem2SlyT5zDoe8hVVdeXi9juSvGrThgMAgF1APLKT/GqS9ye5YB1rP9Pdp2zy\n",
       "PAAAsGs6emyyAAADVElEQVR42So7RnffmuT1SV6Yv/2gnFreRAAAsHuIR3aC1Z+o+itJjltz389U\n",
       "1U2Lr/97kMcAAABHqLqdYwMAAHB4rjwCAAAwEo8AAACMxCMAAAAj8QgAAMBIPAIAADASjwAAAIzE\n",
       "IwAAACPxCAAAwEg8AgAAMBKPAAAAjMQjAAAAI/EIAADASDwCAAAwEo8AAACMxCMAAAAj8QgAAMBI\n",
       "PAIAADASjwAAAIzEIwAAACPxCAAAwEg8AgAAMBKPAAAAjMQjAAAAI/EIAADASDwCAAAwEo8AAACM\n",
       "xCMAAAAj8QgAAMBIPAIAADASjwAAAIzEIwAAACPxCAAAwEg8AgAAMBKPAAAAjMQjAAAAI/EIAADA\n",
       "SDwCAAAwEo8AAACMxCMAAAAj8QgAAMBIPAIAADASjwAAAIzEIwAAACPxCAAAwEg8AgAAMBKPAAAA\n",
       "jMQjAAAAI/EIAADASDwCAAAwEo8AAACMxCMAAAAj8QgAAMBIPAIAADASjwAAAIzEIwAAACPxCAAA\n",
       "wEg8AgAAMBKPAAAAjMQjAAAAI/EIAADASDwCAAAwEo8AAACMxCMAAAAj8QgAAMBIPAIAADASjwAA\n",
       "AIzEIwAAACPxCAAAwEg8AgAAMBKPAAAAjMQjAAAAI/EIAADASDwCAAAwEo8AAACMxCMAAAAj8QgA\n",
       "AMBIPAIAADASjwAAAIzEIwAAACPxCAAAwEg8AgAAMBKPAAAAjMQjAAAAI/EIAADASDwCAAAwEo8A\n",
       "AACMxCMAAAAj8QgAAMBIPAIAADASjwAAAIzEIwAAACPxCAAAwEg8AgAAMBKPAAAAjMQjAAAAI/EI\n",
       "AADASDwCAAAwEo8AAACMxCMAAAAj8QgAAMBIPAIAADASjwAAAIzEIwAAACPxCAAAwEg8AgAAMBKP\n",
       "AAAAjMQjAAAAI/EIAADASDwCAAAwEo8AAACMxCMAAAAj8QgAAMBIPAIAADASjwAAAIzEIwAAAKMd\n",
       "G49VdduyZwAAANgtdmw8JullDwAAALBb7OR4BAAAYIuIRwAAAEbiEQAAgJF4BAAAYCQeAQAAGO3k\n",
       "ePRpqwAAAFukujUYAAAAh7eTrzwCAACwRcQjAAAAI/EIAADASDwCAAAwEo8AAACMxCMAAAAj8QgA\n",
       "AMBIPAIAADD6/63vqGnX3umpAAAAAElFTkSuQmCC\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0x172a8b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Plot the similarity of words on 2 dimensions.\n",
    "\"\"\"\n",
    "for i, word in enumerate(vocab_list):\n",
    "    plt.text(U[i,0], U[i,1], word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 1:\n",
    "        x = x.reshape(1,x.shape[0])\n",
    "    #print \"Computing softmax\"\n",
    "    #print x, x.shape\n",
    "    max_row = np.max(x,axis=1).reshape(x.shape[0],1)\n",
    "    x = x - max_row\n",
    "    exp_x = np.exp(x)\n",
    "    sum_row = np.sum(exp_x,axis=1).reshape(x.shape[0],1)\n",
    "    #print exp_x\n",
    "    #print sum_row\n",
    "    #print max_row\n",
    "    return exp_x/(sum_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing softmax\n",
      "[[1 2]\n",
      " [3 4]] (2L, 2L)\n",
      "[[ 0.36787944  1.        ]\n",
      " [ 0.36787944  1.        ]]\n",
      "[[ 1.36787944]\n",
      " [ 1.36787944]]\n",
      "[[2]\n",
      " [4]]\n",
      "[[ 0.26894142  0.73105858]\n",
      " [ 0.26894142  0.73105858]]\n",
      "Computing softmax\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]] (3L, 2L)\n",
      "[[ 0.36787944  1.        ]\n",
      " [ 0.36787944  1.        ]\n",
      " [ 0.36787944  1.        ]]\n",
      "[[ 1.36787944]\n",
      " [ 1.36787944]\n",
      " [ 1.36787944]]\n",
      "[[2]\n",
      " [4]\n",
      " [6]]\n",
      "[[ 0.26894142  0.73105858]\n",
      " [ 0.26894142  0.73105858]\n",
      " [ 0.26894142  0.73105858]]\n",
      "Computing softmax\n",
      "[[1 2]] (1L, 2L)\n",
      "[[ 0.36787944  1.        ]]\n",
      "[[ 1.36787944]]\n",
      "[[2]]\n",
      "[[ 0.26894142  0.73105858]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1,2],[3,4]])\n",
    "#the output of your functions should be\n",
    "X_out = [[0.2689, 0.7311],[0.2689, 0.7311]]\n",
    "print softmax(X)\n",
    "\n",
    "X = np.array([[1,2],[3,4], [5,6]])\n",
    "print softmax(X)\n",
    "\n",
    "\n",
    "X = np.array([1,2])\n",
    "print softmax(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.reshape(1,X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing softmax\n",
      "[[1001 1002]\n",
      " [   3    4]] (2L, 2L)\n",
      "[[ 0.36787944  1.        ]\n",
      " [ 0.36787944  1.        ]]\n",
      "[[ 1.36787944]\n",
      " [ 1.36787944]]\n",
      "[[1002]\n",
      " [   4]]\n",
      "[[ 0.26894142  0.73105858]\n",
      " [ 0.26894142  0.73105858]]\n",
      "Computing softmax\n",
      "[[-1001 -1002]] (1L, 2L)\n",
      "[[ 1.          0.36787944]]\n",
      "[[ 1.36787944]]\n",
      "[[-1001]]\n",
      "[[ 0.73105858  0.26894142]]\n"
     ]
    }
   ],
   "source": [
    "print softmax(np.array([[1001,1002],[3,4]]))\n",
    "print softmax(np.array([[-1001,-1002]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\" Sigmoid function \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the sigmoid function for the input here.                #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    x = 1./(1+np.exp(-x))\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return x\n",
    "\n",
    "def sigmoid_grad(f):\n",
    "    \"\"\" Sigmoid gradient function \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the gradient for the sigmoid function here. Note that   #\n",
    "    # for this implementation, the input f should be the sigmoid      #\n",
    "    # function value of your original input x.                        #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    f = f*(1-f)\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_grad(sigmoid(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up fake data and parameters for the neural network\n",
    "N = 20\n",
    "dimensions = [10, 5, 10]\n",
    "data = np.random.randn(N, dimensions[0])   # each row will be a datum\n",
    "labels = np.zeros((N, dimensions[2]))\n",
    "for i in xrange(N):\n",
    "    labels[i,np.random.randint(0,dimensions[2]-1)] = 1\n",
    "\n",
    "params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (dimensions[1] + 1) * dimensions[2], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20L, 10L) (20L, 10L) (115L,)\n",
      "(10L,)\n"
     ]
    }
   ],
   "source": [
    "print data.shape, labels.shape, params.shape\n",
    "print data.T[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_backward_prop(data, labels, params):\n",
    "    \"\"\" Forward and backward propagation for a two-layer sigmoidal network \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the forward propagation and for the cross entropy cost, #\n",
    "    # and backward propagation for the gradients for all parameters.  #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### Unpack network parameters (do not modify)\n",
    "    t = 0\n",
    "    W1 = np.reshape(params[t:t+dimensions[0]*dimensions[1]], (dimensions[0], dimensions[1]))\n",
    "    t += dimensions[0]*dimensions[1]\n",
    "    b1 = np.reshape(params[t:t+dimensions[1]], (1, dimensions[1]))\n",
    "    t += dimensions[1]\n",
    "    W2 = np.reshape(params[t:t+dimensions[1]*dimensions[2]], (dimensions[1], dimensions[2]))\n",
    "    t += dimensions[1]*dimensions[2]\n",
    "    b2 = np.reshape(params[t:t+dimensions[2]], (1, dimensions[2]))\n",
    "    \n",
    "    ### YOUR CODE HERE: forward propagation\n",
    "    # Layer 1\n",
    "    print \"Shapes: x=> %s, W1 => %s, b1 => %s, W2 => %s, b2 => %s, y => %s\" \\\n",
    "        %(data.shape, W1.shape, b1.shape, W2.shape, b2.shape, labels.shape)\n",
    "    N = data.shape[0]\n",
    "    X = data\n",
    "    Y = labels\n",
    "    Z1 = X.dot(W1)\n",
    "    H = sigmoid(Z1+b1)\n",
    "    Z2 = H.dot(W2)\n",
    "    Y_hat = softmax(Z2+b2)\n",
    "    # cost = ...\n",
    "    cost = np.sum(-Y*np.log(Y_hat))/N\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    ### YOUR CODE HERE: backward propagation\n",
    "    print \"z1 => %s, h => %s, z2 => %s, y_hat => %s\" % (Z1.shape, H.shape, Z2.shape, Y_hat.shape)\n",
    "    #gradW1 = ...\n",
    "    #gradb1 = ...\n",
    "    #gradW2 = ...\n",
    "    #gradb2 = ...\n",
    "    #print y\n",
    "    #print y_hat\n",
    "    #print y_hat-y\n",
    "    dZ2 = Y_hat - Y \n",
    "    dW2 = H.T.dot(dZ2)\n",
    "    db2 = np.sum(dZ2, axis=0)\n",
    "    dH = dZ2.dot(W2.T)\n",
    "    dZ1 = dH*sigmoid_grad(H)\n",
    "    dW1 = X.T.dot(dZ1)\n",
    "    db1 = np.sum(dZ1, axis=0)\n",
    "    print \"dZ2 => %s, dW2 => %s, db2 => %s\" % (dZ2.shape, dW2.shape, db2.shape)\n",
    "    \n",
    "    gradW2 = dW2/N\n",
    "    gradW1 = dW1/N\n",
    "    gradb1 = db1/N\n",
    "    gradb2 = db2/N\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    ### Stack gradients (do not modify)\n",
    "    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(), gradW2.flatten(), gradb2.flatten()))\n",
    "    \n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "(115L,)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gradcheck_naive' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-483711033303>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#print forward_backward_prop(data, labels, params)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mgradcheck_naive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mforward_backward_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'gradcheck_naive' is not defined"
     ]
    }
   ],
   "source": [
    "# Perform gradcheck on your neural network\n",
    "print \"=== For autograder ===\"\n",
    "#print forward_backward_prop(data, labels, params)\n",
    "print params.shape\n",
    "gradcheck_naive(lambda params: forward_backward_prop(data, labels, params), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
